,Document,NameCompany,Description,Raiting,DataPublish,Activity,TextArticle
0,"Cocoapods, Carthage, SPM как выбрать менеджер зависимостей в iOS.pdf",red_mad_robot,№1 в разработке цифровых решений для бизнеса,117.94,6 часов назад,"Блог компании red_mad_robot, разработка под ios","117.94 Рейтинг red_mad_robot №1 в разработке цифровых решений для бизнеса  6 часов назад Средний18 мин redmadrobot Cocoapods, Carthage, SPM: как выбрать менеджер зависимостей в iOS 179 Блог компании red_mad_robot ,  Разработка под iOS* FAQ КАК СТАТЬ АВТОРОМ Старший iOS-разработчик red_mad_robot Аня Кочешкова рассказывает, чем отличаются три менеджера зависимостей, в каких случаях и для каких задач подойдёт тот или иной. Материал будет полезен джун-специалистам, которые только начали погружаться в разработку: специально для них подробно объясняем, что такое семантическое версионирование, как устроены модули кода и в чём разница между динамическими и статическими библиотеками. Что такое менеджер зависимостей и зачем он нужен В современной разработке зависимость — это написанный кем-то другим код, который используется в вашей программе. Добавление зависимости позволяет воспользоваться уже сделанной кем-то хорошей работой и избежать «изобретения велосипеда», а также лишних действий: проектирования, написания, тестирования, отладки и поддержки определённой логики. Такие «куски логики» называют пакетами, библиотеками или модулями. Чем сложнее проект, тем больше сторонних библиотек (правда, некоторые приложения могут вообще отказываться от их использования), а у этих библиотек много разных версий. Они могут ссылаться друг на друга и накладывать ограничения на эти версии. Эту работу можно проиллюстрировать таким бытовым примером. Чтобы телефон показал погоду на улице, не нужно встраивать в него термометр и вытаскивать за окно. В нём уже зашита специальная технология, которая позволяет ему связаться с «библиотекой», где хранятся данные гидрометцентра, которые, как и погода, всё время обновляются. Иногда с новой версией добавляется какая-нибудь новая функциональность. В примере с термометром библиотека с новой версией умеет ещё и выдавать карту дождей, а до этого могла показывать только температуру. Менеджеры зависимостей позволяют избежать долгого и муторного решения такого набора «уравнений» — то есть берут на себя часть работы, которую без них разработчик делал бы руками. Это система, которая позволяет управлять зависимостями в вашем проекте. А ещё у сторонних библиотек часто могут выходить новые версии с исправленными ошибками, и благодаря менеджеру зависимостей вам не нужно следить за их выходом — он всё обновит за вас. Предположим, вы решили подключить в ваш проект Firebase Crashlytics и GoogleUtilities. Обе эти библиотеки ссылаются на библиотеку PromisesObjC. Firebase хочет, чтобы она была любой версии, начинающейся с 2.1, а GoogleUtilities — чтобы она была от 1.2 до 3.0. Такое уравнение, конечно, можно решить вручную, но что делать, когда этих уравнений несколько десятков, а уровней зависимостей — два, три или больше? Для таких задач в распоряжении iOS-разработчика существуют три основных менеджера зависимостей: 1. Cocoapods. Пожалуй, самый распространённый сегодня, он требует лишь указывать список зависимостей в определённом формате в текстовом файле. 2. Carthage. Менее удобен в этом плане: вы также указываете список зависимостей в текстовом файле, но помимо этого нужно ещё немного «самостоятельной работы», о которой расскажем ниже. 3. SPM, или Swift Package Manager. Официальный менеджер зависимостей от Apple. Появился недавно, и многие команды переходят на него с Cocoapods. Позволяет управлять зависимостями как через графический, так и через текстовый интерфейс прямо из Xcode. Случается, что нужная библиотека не поддерживает необходимый менеджер зависимостей, потому что разработчики каждой библиотеки сами выбирают, какой менеджер поддержать. Поддержка каждого менеджера — это работа, которая занимает определённое время, поэтому часто выбирают работать только с самым популярным менеджером, Сocoapods. А если в проекте, например, SPM, а библиотека есть только в Cocoapods или просто нужна в одной конкретной версии, приходится добавлять вручную как framework / xcframework, о чём мы также поговорим позже. Но прежде чем начать говорить об управлении зависимостями, нужно разобрать несколько связанных с ним терминов. Фантастическое Semantic Versioning, и где оно обитает Semantic Versioning (SemVer) — это стандарт, который говорит о том, как работать с версиями пакетов/библиотек/приложений, — как их указывать и накладывать на них ограничения. Зачем накладывать ограничения? Возьмём такой пример. Разработчик создаёт приложение и использует некоторый набор библиотек для упрощения работы — например, библиотек для работы с сетью Alamofire. Допустим, начиная с определённой версии в библиотеке появляется поддержка какой-то новой функциональности, например Modern Concurrency. Эта функциональность доступна только начиная с определенной версии библиотеки. В таком случае от разработчика потребуется всего лишь поднять версию Alamofire в манифесте, где он описывает используемые библиотеки. Если начиная с какой-то версии библиотеки исправляется некий важный баг, но не сильно меняется функциональность самой библиотеки, то обновление до свежей исправленной версии произойдёт без лишнего вмешательства разработчика, в следующий раз при обновлении текущих библиотек проекта. Разберём, что такое SemVer, чтобы понимать формат версий, с которыми придётся работать, а также узнаем, как можно указать версию зависимости и наложить на неё ограничения. Этот набор правил — глобальный стандарт, который используется повсеместно, начиная от пакетов Node.js и заканчивая версиями приложений в App Store. По этому стандарту номер версии — это строка, которая состоит из чисел, разделённых точкой, и имеет формат A.B.C, где A — мажорная версия, B — минорная версия и C — патч-версия. Версии сравниваются слева направо, например: 1.0.0 < 2.0.0 < 2.1.0 < 2.1.1. Но зачастую мажорная версия увеличивается или при выпуске каких- либо масштабных фич, или при редизайне приложения, или просто при желании обозначить принципиальное обновление приложения/библиотеки. Синтаксис SemVer в Cocoapods, Carthage и SPM Cocoapods В Cocoapods можно указать одну из стратегий выбора версии: 1. Если необходима конкретная версия, просто укажите её без дополнительных символов, как есть: '5.0.0'. 2. Если необходима точная версия, но при этом вы также хотите обновляться, если придут исправления багов в патч-версии, используйте операнд ~>: '~> 5.0.0'. 3. Можно указывать неравенства: '> 5.0.0', '≥ 5.0.0' или '< 5.0.0', '≤ 5.0.0'. В этом случае Cocoapods обновит библиотеку до 5.0.1, 5.0.2 и т. д., но не возьмёт обновление 5.1.0, так как в нём уже повышена минорная, а не патч-версия. Если вы также хотите завязаться на минорную версию, достаточно просто указать '~> 5.0'. Carthage В Carthage используется тот же формат, что и в Cocoapods, с тем только исключением, что точная версия указывается через равенство: == 5.0.0. SPM SPM позволяет указать стратегию выбора версии прямо в интерфейсе, доступны следующие: Аналогичный синтаксис и у Package.swift файла. Модули — что к чему: библиотеки, фреймворки, XCFramework и Swift-пакеты Организация кода в iOS построена на понятии модулей. Модуль — это отдельный кусок кода или функциональности, который можно распространять разными способами и переиспользовать. Существует четыре вида модулей. Разберёмся, в чём разница: 1. Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру. 2. Фреймворк — папка, содержащая в себе библиотеку и вспомогательные файлы. 3. XCFramework — набор фреймворков под разные архитектуры. 4. Swift-пакет — фреймворк, который используется для работы со Swift Package Manager. Если говорить на языке бытовых примеров, то код — это комната, а модули — разные предметы в ней. Торшер, например, позволяет включать и выключать свет, но как он работает внутри, мы не знаем. И этим торшером могут пользоваться в разных квартирах и комнатах разные люди. Библиотеки Библиотека — это исполняемый код, скомпилированный под конкретную архитектуру, например arm64. Архитектуры могут различаться для разных типов устройств и симуляторов: • iOS-устройства — архитектуры armv7, armv7s, arm64, • iOS-симулятор — архитектуры i386 и x86_64. Исходный код — текст программы, который написал разработчик. Исполняемый код — код, который получился в результате компиляции. Компилятор переводит исходный код в машинный, который сможет исполнить операционная система, — на выходе получается исполняемый код. Библиотеки бывают статическими и динамическими (об этом позже), и представляют собой один бинарный файл. Если представить, что библиотека — это настоящая библиотека со стеллажами, в которых хранятся книги, то эти книги — справочники о том, как что-то сделать (методы для выполнения разных задач). По сути, это скомпилированный набор объектных файлов, которые программа может использовать для выполнения любой задачи. То есть все файлы, которые библиотека собирается использовать, были скомпилированы до этапа компоновки. Фреймворки Фреймворк (.framework) представляет собой папку с файлами, в которой лежит сама библиотека, хедеры и ресурсы. Эта папка должна иметь определённую структуру, но нет нужды запоминать её, так как фреймворки, как правило, собираются самим Xcode. Поддержка фреймворков добавилась в iOS 8. Фреймворки служат той же цели, что и библиотеки, но могут включать в себя различные ресурсы — картинки, документацию, строковые файлы. Они также могут быть статическими или динамическими. Что использовать: фреймворк или библиотеку В чём же разница между фреймворком и библиотекой? И когда их использовать? Принципиальное различие — в архитектуре. XCFrameworks Для начала стоит рассказать, что такое fat-фреймворки. Это такие фреймворки, которые содержат библиотеки для разных архитектур и платформ, соединённые в одну. С появлением Xcode 11 на замену им пришли XCFrameworks, которые ввели структуру, позволяющую разнести библиотеки для разных платформ и архитектур по папкам. XCFramework — это, по сути, такой усовершенствованный fat- фреймворк. Он также содержит различные библиотеки для разных архитектур и платформ, но теперь нет необходимости трудиться над тем, чтобы объединить их в одну. В одном XCFramework может лежать несколько разных фреймворков — для iOS, macOS, tvOS, watchOS, а также для разных архитектур — armv7, x86_64 и т. д., каждый в своей папке. В каком-то смысле фреймворк — тоже разновидность библиотеки, а XCFramework — разновидность фреймворка, но всё же эти понятия принято разделять. Создатель зависимости сам выбирает, в каком виде её распространять. Нам важно понимать различие между этими видами, выбор можно сделать, только создавая свою библиотеку. Swift Package Swift-пакет используется для распространения только через SPM. Содержит исходный код, а также Package.swift файл, который описывает его конфигурацию. Доступен начиная со Swift 3. Static vs Dynamic Прежде чем начать, наконец, говорить об управлении зависимостями, важно разобраться, в чём различие между статическими и динамическими библиотеками и фреймворками. Первое, что здесь важно понимать: системные iOS- и macOS- библиотеки — динамические. Приложения могут хранить ссылку на библиотеку. Например, системная библиотека Foundation — динамическая. Она лежит на айфоне в единственном экземпляре, а все приложения лишь ссылаются на неё. Фреймворки, как и библиотеки, бывают динамические и статические и представляют собой обёртку библиотеки. Так, динамические библиотеки вне фреймворков не поддерживаются на iOS, watchOS и tvOS, и здесь важно отметить, что во всех дальнейших сравнениях, говоря «динамическая библиотека», мы будем иметь в виду её использование в рамках динамического фреймворка. Фреймворки в Cocoapods и Carthage Зависимости, распространяемые через Cocoapods, называются кокоаподами, или подами. До iOS 8 под представлял собой fat- статическую библиотеку. Есть специальная инструкция — по ней Cocoapods использует фреймворки вместо статичных библиотек, которые не поддерживались Swift до Xcode 9 и CocoaPods 1.5.0. Так что раньше эта директива была обязательна, сейчас — опциональна. Если нужно более быстрое время запуска, можно её убрать (см. предыдущий раздел). Carthage поддерживает только фреймворки. target 'TargetName' do   use_frameworks! end Три менеджера зависимостей: Cocoapods, Carthage, SPM Наконец, то, ради чего мы тут собрались, — менеджеры зависимостей. Посмотрим, какие они бывают, чем отличаются и как выбрать между ними. Cocoapods Это один из самых популярных сегодня менеджеров зависимостей. Позволяет указать список зависимостей в текстовом файле, а затем генерирует на основе его .xcworkspace, который будет содержать два проекта — ваш проект и проект с подами. Как правило, почти все библиотеки поддерживают Cocoapods, хотя с развитием SPM стали появляться и такие, которые поддерживают исключительно SPM (в основном это библиотеки Apple). В таком случае, если ваш менеджер зависимостей Cocoapods, единственный вариант воспользоваться библиотекой — вручную её собрать и прилинковать к проекту. Чтобы понять, поддерживает ли библиотека Cocoapods, достаточно заглянуть в readme, либо обратить внимание на наличие .podspec- файла в корне репозитория. Все библиотеки, опубликованные в Cocoapods, можно посмотреть на их официальном сайте. Cocoapods в вашем проекте Для понимания работы Cocoapods важны два термина — подфайл и спека. Подфайл описывает все зависимости приложения, а спека — библиотеку (её название и набор её зависимостей) для Cocoapods. Спеки хранятся в общем репозитории спек Cocoapods. Ещё их можно хранить в приватных репозиториях. Для описания конфигурации зависимостей используется подфайл. Этот файл написан на языке Ruby. 1. В файле опционально укажите источник спек. Источников может быть несколько, даже приватные. source 'https://github.com/CocoaPods/Specs.git' — официальный источник спек, который указан по умолчанию. source 'https://github.com/Artsy/Specs.gi — можно указать свои источники спек, в том числе приватные. 2. Укажите минимальную версию iOS: platform: ios, '9.0'. 3. Укажите таргет и набор зависимостей для него. Если таргетов несколько, укажите все, так как зависимости для них могут различаться. 4. Декларируйте зависимость. Первым идёт её официальное название (то самое, которое указано в спеке), затем версия. Используйте в названии правила из блока про SemVer. Есть и другие способы указать зависимости. Это может понадобиться в случаях, когда нужно использовать какую-то конкретную версию библиотеки. Например, из определённой ветки репозитория, где она хранится, или даже из определённого коммита. Либо указать путь локально, если есть только её файл.     target 'MyApp' do     pod 'GoogleAnalytics', '~> 3.1'     ...     end 1. Можно указать ресурс репозитория, с которого будем загружать библиотеку: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git' 2. Можно указать нужную ветку в репозитории: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :branch => 'dev' 3. Или тег: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :tag => '0.7.0' 4. Или коммит: pod 'CRToast', :git => 'https://github.com/akhatmullin/CRToast.git', :commit => '082f8310af' 5. Можно указать на локальный под (путь должен вести на папку, где лежит .podspec): pod 'AFNetworking', :path => '~/Documents/AFNetworking' Рядом с подфайлом обычно лежит Podfile.lock. Этот файл описывает конечные версии зависимостей после их установки. Он обычно так же заливается в GitHub, как и сам Podfile, чтобы избежать ошибок и рассинхрона при установке зависимостей у разных разработчиков. Ещё он ускоряет выполнение pod install. Чтобы приступить к работе, запустите pod install. Можно выполнить pod update, если вы хотите обновить все библиотеки в проекте, иначе же будут использоваться фиксированные версии зависимостей из Podfile.lock. Как Cocoapods работает под капотом Pod install генерирует workspace — файл .xcworkspace, состоящий из двух проектов — проекта с вашим приложением и отдельного проекта, в котором хранятся зависимости (поды). Чтобы всё работало корректно, всегда открывайте workspace-файл. Если внимательнее посмотреть, что происходит, можно увидеть, что продукт этого проекта линкуется в основной проект: Также Cocoapods добавляет пару скриптов в Build Phases: Поддержка Cocoapods в вашей библиотеке Чтобы библиотека была доступна через Cocoapods, нужно сначала описать библиотеку или под при помощи спеки, собрать её и отправить спеку в приватный либо глобальный репозиторий Cocoapods. Разберёмся, как это сделать. 1. Создайте спеку и файл лицензии. Лицензию нужно добавлять для любой библиотеки, и чаще всего используется лицензия MIT — самая популярная лицензия для программ с открытым исходным кодом. Её текст можно найти в интернете. Здесь нужно описать вашу библиотеку. Файл принято форматировать так, чтобы вторая часть строки была выровнена.     Pod::Spec.new do |spec|     # Название библиотеки. Именно его будут указывать разраб     spec.name                    = 'NAME'      # Версия     spec.version                 = '1.2.1'      # Описание     spec.summary                 = 'Describe your framework.     # Здесь можно указать путь на вебсайт или просто GitHub-     spec.homepage                = 'https://xxx'      # Путь к лицензии. Да, её тоже нужно создать     spec.license                 = { type: 'MIT', file: 'LIC     # Автор библиотеки     spec.author                  = { ""Your Name"" => 'your-em     # Путь к исходным файлам (чаще всего это ваш репозитоий)     spec.source                  = { :git => 'https://github     # Минимальная поддерживаемая версия iOS и Swift     spec.ios.deployment_target   = '13.0'      spec.swift_version           = '5.0'     # Если библиотека использует локальный фреймворк, его ну     spec.vendored_frameworks     = 'Path/To/Files/NameOfVend 2. Проверьте, что всё корректно. Следующая команда валидирует спеку — проверяет, что она не имеет ошибок, а проект билдится — pod lib lint. 3. Отправьте спеку в репозиторий спек. Как правило, это делается уже на финальном этапе разработки — после того, как прошла проверка и вы готовы публиковать работу, — pod trunk push NAME.podspec. Carthage     # Указываем вайлдкард-пути к файлам, которые необходимо      spec.source_files            = 'Path/To/Files/**/*.{swif     # Путь к ресурсам     spec.resources               = 'Path/To/Files/Resources/     # Укажите здесь все ассеты, которые вам нужны, включая .     spec.resource_bundles        = {'BundleName' => ['Path/T     # Указываем зависимости     spec.dependency           ""Alamofire"", ""~> 5.0.0""      # И системные зависимости     spec.frameworks           = 'Foundation'      spec.ios.frameworks       = 'UIKit'     end Carthagе — это децентрализованный менеджер зависимостей, которые не нужно «поддерживать» разработчику. Carthage сам выкачивает библиотеку из репозитория, компилирует её и предоставляет в готовом виде. Поэтому, если хотите использовать Carthage, не нужно искать в репозитории с библиотекой определённые инструкции или слова о том, что он поддерживается, — просто попробуйте. Нет у Carthage и централизованного источника, как у Cocoapods. Этим источником, по сути, является сам GitHub. Конечно, этого не всегда бывает достаточно. Некоторые библиотеки Carthage просто не может «переварить» — что-то не собирается, а что-то отваливается. Задача разработчика — проверить, всё ли работает корректно. Наконец, Carthage может автоматически подхватить уже собранный фреймворк, если он прикреплён к соответствующему релизу на GitHub. Carthage в вашем проекте Зависимости в Carthage описываются в текстовом файле Cartfile. Он очень похож на Podfile, только гораздо проще по структуре. Мы указываем источник, имя зависимости и версию. 1. Источник зависимости. Как правило, GitHub, либо git для универсальных репозиториев Git, размещённых в другом месте. Ключевое слово git сопровождается путём к репозиторию, будь то удалённый URL-адрес, используя git://, http://, или ssh://, или локальный — путём к репозиторию git на компьютере разработчика. 2. Название. Через слеш указывается владелец и название репозитория, которые можно подсмотреть в ссылке на репозиторий. 3. Версия. Особенности указания версий мы уже рассмотрели в блоке про семантическое версионирование, но можно также указывать и ветки. Примеры: 1. Версия — github ""Alamofire/Alamofire"" == 2.0 2. Ветка — github ""username/project"" ""branch"" 3. Локальный проект — git ""file:///directory/to/project"" ""branch"" 4. Бинарник (путь указывается в json) — binary ""https://my.domain.com/release/MyFramework.json"" ~> 2.3 После описания зависимостей потребуется ещё несколько дополнительных шагов. 1. Вызовите carthage update. Carthage склонирует репозитории с указанных в Cartfile путей и затем сбилдит для каждой зависимости фреймворк. После окончания работы вы увидите несколько файлов и каталогов: 2. Полученные фреймворки затем вручную перетащите из папки Build в проект в секцию Frameworks, Libraries и Embedded Content. 3. Carthage требует добавления нового скрипта в Build Phases в качестве workaround бага Apple. Это позволяет dSYM- файлам корректно подтягиваться при архивации. 4. Перейдите в Build Phases и добавьте новый Run Script. Добавьте следующую команду: /usr/local/bin/carthage copy- frameworks. 5. Нажмите на плюс под Input Files и добавьте запись для каждой платформы: $(SRCROOT)/Carthage/Build/iOS/Alamofire.framework. Как Carthage работает под капотом Как и в случае с Cocoapods, создаётся лок-файл Cartfile.resolved, который фиксирует конечные версии зависимостей. Каталог Carthage содержит два подкаталога: 1. Build. Содержит собранный фреймворк для каждой зависимости. 2. Checkouts. Содержит клонированные исходники, которые Carthage потом компилирует. Это своего рода кэш, который ускоряет работу. Поддержка Carthage в вашей библиотеке Для поддержки Carthage достаточно сделать релиз на GitHub. Но никогда не помешает проверить, всё ли билдится корректно в вашем случае и, если есть какие-то проблемы, решить их. Для этого, выполните carthage build --no-skip-current. Carthage попытается сбилдить ваш проект и создаст в нём папку Carthage. Также стоит обратить внимание, что он видит только те схемы, которые помечены в проекте как Shared. SPM Или Swift Package Manager — официальный менеджер зависимостей от Apple. Работа с ним происходит прямо в Xcode, а .Xcodeproj файлы больше не нужны (хотя при желании есть возможность сгенерировать файл проекта из SPM-пакета). Если заглянуть в репозиторий библиотеки, то первое, что будет говорить о поддержке SPM, — это наличие Package.swift-файла. SPM в вашем проекте Добавление зависимостей в проект происходит в секции Package Dependencies. 1. Для начала выберите File > Swift Packages > Add package dependecy. 2. Найдите модуль по поиску, либо введите URL репозитория. 3. Укажите версию: Как и во всех предыдущих случаях, SPM создает Package.resolved, назначение которого то же, что и у других менеджеров зависимостей. В меню File > Swift Packages доступно несколько действий: • Reset Package Caches удаляет кэши из Derived Data. • Resolve Package Versions резолвит версии зависимостей. Создаёт Package.resolved, если его не существует, но не меняет его, если он уже есть. Может потребоваться, если Package.resolved был, например, обновлён извне. • Update to Latest Packages Versions обновляет версии пакетов, может модифицировать Package.resolved. Поддержка SPM в вашей библиотеке Если вы разрабатываете библиотеку и хотите распространять её через SPM, нужно будет создать Package.swift файл. Таким образом может быть сконфигурирован и обычный проект, что позволяет отойти от использования project-файлов. Конфигурация через SPM выглядит чище и более проста для понимания и редактирования, чем стандартные project-файлы, которые зачастую нечитаемы. Если вы пишете библиотеку и хотите распространять её через SPM, обязательно убедитесь, что потенциальные потребители библиотеки не используют Cocoapods. Одновременная поддержка Cocoapods и SPM — часто непростая задача. import PackageDescription   let package = Package(     // Название нашего пакета     name: ""Resources"",     // Платформы, которые поддерживаются нашим пакетом     platforms: [         .iOS(.v11),     ],     // То, что будут использовать сторонние приложения     products: [         .library(             name: ""Resources"",             // Динамический или статический продукт             // по дефолту значение nil - SPM сам будет поним             // преференция, скорее всего, будет отдаваться .             type: .dynamic,             targets: [""Resources""]),     ],         // Зависимости, необходимые для работы нашего пакета   // здесь они просто загружаются, добавляются они в target     dependencies: [         // Название пакета, путь к нему и источник (ветка, в         .package(             name: ""R.swift.Library"",              url: ""https://github.com/mac-cain13/R.swift.Libr             branch: ""master""),         .package(             name: ""SVGKit"",             url: ""https://github.com/SVGKit/SVGKit.git"", И как выбрать подходящий менеджер             .upToNextMajor(from: ""3.0.0"")),         .package(             name: ""PhoneNumberKit"",             url: ""https://github.com/marmelroy/PhoneNumberKi             from: ""3.3.4""),         // Пример подключения локального пакета         .package(path: ""../Core"")     ],     targets: [         // Это то, из чего мы будем складывать наш продукт         .target(             name: ""Resources"",             dependencies: [                 // Здесь мы указываем зависимости, которые м                 .product(name: ""RswiftDynamic"", package: ""R.             ],             resources: [                 // Все ресурсы, которые мы хотим использоват                 // Путь к ним относительный от Sources/имя_п                 .process(""Resources"")             ])     ] ) Универсального ответа нет — выбор менеджера зависимостей зависит исключительно от нужд и особенностей вашего проекта. SPM чаще всего не будут поддерживать старые библиотеки, в то время как Cocoapods вполне могут не поддерживать новые (хотя это и большая редкость, чем первое). Carthage в этом плане может показаться универсальнее. Хоть он более громоздкий и сложный во взаимодействии, но он единственный разделяет шаги управления зависимостями и добавления их в проект. Это даёт контроль над ситуацией и ускоряет разработку — не будет лишних пересборок или заморозки интерфейса, потому что нужно зарезолвить зависимости. Над материалом работали: • текст — Аня Кочешкова, Ника Черникова, • редактура — Виталик Балашов, • иллюстрации — Юля Ефимова. Делимся железной экспертизой от практик в нашем телеграм-канале red_mad_dev. А полезные видео складываем на одноимённом YouTube-канале. Присоединяйся! Теги:   red_mad_robot , ios , cocoapods , carthage , spm , swift package manager , зависимости , управление зависимостями Хабы:   Блог компании red_mad_robot , Разработка под iOS +1 3 1 red_mad_robot №1 в разработке цифровых решений для бизнеса 57 Карма 13 Рейтинг @redmadrobot Пользователь Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты ИНФОРМАЦИЯ Сайт redmadrobot.ru Дата регистрации 16 августа 2009 Дата основания 16 ноября 2008 Численность 1 001–5 000 человек Местоположение Россия Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
1,Deutsche Telekom и Perplexity объявили о новом «AI Phone» стоимостью менее 1 000 долларов _ Хабр.pdf,technokratiya,-,0.0,1 час назад,-,"Еще до начала MWC в Барселоне было очевидно, что хотя бы один оператор представит амбициозный проект по созданию смартфона совместно с ведущей AI-компанией. И вот он: Deutsche Telekom (DT) сообщает, что готовит «AI Phone» — недорогой аппарат, разработанный в тесном сотрудничестве с Perplexity, а также Picsart и другими партнерами, и дополняет его новым приложением-ассистентом под названием «Magenta AI». DT планирует представить устройство во второй половине этого года, а продавать его начнет в 2026-м по цене менее 1 000 долларов. Как уточнил представитель компании, в первую очередь модель будет ориентирована на европейский рынок. technokratiya 1 час назад Deutsche Telekom и Perplexity объявили о новом «AI Phone» стоимостью менее 1 000 долларов 4 мин 419 Искусственный интеллект, Смартфоны КАК СТАТЬ АВТОРОМ Зарплаты айтишников Найдено: весенние мероприятия для айти… «Мы становимся AI-компанией», — заявила на пресс-конференции в понедельник член правления DT, отвечающая за технологии и инновации, Клаудия Немат. При этом она добавила, что оператор не создает фундаментальные большие языковые модели, а разрабатывает «AI- агентов». Примечательно, что Perplexity, стартап из Кремниевой долины, который, по слухам, уже оценивается примерно в 9 миллиардов долларов, отводится ключевая роль в создании смартфона. Это показывает, что компания, прежде всего известная благодаря генеративному AI- поисковику, делает шаг к созданию более «проактивных» решений. «Perplexity переходит от простой “машины ответов” к “машине действий”, — отметил сооснователь и генеральный директор Perplexity Аравинд Шринивас на сцене мероприятия. — Теперь мы не просто отвечаем на вопросы, но и можем бронировать рейсы, столики в ресторанах, отправлять письма, сообщения, совершать звонки и многое другое, включая установку умных напоминаний». Хотя это, по всей видимости, первая крупная сделка Perplexity с оператором связи для разработки AI-интерфейса смартфона, компания уже имеет некоторый опыт в сфере помощников: в январе Perplexity выпустила Android-приложение, которое, судя по всему, может служить прообразом нового «AI Phone». Эта новость — очередная глава знакомой истории из мира телекоммуникаций. Долгие годы как мобильные, так и проводные операторы искали пути усилить позиции в конкурентной борьбе с технологическими гигантами. В частности, они пытались соперничать с Apple и Google, чьи операционные системы и смартфоны в значительной степени отодвигают операторов на периферию, лишая их существенной доли прибыли от приложений и прямого взаимодействия с пользователями. За эти Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп годы были попытки объединиться, к примеру, с Mozilla для создания «операторского» смартфона, который смог бы конкурировать с этими двумя компаниями (телефон под названием Firephone, впрочем, так и не нашел отклика). Были и «теплые» отношения с Facebook, стремившейся укрепить собственные позиции в мобильном мире (ныне Meta переключила усилия на другие направления в сфере оборудования и сетей). Подход «двигаться быстро и ломать все на своем пути» не слишком близок консервативным телекомам. Perplexity и Deutsche Telekom сотрудничают уже с апреля 2024 года, а впервые о «AI Phone» DT заговорила год назад на предыдущем Mobile World Congress. Немат не раскрыла подробностей о технических характеристиках и производителе нового девайса, а также не уточнила, на какой операционной системе он будет работать (на презентационных рендерах можно заметить явные черты Android). Представитель DT сообщил, что эта информация появится во второй половине года. Известно лишь, что в смартфон будет глубоко интегрирован AI, созданный с участием Perplexity, чтобы, по словам Немат, «пользователь получил весь спектр возможностей». Она подчеркнула, что AI будет работать прямо с экрана блокировки. Кроме Perplexity, в создании телефона принимают участие Google Cloud, ElevenLabs и Picsart, сообщили в DT. Приложение Magenta AI, являющееся самостоятельной версией голосового помощника от DT, будет доступно для установки на любые Android- и iOS-устройства, но только для 300 миллионов клиентов оператора, добавила Немат. Стремясь использовать нынешний всеобщий интерес к искусственному интеллекту — главную тему MWC этого года, — DT в очередной раз пытается закрепиться в сфере «якорного» оборудования, предлагая и приложение для тех, кто не хочет или не может купить фирменный смартфон. Perplexity же конкурирует не только с хорошо финансируемыми OpenAI и Anthropic, но и с такими гигантами, как Google, уже внедрившей свою модель Gemini во все основные продукты поиска. Выход на рынок «машин действий» в сотрудничестве с крупным оператором связи дает компании определенное преимущество, пусть и временное. Теперь Perplexity, похоже, переходит к следующему этапу эволюции, чтобы предоставить пользователям еще более удобные AI-инструменты. «Раньше для решения этих задач нужно было по отдельности осваивать разные приложения, — говорит Аравинд Шринивас. — Теперь все станет проще, и вы сможете сосредоточиться на более важных делах. Это действительно следующий виток развития, где AI перестанет быть чисто реактивным и превратится в неотъемлемую функцию вашего смартфона, которая всегда на связи и готова помогать проактивно». Пока не ясно, смогут ли DT и Perplexity добиться успеха в сложном рынке смартфонов, где доминируют считаные игроки, а крупные корпорации вроде LG в итоге полностью ушли из этой сферы. 17 Карма 4.4 Рейтинг Технократия @technokratiya ИТ-Компания Сайт Сайт Тем не менее это свидетельствует о том, насколько силен «притягательный эффект» AI сегодня, как даже давние игроки видят в нем спасительное решение, а передовые стартапы — возможность укрепить свои позиции на фоне жесткой конкуренции. Источник: TechCrunch Чтобы не пропустить анонс новых материалов подпишитесь на «Голос Технократии» — мы регулярно рассказываем о новостях про AI, LLM и RAG, а также делимся полезными мастридами и актуальными событиями. Теги: смартфоны, ии, искусственный интеллект, мобильные телефоны, ai, artificial intelligence,  perplexity, deutsche telekom Хабы: Искусственный интеллект, Смартфоны Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта Подписаться "
2,OSINT & Hacking — как работает фишинг для нельзяграма _ Хабр3.pdf,Cloud4Y,#1 Корпоративный облачный провайдер,71.07,5 часов назад,"Блог компании Cloud4Y, информационная безопасность, социальные сети и сообщест","71.07 Рейтинг Cloud4Y #1 Корпоративный облачный провайдер Автор оригинала: Yashwant Singh Взлом Instagram*аккаунта — популярный запрос в поисковиках. Поэтому есть смысл рассказать о том, как это обычно работает. Просто для того, чтобы вы знали, откуда может пойти атака.  5 часов назад Простой3 мин Cloud4Y OSINT & Hacking — как работает фишинг для нельзяграма 1.1K Блог компании Cloud4Y ,  Информационная безопасность* ,  Социальные сети и сообщест Обзор Перевод КАК СТАТЬ АВТОРОМ Чтобы начать попытки заполучить доступ к аккаунту, вы должны знать ник человека, которого вы пытаетесь взломать. Так что небольшая разведка будет очень кстати. Только не увлекайтесь. Существуют различные инструменты для разведки, в первую очередь, поиск пользователя в конкретной соцсети с целью узнать его ник. Я нашёл отличный инструмент под названием «Slash», который можно использовать для поиска любых учётных записей пользователя, если он везде регистрируется под одним ником. Ставим Slash Я проверил Slash на себе, и посмотрите на эти результаты. Некоторые из учетных записей, перечисленных здесь, были созданы много лет назад. git clone https://github.com/theahmadov/slash  cd slash  pip install -r requirements.txt  python slash.py help Slash — это простой консольный инструмент. Но вы также можете использовать такие инструменты, как WhatsMyName Web, который совершенно бесплатен. Вот, посмотрите. Я проверил WhatsMyName на себе. Мой ник «earthtoyash». Теперь, когда мы знаем больше о нашем пользователе, можно использовать эти знания. Например, через отправку фишинговых ссылок. Для этого создадим полезную нагрузку с помощью Zphisher. Ставим Zphisher с GitHub Клонируем репозиторий: Запускаем файл zphisher.sh: При первом запуске он установит зависимости и на этом всё. Система скажет, что Zphisher установлен. После установки вам нужно будет снова запустить zphisher.sh в каталоге zphisher командой ./zphisher , и тогда вы получите что-то вроде этого: git clone --depth=1 https://github.com/htr-tech/zphisher.git cd zphisher && ./zphisher.sh Как вы можете видеть, есть много вариантов и шаблонов, благодаря чему любой может заниматься фишингом. Мы сосредоточимся на нельзяграме. Итак, введите «2» и нажмите Enter. Следующий шаг полностью зависит от вас, выберите любой из них. Затем появится окно с выбором. Я выбрал третий вариант, так как он минималистичный и удобен для того, чтобы показать возможности инструмента. Опять же, чтобы все было просто, я пропущу пользовательский порт, но если вы уже используете порт 8080, то можете изменить его на 8000. Если нет, оставляйте всё как есть. Также важно маскировать URL, ну просто в целях безопасности. Можно использовать чтото вроде этого: Всё, Zphisher создал фишинговую ссылку, которую можно отправить жертве. Как только она нажмёт на ссылку, вы начнёте получать информацию о ней. Например, IPадреса, имена пользователей, пароли и т. д. Ещё можно использовать обратный поиск IP, чтобы определить местоположение вашей цели и многое другое. Итак, вот эти фишинговые ссылки. При нажатии открывается страница, похожая на официальную страницу входа в запрещённую соцсеть. Вот она, нехорошая После ввода учётных данных можно получить много информации на «хакерской» стороне терминала. Вот так, господа и дамы, можно без особого труда взломать учетную запись в нельзяграме. Поэтому в очередной раз напоминаем: нельзя нажимать на ссылки, которым вы не доверяете. Само собой разумеется, не используйте информацию из этой статьи с намерением причинить кому-либо вред. OSINT законен, но фишинг и кража личных данных даже в запрещённой соцсети является уголовным преступлением. И да, * Организация Meta, а также её продукт Instagram, на которые мы ссылаемся в этой статье, признаны экстремистскими и запрещены на территории РФ. Спасибо за внимание! Что ещё интересного есть в блоге Cloud4Y → Информационная безопасность и глупость: необычные примеры → NAS за шапку сухарей → Взлом Hyundai Tucson, часть 1, часть 2 → Столетний язык программирования — какой он → 50 самых интересных клавиатур из частной коллекции Теги:   взлом , соцсети , osint Хабы:   Блог компании Cloud4Y , Информационная безопасность , Социальные сети и сообщества 0 19 1 Cloud4Y #1 Корпоративный облачный провайдер Сайт Facebook Twitter ВКонтакте Telegram 149 Карма 53.5 Рейтинг Cloud4Y @Cloud4Y Корпоративный облачный провайдер Сайт ВКонтакте Telegram Комментарии 1 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты ИНФОРМАЦИЯ Сайт www.cloud4y.ru Дата регистрации 29 июля 2011 Дата основания 2009 Численность 51–100 человек Местоположение Россия Представитель Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
3,"«Мне кажется, так неудобно» — как аргументировать дизайн-решение без исследований и конфликтов _ Хабр.pdf",Selectel,IT-инфраструктура для бизнеса,2055.96,28 фев в 13:40,"Блог компании Selectel, Usability*, Веб-дизайн*, Графический дизайн*, Интерфейсы","4.45 Оценка 2055.96 Рейтинг Selectel IT-инфраструктура для бизнеса Подписаться Привет, Хабр! Наверное, каждый дизайнер или проектировщик сталкивался с ситуацией, когда другие точно знали «как лучше сделать». Например, приходишь на командную встречу, показываешь решение, чтобы обсудить финальный макет или корнер-кейсы, а в ответ: «Мне кажется, эта кнопка слишком яркая!», «А давай сделаем фильтры, как у Google?», «На прошлой работе дизайнер сделала вот так, давайте покажу». Я Даша, проектировщик интерфейсов в Selectel. В этой статье расскажу, как защитить свое d_hutoryanskaya 28 фев в 13:40 «Мне кажется, так неудобно» — как аргументировать дизайн- решение без исследований и конфликтов 5 мин 6.8K Блог компании Selectel, Usability*, Веб-дизайн*, Графический дизайн*, Интерфейсы* +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Зарплаты айтишников Травка зеленеет, солнышко блестит, а ски… дизайн-решение, если коллеги или заказчики хотят все поменять. Подробности под катом. Используйте навигацию, если не хотите читать текст полностью: → Как объяснить идею или решение → Как обосновать UX / UI → Выводы Знакомо? Если у вас побежали мурашки — вы не одиноки. Коллеги могут предлагать правки по разным причинам. Например, потому что переживают за результат и хотят помочь с макетом. Но в продуктовой команде вы UX-эксперт. Нужно уметь отстаивать свою позицию и аргументированно отвечать на предложения или советы команды. А как это сделать, если вы при подготовке решения не проводили исследования, расскажу в статье. Как объяснить идею или решение Дизайн-процесс предполагает решение какой-либо проблемы пользователя или бизнеса. Если вас спрашивают, почему вы предлагаете сделать именно так, расскажите об этом. Объясните, что именно такой подход лучше всего решает пользовательскую задачу. Для этого рекомендую инструмент HMW. How Might We (HMW) — методика для генерации идей и поиска решений на основе проблем или возможностей. Она строится на формулировке вопросов, начинающихся с фразы «Как мы можем...», чтобы стимулировать креативное мышление. Например, «Как мы можем сделать процесс регистрации более увлекательным и быстрым?» Хорошо, если вопрос о релевантности решения появляется задолго до демонстрации макетов. Например, вы услышали его при обсуждении результатов исследования, USM, табуретки фичи или другого артефакта, который показывает объем и причины изменений. Но мы же с вами знаем, что бывает всякое?) Так что, если вдруг вам задали подобный вопрос, отсылайте спрашивающих к сделанным артефактам. Чтобы прийти к дизайн-решению, необходимо следовать дизайн-процессу. Особенно хочу обратить ваше внимание на этап генерации идей на шаге проектирования. Именно здесь оценивается сложность и эффективность любого решения (которое, напомню, не всегда лежит в пределах интерфейса). Как обосновать UX / UI Обсуждение UI/UX — широкое поле для споров. Решение несложно обосновать, если вы проводили исследование например, UX-тестирование, карточную сортировку или что-то еще. Но если исследования не было и вам нечем оперировать, можно опираться на общепринятые стандарты, мнение экспертов или опыт известных компаний. Чтобы аргументировать решение, нужно самому верить в него. Перед проведением демо для продуктовой команды или коллег-дизайнеров рекомендую проверить его самостоятельно по пунктам списка ниже. UX-стандарты В индустрии существует ряд стандартов, на которые можно опираться в аргументации. Эвристики Эвристика — это рекомендация, правило или суждение эксперта, которое принято считать верным. Самые популярные эвристики в UX — от экспертов Нильсена Нормана, Бена Шнейдермана, Вайншенка и Баркера. Например, среди эвристик Нильсена Нормана есть такая: #5: Error prevention Лучше хорошо оформленного сообщения об ошибке — только отсутствие ошибки. Либо измените условия, провоцирующие ошибки, либо дайте пользователям подсказку заранее, чтобы он ее не совершил. Как ее применить? Например, в меню есть опция, которая доступна не всем пользователям, и вы решили скрыть ее от тех, у кого ограничены права доступа. Если вас спросят, зачем вы так сделали, можете сослаться на эту эвристику. Ведь если оставить такую опцию, некоторые пользователи попробуют ею воспользоваться, и тогда система вернет ошибку. Перед передачей макетов в разработку полезно свои макеты по эвристикам. Можно использовать любые — я привыкла работать с рекомендациями от Нильсена Номана. Спецификации дизайн-систем вашей компании или принципы гайдов Material, Apple Пожалуй, каждый дизайнер на демо макетов слышал вопрос вроде: «Мне кажется, эта кнопка слишком яркая и перетягивает внимание» или «Какой-то шрифт крупноватый». Если в компании есть дизайн-система с описанием компонентов и базовых стилей, а также спецификации, как с этим работать — можете отсылать вопрошающих к этим документам. Объясните, что вы использовали такой шрифт и кнопку, потому что это стандарты принятой в компании дизайн-системы. Если документа нет, можете обосновывать свое решение гайдами, написанными титанами индустрии: Material Design от Google и Human Interface Guidlines от Apple. Стандарты доступности (accesability) Существуют международные стандарты доступности, которые важно соблюдать, чтобы сайтами могли пользоваться все люди вне зависимости от состояния здоровья. Можете ознакомиться с ними в адаптации на русском языке или оригинале. Такие стандарты помогут ответить, почему этот синий такой яркий или почему ошибка подсвечивается не только красным цветом, но еще и иконкой. Например, вы выбрали такой синий, потому что он проходит по стандартам контрастности, а ошибка подсвечивается еще и иконкой, поскольку «цвет не должен служить основным источником информации для пользователей» — этот принцип актуален для пользователей, не различающих цвета. Рекомендую использовать эти принципы не только перед командной встречей, но и для проверки собственных макетов. Порой можно случайно забыть о каком-то критерии и затем найти недочеты в уже готовом решении. Гештальт-принципы Гештальт-принципы — это психологические законы, описывающие, как люди воспринимают и группируют визуальную информацию. Ссылаясь на них, вы ответите на все вопросы в духе: «Кажется, в этом компоненте много воздуха». Например, вы сгруппировали карточки товаров с помощью рамок и получили обратную связь: теперь они выглядят «шумно». Можете ответить коллегам, что следовали «Принципу общности». Он гласит: «Элементы, соединенные визуально, например, рамкой или линией, воспринимаются как связанные». Так что ваше решение обосновано, ведь, следуя психологическим принципам восприятия информации, вы уменьшаете когнитивную нагрузку, и пользователю легче находить нужную информацию. Другие продукты Когда вы работаете без полевых исследований, а только с кабинетными, например изучаете конкурентов на предмет функциональности фичи, один из основных аргументов — апеллировать к стандартам на рынке. Но здесь важно не скатиться в «Давайте сделаем, как у Apple» и к любому решению конкурента подходить критически. Некоторые продукты считаются успешными, несмотря на не самый качественный UX. Например, Amazon, хоть и является одним из мировых лидеров в области e- commerce, однако в профессиональном сообществе проектировщиков вызывает скорее удивление, нежели желание следовать его паттернам. Как так? Amazon – многофункциональный продукт, который располагает широким ассортиментом и быстрой доставкой, а это и есть его основное конкурентное преимущество. Экспертиза в UX и в сфере продукта Этот пункт релевантен для проектировщиков уровня «Халк», то есть опытных сеньоров. Обращаться к своей экспертизе можно только при одном условии: если у вас одновременно два увесистых «багажа опыта»: в UX и сфере продукта. Но, скорее всего, если вы сеньор, то вы и так Selectel IT-инфраструктура для бизнеса ВКонтакте Telegram Сайт 19 Карма 50 Рейтинг Дарья Хуторянская @d_hutoryanskaya UX/UI Designer Публикации ky0 10 часов назад Ваше мнение очень важно для нас (нет) 3 мин это знаете. И написанное выше вам было не ново:) Выводы Объясняйте себе и другим, почему вы приняли то или иное решение. Чем меньше абстрактных формулировок вы будете использовать, отсылая собеседника к конкретным решениям, стандартам, правилам и гайдам, там меньше будете слышать сомнений относительно вашего решения. И тем быстрее вы дорастете до проектировщика уровня «Халк». Теги: дизайн, интерфейсы, ux, selectel, дизайн-решения Хабы: Блог компании Selectel, Usability, Веб-дизайн, Графический дизайн, Интерфейсы Комментарии 5 +5 ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 4.7K Кейс Подписаться +50 38 5 +5 Подписаться alizar 10 часов назад Глупо покупать технику последней модели, если старая работает хорошо. И можно собрать ПК из комплектующих Простой 6 мин omyhosts 22 часа назад Артефакт эпохи: рождение, взлет и падение клипарта Простой 6 мин ru_vds 6 часов назад Я мучился с Git, поэтому создал про него игру Средний 8 мин DRoman0v 11 часов назад Очередная прогулка по барахолке в Испании: отличный ноутбук и много необычных штук 4 мин RationalAnswer 14 часов назад Уголовный кодекс vs Блиновская и Митрошина, а также вебкамщицы со стволами против криптоворов 11 мин +77 14 112 +112 7.7K Мнение +36 23 42 +42 3.4K Ретроспектива +29 12 22 +22 3.8K Кейс Перевод +24 36 5 +5 6.3K +24 3 10 +10 12K Дайджест Stepan_Burmistrov 9 часов назад Использование лидара от робота-пылесоса для системы предотвращения столкновений в автономных роботах Средний 22 мин ProstoKirReal 11 часов назад Сложно о простом. Как работает интернет. Часть 4. Что такое LAN, MAN, WAN, сети Clos и иерархия операторов Средний 20 мин savpek 9 часов назад От психолога до эльфа 80-го уровня: как создать свою уникальную роль для нейросети в домашних условиях и не сойти с ума Простой 26 мин dspmsu 12 часов назад Как я решал задачу 2025 года. Часть 1 Средний 9 мин Показать еще ВАКАНСИИ КОМПАНИИ «SELECTEL» Старший системный инженер / SRE (PaaS) +23 8 35 +35 2.6K Туториал +20 37 10 +10 5K +19 94 6 +6 1.6K Туториал +17 21 0 1K +14 7 0 Selectel · Москва · Можно удаленно Больше вакансий на Хабр Карьере ИНФОРМАЦИЯ Сайт slc.tl Дата регистрации 16 марта 2010 Дата основания 11 сентября 2008 Численность 1 001–5 000 человек Местоположение Россия Представитель Влад Ефименко ССЫЛКИ GPU в облаке от 29 рублей в час selectel.ru Серверы для Data Science за 4,12 ₽/час selectel.ru Серверы для ML-разработки за 4,12 ₽/час selectel.ru 50 ГБ хранилища от 80 рублей в месяц selectel.ru FAQ slc.tl Реферальная программа slc.tl Телеграм-канал о технологиях t.me Телеграм-канал про карьеру в IT t.me Вакансии slc.tl Академия Selectel slc.tl ВКОНТАКТЕ ВИДЖЕТ ВИДЖЕТ БЛОГ НА ХАБРЕ 11 часов назад Очередная прогулка по барахолке в Испании: отличный ноутбук и много необычных штук 9 мар в 13:15 Как появление знаменитостей (не) сделало игры лучше 8 мар в 13:00 Framework Desktop: игровой ПК от производителя модульных ноутбуков. Что за система 7 мар в 13:02 Как перестать бояться и задеплоить Django-проект в облако самым простым способом 6 мар в 13:02 Ультимативные крестики-нолики и iPXE 6.3K 10 +10 3.8K 11 +11 6.9K 17 +17 3.4K 1 +1 5.1K 4 +4 Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2025, Habr "
4,«Я — робот Вертер» или Нулевой закон робототехники _ Хабр.pdf,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2657.28,26 ноя 2024 в 16:01,"Блог компании RUVDS.com, IT-стандарты*, Будущее здесь, Голосовые интерфейсы*, Законодательство в IT","2657.28 Рейтинг RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Подписаться inetstar 26 ноя 2024 в 16:01 «Я — робот Вертер» или Нулевой закон робототехники Простой 8 мин 11K Блог компании RUVDS.com, IT-стандарты*, Будущее здесь, Голосовые интерфейсы*, Законодательство в IT Мнение КАК СТАТЬ АВТОРОМ Зарплаты айтишников Найдено: весенние мероприятия для айти… В этой статье речь пойдёт не об Азимове и его законах. Великий фантаст не мог предусмотреть всё. А о порочной практике, когда текстовые и голосовые роботы представляются человеческими именами. Или вообще не представляются. Разработчики и собственники таких роботов рассчитывают на то, что люди, собеседники роботов, будут введены в заблуждение, что говорят с человеком. Это даёт небольшой шанс владельцу робота что-то продать или предложить потребителям, сэкономив деньги на зарплате сотруднику колл-центра. Разберём подробно, как правильно должны представляться роботы и почему это важно. А также обсудим пути к созданию цивилизованной среды, в которой взаимодействие с роботами будет приносить больше пользы. ▍ Прозрачность и честность Представьте себе ситуацию, когда вы разговариваете с телефонным роботом, который представляется как «Мария» или «Алексей». Не зная о его истинной природе, вы ожидаете человеческого уровня понимания и эмпатии. Однако текущий уровень развития таких роботов очень далёк от человеческого уровня интеллекта, и это приводит к раздражению, злобе и неудовлетворённости пользователей. Наверное, вы множество раз были свидетелем подобных бесед по телефону или сами были их вынужденным участником: — Меня зовут Олег. Я помогу вам. У нас есть такая услуга, сякая услуга (долгое перечисление). — Мне нужен оператор. — Я уверен, что смогу помочь вам, назовите свою проблему. — Оператор. — Сформулируйте свой вопрос по-другому. — Оператор. — На какую тему вы хотите поговорить с оператором? — Оператор. — Какую проблему вы бы хотели обсудить с оператором? — Оператор. — Нажмите 1, нажмите 2, нажмите 3. — Оператор. И так до тех пор, пока скрипт (робот) не переключит на человека. Кристально ясно, что с вами говорили не Мария, Алексей или Олег, а всего лишь тупой скрипт. И это ещё не худший вариант подобной беседы. Куда хуже, когда звонишь конкретному сотруднику, а его «интеллектуальный» помощник гоняет вас по кругу, предлагая оставить сообщение, а затем повторить то, что ему якобы неслышно. И при этом уверяет, что ваш звонок безумно важен. И чемпион по выбешиванию в такой ситуации — это повторяющийся вопрос, по личному вы вопросу звоните или по служебному. +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп Авторы роботов, которые наиболее успешно с помощью интонаций голоса обманывают людей, раздуваются от гордости и пишут статьи, как они создали интеллектуального помощника Машу, которая повысила конверсию звонков в сообщения по сравнению с автоответчиком. Но меня эти фальшивые эмоции «Ой, наконец-то я вам дозвонилась» бесят и являются маркером того, что нужно бросить трубку. Отсутствие законов в этой области и каких-либо договорённостей между крупными игроками вынуждает очень многих людей не брать трубку, если звонит кто-то не из списка контактов, превращая людей в плане телефонного общения в хикикамори. Мне, конечно, понятно, стремление собственников бизнеса переадресовать любые вопросы клиентов скрипту, который работает практически даром, за электричество, в отличие от живых людей. По данным исследовательской компании MarketsAndMarkets, к 2027 году объём рынка искусственного интеллекта для колл-центров достигнет 4,1 миллиарда долларов. Я думаю, эти цифры давно уже достигнуты, так как прогноз был сделан ещё в 2022 году. — Ну хорошо, а если мой робот несколько умнее скрипта, почему ему нельзя представляться человеческим именем? — спросит меня владелец крупной корпорации. И вот что я отвечу: ▍ Роботы и бессмертие Роботы, в отличие от людей, потенциально бессмертны. Они могут функционировать бесконечно долго, пока кто-то платит за электричество и связь, а так же, изредка, меняет компьютерное железо. Тогда как радикальные методы омоложения и бессмертие для людей пока остаются чем-то из области фантастики. Роботы могут атаковать людей рекламными предложениями день и ночь. Мошенники тоже могут использовать голосовых, текстовых и видеороботов (deep fake). Сколько из нас получали звонки «от вашего мобильного оператора», с предложением перейти на новый домашний интернет или на новый тариф? Мне неоднократно звонил робот МТС, выдававший себя за женщину, но после многочисленных вопросов о его истинной сущности всё же признавался, что он робот, с просьбой не рассказывать об этом другим. Каждый такой звонок отгрызает кусочек от нашего оставшегося времени жизни, которая и так не такая уж долгая. Крупная компания, запускающая такой автоматизированный колл-центр, который ежедневно делает миллионы звонков, — это примерно как автоматический шестиствольный пулемёт, который калечит, убивает направо и налево, крадёт время жизни у своих клиентов. Использование таких технологий просто аморально. Собственник, конечно, размышляет по- другому. Он смотрит, сколько денег он потратил на робота и сколько денег тот ему принёс. Сколько времени робот украл у других, сколько раздражения принёс людям — собственнику практически безразлично. А ведь люди далеко не многозадачны. Каждый звонок продолжительностью даже менее минуты выбивает из контекста, человеку приходится потом вспоминать, чем он занимался, и тратить на это до 15 минут. Все, кто сосредоточенно писал код, меня поймут. Человек должен знать, на что он тратит свою жизнь. В идеале он должен знать не только, что говорит с роботом, но и уровень интеллекта этого робота. А для этого нужны какие-то стандарты, возможно, тесты IQ для роботов. Я хотел в качестве положительного примера привести пример, как представляется робот Вертер из «Гостьи из будущего». Однако не смог найти знаменитую мемную фразу «Я — робот Вертер» в фильме, видимо, её придумали позже. Поэтому приведу другое фантастическое произведение. В научно-фантастическом цикле романов «Культура» Иэна Бэнкса роботы и ИИ обладают различными общеизвестными уровнями сознания и мощи, что помогает установить ясные ожидания и оптимизировать взаимодействие. Если с человеком в романах Бэнкса говорит внесистемник (могущественный ИИ, в огромном корпусе корабля для путешествий между звёздными системами), то это почти что голос Бога, и ему явно стоит уделить внимание. Если же робот рангом поменьше, ценность такой беседы снижается. Подобное представление в начале разговора добавило бы ясности и существенно упростило бы людям решение их задач. Сейчас же непонимание реальных способностей бота создаёт значительное напряжение при общении человека с роботом, поскольку люди могут испытывать разочарование и раздражение от длительного и зачастую бесполезного взаимодействия с несовершенными технологиями. ▍ Имена, идентификация, предпосылки в человеческой культуре Интересно заметить, что люди дают животным имена, непохожие на людские, чтобы избежать путаницы. Например, кошка может носить имя «Мурзик», а собака — «Барон». Представьте себе ситуацию, когда кто-то идёт с расстроенным лицом, его спрашивают в чём дело, а человек сообщает, что Николай умер. Для собеседника сразу становится очевидным наличие огромной проблемы, и он начинает уточнять, что это за Николай, так как у него полно знакомых с таким именем. Если же человек упоминает, что «Шарик помер», вполне ясно, что, скорее всего, это событие совсем другого масштаба. Чтобы избежать лишнего эмоционального стресса и прояснить коммуникацию в нашей культуре, животным не принято давать человеческие имена. Из имени сразу становится ясно, что речь идёт не о человеке. Подобная практика должна применяться в отношении роботов. Как вариант, можно бы ввести принятые для этих целей поименования — разные для разного уровня ИИ — к примеру, Скриптун, Цифрун, Выберун и просто Дубок. То есть робот должен не только сообщать, что он является роботом, но и предоставлять информацию о своём уровне интеллекта. Кстати, у того же Бэнкса роботы (в особенности, внесистемники) обладали именами, которые было просто невозможно спутать с человеческими, настолько они были длинными и мудрёными. Понимание человеком уровня интеллекта и полномочий конкретного робота исключает раздражающий фактор. Люди могут позитивно относиться к роботам. Примером этого может послужить взаимодействие людей с роботами-доставщиками. Уровень их компетенций соответствует поставленным перед ними задачам и ожиданиям пользователей. Хотя… Они не говорят, и, может быть, в этом секрет их позитивного восприятия. ▍ Не обманешь — не продашь Это выражение знают все, кто занимается продажами. Очень печально, что в отношении создания голосовых роботов многие компании (и даже очень крупные) идут по этому пути. В отношении цифровых помощников в нашей стране сейчас настоящий Дикий Запад. Представление человеческими именами, фальшивые эмоции, наложенный офисный шум — всё делается для того, чтобы обмануть человека и представить робота заслуживающим внимания. Нет никакого регулирования, нет никаких внутриотраслевых стандартов. Если удалось обмануть хотя бы 5% клиентов, внушить им, что они говорят с человеком, а потом что-то им продать, перевести на более «выгодный» тариф (как правило, более дорогой), то значит овчинка стоила выделки, а сколько при этом разрушено нервов и отнято времени у других людей — наплевать. Фактически, наши права потребителей постоянно нарушаются. Но для борьбы с монополиями есть антимонопольное ведомство, для защиты прав потребителей при покупке бракованных товаров есть законы о защите потребителей. Но наше время, которое единственное, что у нас реально есть, не защищено ничем. Каждый выстраивает свои линии защиты. Нет никаких шерифов. Отдельные люди как-то умеют подавать в суд, как-то защищаться от непрошенных звонков, ценой потери какого-то количества реально нужных звонков с незнакомых номеров. Возможно, нужно законодательно запретить роботам делать исходящие звонки. Если уж спам, то пусть его делают реальные люди, а не скрипты. Насколько я понимаю, сейчас операторам связи не выгодно бороться даже с мошенниками и чужими роботами делающими исходящие звонки, так как они платят операторам за исходящий трафик. Не говоря уж о своих роботах, которые не платят ничего за связь. Очевидно, что в этом есть противоречие между интересами общества и операторов связи. Скорее всего, потребуется государственное регулирование, чтобы решить этот вопрос. Если же говорить о потере времени, когда тебе реально что-то нужно от техподдержки, а тебя приветствует «Олег» или «Максим», пока защиты нет. ▍ Как хотелось бы в идеале В идеальном случае, когда робот обладает разумом не ниже человеческого и полномочиями для принятия решений, то общение с ним может быть даже предпочтительнее, чем с человеком, так как, будем честны, в техподдержку идут не доктора наук и профессиональные психологи. У реальных людей из служб поддержки нет безграничного терпения и массы свободного времени. И они часто спят по ночам и работают по графику, который может быть несовместим с вашим. Возможно, с развитием ИИ люди, наоборот, услышав, что говорят с человеком, будут просить соединить с ИИ. Но выбор должен оставаться за человеком. Поэтому роботы должны ясно сообщать, что они роботы, а не пытаться выдать себя за людей. Сейчас мы ещё далеки от всего этого. Возможно, компаниям, хотящим, чтобы их цифровые помощники были востребованы, следует стимулировать людей на общение с ними с помощью предложения различных бонусов, кэшбэков и призов. Тогда человек охотнее станет решать свою проблему при помощи цифрового помощника. Сейчас у людей нет выбора, и чтобы дорваться до человека, нужно пройти целый квест. А если всё-таки пытаешься общаться с таким цифровым помощником, то весь разговор не отпускает ощущение, что это напрасная трата времени, и что в итоге всё равно придётся как-то пробиваться к человеку. Возможно, даже нужен специальный телефонный код, за звонки с которого начисляются деньги абонентам, и с которого звонят только роботы. Или появятся специальные номера техподдержки с операторами-людьми с поминутной оплатой (для клиентов) или специальные платные чаты, в которых общение ведётся только между людьми. Крупный бизнес, который ввёл бы подобные стандарты, получил бы конкурентное преимущество из-за повышенной лояльности клиентов. ▍ Экономия и эксплуатация Многие компании используют роботов в колл-центрах для экономии средств. Тем не менее, в большинстве случаев такие роботы не могут реально решать проблемы, а лишь помогают владельцам бизнеса снижать их затраты. Это зачастую приводит к бесполезному общению, которое, скорее, высасывает время и энергию пользователей, чем приносит им реальную пользу. Исследование Gartner показало, что к 2027 году 25% взаимодействий с клиентами проводятся виртуальными помощниками, однако пока что удовлетворённость клиентов от таких услуг остаётся крайне низкой. RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 131 Карма 0 Рейтинг Мой личный опыт говорит о том, что уже сейчас мне требуется 50% времени тратить на преодоление препятствий, чтобы добраться до человека, который в состоянии решить мою проблему. Экономические стимулы для предпринимателей очень сильны, и внедрение цифровых помощников будет только возрастать со временем. И поэтому и нужен нулевой закон робототехники из этой статьи. ▍ Заключение Нулевой закон робототехники подчёркивает важность прозрачности и честности во взаимодействии между роботами и людьми. Роботы должны всегда представляться как роботы, избегать использования человеческих имён (возможно, обладать набором собственных наименований) и информировать о своих способностях. Это позволит избежать недопонимания, сократить потери времени и энергии пользователей, а также улучшить общее восприятие роботизированных систем. В конечном итоге, такие меры будут способствовать более гармоничному и продуктивному сосуществованию человека и искусственного интеллекта. © 2024 ООО «МТ ФИНАНС» Telegram-канал со скидками, розыгрышами призов и новостями IT 💻 Теги: голосовые роботы, текстовые роботы, боты, чат-боты, закон робототехники, ruvds_статьи Хабы: Блог компании RUVDS.com, IT-стандарты, Будущее здесь, Голосовые интерфейсы,  Законодательство в IT Если эта публикация вас вдохновила и вы хотите поддержать автора — не стесняйтесь нажать на кнопку Задонатить Подписаться Сергей Ю. Каменев @inetstar Алгоритмист. Автор. Поставщик SSD, RAID, серверов. Сайт Сайт Публикации ky0 10 часов назад Ваше мнение очень важно для нас (нет) 3 мин alizar 10 часов назад Глупо покупать технику последней модели, если старая работает хорошо. И можно собрать ПК из комплектующих Простой 6 мин omyhosts 22 часа назад Артефакт эпохи: рождение, взлет и падение клипарта Простой 6 мин ru_vds 6 часов назад Я мучился с Git, поэтому создал про него игру Средний 8 мин Комментарии 113 +113 ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 4.7K Кейс +77 14 112 +112 7.7K Мнение +36 23 42 +42 3.4K Ретроспектива +29 12 22 +22 3.8K Подписаться DRoman0v 11 часов назад Очередная прогулка по барахолке в Испании: отличный ноутбук и много необычных штук 4 мин RationalAnswer 14 часов назад Уголовный кодекс vs Блиновская и Митрошина, а также вебкамщицы со стволами против криптоворов 11 мин Stepan_Burmistrov 9 часов назад Использование лидара от робота-пылесоса для системы предотвращения столкновений в автономных роботах Средний 22 мин ProstoKirReal 11 часов назад Сложно о простом. Как работает интернет. Часть 4. Что такое LAN, MAN, WAN, сети Clos и иерархия операторов Средний 20 мин savpek 9 часов назад От психолога до эльфа 80-го уровня: как создать свою уникальную роль для нейросети в домашних условиях и не сойти с ума Простой 26 мин Кейс Перевод +24 36 5 +5 6.3K +24 3 10 +10 12K Дайджест +23 8 35 +35 2.6K Туториал +20 37 10 +10 5K +19 94 6 +6 1.6K dspmsu 12 часов назад Как я решал задачу 2025 года. Часть 1 Средний 9 мин Показать еще Туториал +17 21 0 1K +14 7 0 ИНФОРМАЦИЯ Сайт ruvds.com Дата регистрации 18 марта 2016 Дата основания 27 июля 2015 Численность 11–30 человек Местоположение Россия Представитель ruvds ССЫЛКИ VPS / VDS сервер от 139 рублей в месяц ruvds.com Дата-центры RUVDS в Москве, Санкт-Петербурге, Казани, Екатеринбурге, Новосибирске, Лондоне, Франкфурте, Цюрихе, Амстердаме, Измире ruvds.com Помощь и вопросы ruvds.com Партнерская программа RUVDS ruvds.com VPS (CPU 1x2ГГц, RAM 512Mb, SSD 10 Gb) — 209 рублей в месяц ruvds.com VPS Windows от 588 рублей в месяц. Попробуйте бесплатно на 3 дня ruvds.com Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2025, Habr VDS в Цюрихе. Дата-центр TIER III — швейцарское качество по низкой цене. ruvds.com Антивирусная защита виртуального сервера. Легкий агент для VPS. ruvds.com VPS в Лондоне. Дата-центр TIER III — английская точность за рубли. ruvds.com VPS с видеокартой на мощных серверах 3,4ГГц ruvds.com ПРИЛОЖЕНИЯ RUVDS Client Приложение для мониторинга и управления виртуальными серверами RUVDS с мобильных устройств. Android iOS ВИДЖЕТ БЛОГ НА ХАБРЕ 6 часов назад Я мучился с Git, поэтому создал про него игру 10 часов назад 3.8K 5 +5 Глупо покупать технику последней модели, если старая работает хорошо. И можно собрать ПК из комплектующих 9 мар в 18:01 Жизнь и смерть Карла Коха: первый хакер на службе КГБ в поисках всемирного заговора и тайны числа 23. Часть 3 9 мар в 14:01 История S.u.S.E. Кульминация 8 мар в 18:01 Ростовская Тоника, электрогитара-легенда из СССР 7.7K 42 +42 4.3K 2 +2 6.3K 5 +5 6.7K 26 +26 "
5,Быстрое начало работы с Gitlab CICD.pdf,Southbridge,Обеспечиваем стабильную работу highload-проектов,280.79,2 часa назад,"Блог компании Southbridge, тестирование it-систем, системное администрирование, туториал","4.29 Оценка 280.79 Рейтинг Southbridge Обеспечиваем стабильную работу highload-проектов Автор оригинала: Seifeldin Mahjoub Перевели статью о создании пайплайна для развертывания статического веб-сайта на AWS S3 Bucket на примере Gitlab CI/CD, чтобы быстро вникнуть в основы технологии и начать применять ее в работе. В статье рассматриваются базовые концепции CI и CD, а также этапы CI/CD-пайплайна.  2 часа назад 5 мин zubarek Быстрое начало работы с Gitlab CI/CD: пайплайн для веб-сайта на AWS S3 Bucket 362 Блог компании Southbridge ,  Тестирование IT-систем* ,  Системное администрирование* ,  Туториал Перевод КАК СТАТЬ АВТОРОМ От автора Мне повезло быть частью некоторых профессиональных команд, каждая из которых применяла несколько DevOps практик. И меня поразило то, как качество кода, скорость разработки и позитивный настрой команды коррелируют с CI/CD-пайплайном. По моему мнению, зрелость пайплайна может служить прекрасным показателем опытности разработчика, качества кода и эффективности всей команды. Во многих случаях, которые я наблюдал, пайплайны были выстроены либо DevOps-инженером, либо отдельной DevOps- командой. Да и последний отчет State of CD 2022 продемонстрировал, что только 22% разработчиков создают пайплайны. Моя цель — увеличить это число: помочь разработчикам взять на себя ответственность за пайплайны, выстраивать непрерывный процесс доставки и создавать качественный код. В статье рассматриваются фундаментальные концепции CI и CD. Что такое CI/CD? Многие бизнесы применяют фреймворки Agile, так как они позволяют менять приоритеты и повышать скорость доставки. Кроме всего прочего, такой подход улучшает атмосферу в команде и помогает увеличить прибыль. Если ваша компания следует по пути Agile, то принятие культуры, философии и практик DevOps станет ее большим преимуществом. Модное словечко последних десятилетий, DevOps сегодня считается настоящим стандартом индустрии. CI/CD — это практика DevOps, которая помогает разработчикам ПО доставлять изменения в коде с высокой частотой и надежностью. «Быстрый билд, быстрый тест, быстрый фейл» При наличии автоматизированных тестов команды тяготеют к общей автоматизации задач и частым, надежным поставкам кода. Создание CI/CD-пайплайна в этом случае может привести к нескольким преимуществам. Бизнес выигрывает от снижения затрат и повышения производительности, ускорения Time to Market и адаптации к изменяющимся требованиям рынка. Команда выигрывает от быстрой обратной связи, улучшения эффективности разработки, уменьшения количества бутылочных горлышек и повышения уровня вовлеченности и удовлетворенности сотрудников. Фазы CI и CD CI — непрерывная интеграция. Непрерывная интеграция позволяет по много раз в день коммитить изменения в основную ветку вашей кодовой базы. Учитывая ограниченные когнитивные способности человека, CI стимулирует разработчиков вносить в код небольшие изменения, которые легче рассмотреть, покрыть автоматическими тестами и часто релизить. Это позволяет избежать напряженных и переполненных merge conflict-ами дней подготовки к релизу с тоннами ручного тестирования. CD — непрерывная доставка. Следующий шаг после CI позволяет гарантировать, что кодовая база постоянно готова к деплою, а задеплоить ее можно одним нажатием кнопки. При этом неважно, с чем вы работаете: с масштабной распределенной системой, сложной производственной средой и т. д. Ключевой момент — автоматизация. CD — непрерывное развертывание. Последний этап зрелого CI/CD-пайплайна, где все изменения в коде автоматически развертываются в продакшн без ручного вмешательства. Само собой, для этого требуется большое количество хорошо продуманных автоматических тестов. State of CD 2022 утверждает, что «47% разработчиков применяют CI или СD, но только один из пяти использует оба подхода для автоматизации сборки, тестирования и развертывания кода». Книга Accelerate подводит итоги многолетнего исследования с использованием отчетов State of DevOps, основанных на 23 000 наборов данных компаний по всему миру. Как видите, высокопроизводительные команды могут деплоить по требованию (или несколько раз в день). Этапы CI/CD-пайплайна Стадия исходного кода — здесь запускается пайплайн. Обычно это происходит после изменений в Git-репозитории, которые проявляются в открытии нового Pull Request-а или в пуше в ветку. Другой способ заключается в настройке инструментария CI/CD для запуска пайплайна через автоматическое расписание или после запуска другого пайплайна. Стадия сборки — этап, в процессе которого происходит проверка и сборка кода. Здесь особенно полезны такие инструменты, как Docker: они обеспечивают однородную среду. Стадия тестирования — CI/CD невозможно представить без автоматизированных тестов. В конце концов, все хотят быть уверены, что изменения в коде не сломают продакшн. Стадия развертывания — на последнем этапе (после успешного прохождения всех предыдущих стадий) код можно развернуть в выбранной среде. Пример с Gitlab В этом примере будет использован Gitlab CI/CD, однако концепции аналогичны и для остальных инструментов, поэтому их можно применить к другим сервисам хостинга репозиториев. Существует несколько инструментов CI/CD, например всемирно известный Jenkins. Этот инструмент требует некоторой настройки и конфигурации, в то время как другие поставляются сервисами хостинга репозиториев (такими как GitHub Actions и Bitbucket Pipelines) с предварительной настройкой. Поэтому если ваш код размещен на Gitlab, то легче всего использовать Gitlab CI/CD, поскольку код и управление CI/CD находятся на одной платформе. Как все это может работать без настроек? Для ответа на этот вопрос стоит немного погрузиться в архитектуру Gitlab, а именно — в инстансы и раннеры. Инстансы хранят код приложения и конфигурации пайплайна. Раннеры выступают в качестве агентов, выполняющих операции в пайплайнах. В Gitlab каждый инстанс может быть подключен к одному или нескольким раннерам. Gitlab.com — это управляемый инстанс с несколькими раннерами, которые сам Gitlab и поддерживает. Следовательно, если вы используете этот инстанс, то получаете все необходимое из коробки. Приступим к работе Gitlab предлагает несколько шаблонов при создании нового проекта. Конфигурация пайплайна Gitlab CI/CD по умолчанию находится в файле .gitlab-ci.yml в корневом каталоге. Предположим, мы хотим создать простой пайплайн, который проверяет: написан, протестирован и развернут ли код. Вот несколько концепций и терминов для ознакомления перед началом работы. Пайплайн (Pipeline) Пайплайн — это набор заданий, разделенных на этапы. Gitlab предлагает различные типы пайплайнов, например parent-child или multi-project. Полный список см. здесь. Этап (Stage) Этап — это шаг в пайплайне, предоставляющий информацию о том, какие задания запускать (сборка, тестирование и т. д.). Один этап может включать одно или несколько заданий. Задание (Job) Задание — основной блок пайплайна (компиляция, линтинг и т. д.). Для каждого задания должны быть определены name и script. После выполнения всех заданий на этапе пайплайн переходит к следующему. Теперь — к коду Выстраиваем пайплайн Gitlab CI/CD, который собирает, тестирует и разворачивает статический веб-сайт в AWS S3 Bucket. Для начала создадим новый .gitlab-ci.yml 1. Определим переменные 2. Определим этапы 3. Определим задания на каждом этапе variables: # variabiles definitions for easier reuse of valu  CI_NODE_IMAGE: ""node:16.13.2"" # Pipeline stages stages:  - install  - build  - test  - deploy #install job definition install:  stage: install  image: ""$CI_NODE_IMAGE"" # variable reference  script: # Shell script that is executed by the runner.    - npm ci  cache: # List of files that should be cached between subseq    key:      files:        - package.json        - package-lock.json    paths: # directories to cache      - node_modules # Build Job definition На этом все, спасибо за внимание. Научиться работать с пайплайнами, билдами и артефактами можно на курсе Gitlab CI/CD в Слёрм. Вы узнаете, из чего состоит Gitlab и какие у него возможности и настройки, а также разберете лучшие практики построения пайплайна, особенности шаблонизации и работы с переменными. build:  stage: build  image: $CI_NODE_IMAGE  script:    - npm run build  artifacts: # list of files and directories that are attache    paths:      - dist/  cache:    key:      files:        - package.json        - package-lock.json    paths:      - node_modules    policy: pull # Test Job definition test:  stage: test  image: $CI_NODE_IMAGE  script:    - npm run test # Deploy Job definition deploy:  stage: deploy  image: registry.gitlab.com/gitlab-org/cloud-deploy/aws-base  script:    - aws s3 cp --recursive dist s3://bucket-name # copies th Southbridge Обеспечиваем стабильную работу highload-проектов Сайт Сайт 14 Карма 16.9 Рейтинг Лиза Зубарькова @zubarek Пользователь Комментировать Публикации Видеокурс доступен всегда. Посмотреть программу: https://slurm.club/3JUKdzT Теги:   ci/cd , gitlab-ci , aws , gitlab , pipeline Хабы:   Блог компании Southbridge , Тестирование IT-систем , Системное администрирование , Программирование , DevOps +7 14 0 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ Ваш аккаунт Разделы Информация Услуги  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт southbridge.io Дата регистрации 15 ноября 2012 Дата основания 22 февраля 2008 Численность 51–100 человек Местоположение Россия Представитель Антон Скобин Войти Регистрация Публикации Новости Хабы Компании Авторы Песочница Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
6,Генеральный директор Mozilla покинула свой пост _ Хабр.pdf,-,-,0.0,12 минут назад,-,"Митчелл Бейкер, гендиректор Mozilla с 2020 года, объявила, что покидает свой пост и возвращается на должность председателя совета директоров Mozilla Corporation, которую она занимала ранее. Временным генеральным директором компании станет член правления Лора Чемберс. «За 25 лет работы в Mozilla я побывала на многих должностях. Мой теперешний шаг вызван желанием сосредоточить внимание на предстоящих задачах. Я руководила бизнесом Mozilla в период преобразований, а также курировала миссию Mozilla в более широком смысле. Стало очевидно, что оба направления требуют преданного и постоянного руководства», — заявила Бейкер в сообщении в блоге Mozilla. Митчелл работает в Mozilla с эпохи Netscape Communications. Она основала Mozilla Foundation и учредила лицензию Mozilla Public License. AnnieBronson 12 минут назад Генеральный директор Mozilla покинула свой пост 1 мин 107 IT-компании, Управление персоналом*, Карьера в IT-индустрии КАК СТАТЬ АВТОРОМ Зарплаты IT-специалистов Новый год → новая жизнь → новые с… Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп 141 Карма 145.1 Рейтинг @AnnieBronson Информационная служба Хабра Реклама Комментарии Здесь пока нет ни одного комментария, вы можете стать первым! Другие новости AnnieBronson 59 минут назад Лора Чемберс в должность генерального директора до конца этого года. Она входила в состав совета директоров Mozilla в течение трёх последних лет, а до этого работала в Airbnb, PayPal, eBay и Willow Innovations. По словам Бейкер, внимание Лоры « будет сосредоточено на предоставлении успешных продуктов, продвигающих миссию компании, и создании платформ, ускоряющих её развитие».  «Мы находимся на критическом этапе, когда общественное доверие к институтам, правительствам и Интернету достигло беспрецедентно низкого уровня. Происходит тектонический сдвиг: каждый борется за будущее ИИ. Это возможность и необходимость для Mozilla создать лучшее будущее», — заключила Бейкер. Теги:  mozilla, mozilla foundation Хабы:  IT-компании, Управление персоналом, Карьера в IT-индустрии Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта Только полноправные пользователи могут оставлять комментарии. Войдите, пожалуйста. ai.sbergraduate.ru РЕКЛАМА Вакансии для студентов без опыта работы Sberseasons: подай заявку на стажировку по IT и другим специальностям SpaceX планирует использовать беспилотные баржи в качестве морских интернет-станций для Starlink 2 мин SLY_G 3 часа назад Биологи научились регулировать кислотность клеток человека при помощи света 3 мин CloudMTS 3 часа назад Облачный дайджест: затраты на VDI растут, SAP закрывает доступ, дата-центр строят на 3D-принтере 2 мин Cloud4Y 4 часа назад Что делает тихоходок настолько живучими? Новое исследование указывает на свободные радикалы 6 мин IgnatChuker 5 часов назад В Австралии приняли в первом чтении закон о праве работника игнорировать рабочие звонки и сообщения в нерабочее время 2 мин AnnieBronson 5 часов назад Microsoft изменила интерфейс Copilot для Windows, чтобы сервис по умолчанию запускался с большим окном чата 1 мин 301 +5 0 0 448 +6 0 0 633 +1 0 0 2.8K +5 4 3 +3 3.2K +23 8 29 +29 1.6K avouner 5 часов назад Российские банки выступили против оборотных штрафов до 500 млн рублей за утечки данных 2 мин LizzieSimpson 5 часов назад Disney за $1,5 млрд купит долю в Epic Games 1 мин daniilshat 5 часов назад Безопасность конфигураций веб-сервера Angie PRO теперь контролирует X-Config 1 мин Travis_Macrif 5 часов назад Министерство внутренней безопасности США создало новое подразделение под названием AI Corps 2 мин Показать еще МИНУТОЧКУ ВНИМАНИЯ 0 0 7 +7 2.2K +5 1 7 +7 874 +5 0 3 +3 183 +4 0 0 490 +3 0 2 +2 Интересно Интересно  Турбо ВОПРОСЫ И ОТВЕТЫ Есть ли плагин-""мусорка"" закладок для браузера Огнелис? Mozilla · Простой · 1 ответ Где скачать русскоязычную модель для TTS от Mozilla? Mozilla · Простой · 1 ответ Что делать, если сайт некорректно отображается в Mozilla FireFox? Mozilla · Простой · 1 ответ Огромные лаги в интерфейсе Firefox. Как исправить? Mozilla · 2 ответа Почему не работает слайдер jquery.jcarousel.js в браузерах Фаерфокс и ИЕ? Mozilla · 0 ответов Больше вопросов на Хабр Q&A Зарплаты IT-специалистов во второй половине 2023 Ах, этот скидочный снегопад: поймай свою снежинку Хабракалендарь, отворись! Какие IT-ивенты ждут нас в 2024 Реклама ЧИТАЮТ СЕЙЧАС Тинькофф банк вернул 200 тысяч. Это был не дипфейк Люди не понимают ООП В России госведомства начали объявлять тендеры на уничтожение техники Apple Yandex N.V. запретили в течение пяти лет создавать аналогичные «Яндексу» сервисы «МегаФон» повысил цены на связь для некоторых абонентов, так как они от этого не отказались 5.8K 26 +26 50K 247 +247 45K 218 +218 13K 25 +25 8.8K 52 +52 sberbank.com Кешбэк до 10% бонусами СберСпасибо — сервис СберПремьер 4,6 Рейтинг организации Акция для владельцев. Оформите онлайн пакет услуг и получайте выгоду от покупок Оформить Выделенная линия Узнать больше РЕКЛАМА Знаете, что поменять в обучении ИБ? Рассказывайте! Спецпроект ИСТОРИИ БЛИЖАЙШИЕ СОБЫТИЯ Реклама Хорошие статьи из блогов компаний Зарплаты в IT во второй половине 2023 Учим английский Не работай через силу Позовите автора! Конк Хабр Как использовать облачные платформы Сloud.ru: серия демо- встреч по четвергам  1 – 29 февраля  11:00 Онлайн Подробнее в календаре Конкурс грантов на обучение IT-профессиям от Хекслета  5 – 29 февраля Онлайн Подробнее в календаре Открытый урок «Behaviour Tree в   8 февраля  19 Онлайн Подробнее в календаре ADRIVER Ваш аккаунт Войти Регистрация Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2024, Habr "
7,За что безопасники будут гореть в аду_ _ Хабр.pdf,Простой,-,0.0,46 минут назад,-,"Для привлечения внимания расскажу историю. Уже много лет живу далеко за пределами нашей всеми любимой родины. И на днях, понадобилось мне войти в старую почту gmail... Логин и пароль надежно сохранены. Однако Google не торопится впускать нас в собственную почту. — Нам кажется что это не вы, подтвердите что это вы. Введите номер телефона когда-то использовавшийся при регистрации. Что ж, и это можно. Ввожу номер. — Увы, мы не можем отправить СМС на этот номер. Хотите завести другой аккаунт? Итак, все данные есть, однако подтвердить что это вы по техническим проблемам самого сервиса невозможно. Связанные аккаунты, старая переписка, и файлы в облаке утрачены навсегда. Какое-то знакомое чувство, не так ли? Другой аккаунт Google, прикрепленного номера телефона нет и не было. Подтвердите что это вы. Выберите способ подтверждения, Увы, доступных способов подтверждения нету. Завести новый аккаунт? Если вы уже начали понимать к чему клоню, то следующая история вам понравится еще больше. Находясь в путешествии в дальних заморских странах, понадобилось три тысячи долларов снять для оплаты жилья. Из приложения банка делаю перевод на свое собственное имя... и вуаля, вы заблокированы! Войти в приложение нельзя для вашей собственной безопасности! Пишу в поддержку. — Нам кажется что вы это не вы, надо подтвердить что это вы. Укажите кодовое слово, номер того-и-сего. Указываю. Прошу разблокировать банк и сделать транзакцию. В поддержку сейчас пишет много людей. Ответ может занять неопределенное время. Подождите пожалуйста подольше. Мы делаем все для клиентов! Так проходит еще час. И поверьте, имена брендов в этой истории совершенно не важны. Не потому, что хабр не жалобная книга, а потому что раком безопасности заражена вся индустрия, не осталось ни одного захудалого приложения для заказа пиццы, не построившего бы пароноидально-шизофреническую систему входов-подтверждений. manul 46 минут назад За что безопасники будут гореть в аду? Простой 4 мин 466 Разработка мобильных приложений*, Разработка облачных платформ, Исследования и прогнозы в IT*, Программи Конкурс рассказов Открываем сезон футурологии с крутыми призами РЕКЛАМА КАК СТАТЬ АВТОРОМ Зарплаты айтишников Huawei Mate 70 Pro: тест нови… 07.03.2025, 08:20 За что безопасники будут гореть в аду? / Хабр https://habr.com/ru/articles/888708/ 1/11 Из опыта, на посошок. Еще одна страна, еще один банк, еще одна сим-карта для подтверждений, ведь других вариантов подтверждений банки сейчас не предлагают. Дальнейшее путешествие. И вот сим-карта превращается в тыкву в роуминге. А следом за ней и банк. Поддержка банка говорит что в целях безопасности без подтверждений по СМС никак нельзя. Небезопасно. Достучаться до поддержки оператора в свою очередь невозможно. Надо проходить безумный квест ""нажмите цифру"", а затем победить электронного помощника-дебила. После этого услышать ответ по скрипту от человека, у нас проблем не зафиксировано. Все для безопасности! Божок безопасности доволен! Итак, это новая болезнь нашего мира, и она охватывает все сферы жизни. Она страшнее, ближе, и опаснее, чем государственные регуляции. Из хозяев своих данных, под благовидным предлогом, мы превратились в рабов безопасности всевозможных сервисов, в том числе частных. Теперь не безопасность служит нам, а мы ей. А придумали, создали и внедрили эту безумную систему подтверждений, вторых, пятых и десятых факторов, настрочив об этом тонны статей, получив множество придуманных самостоятельно ачивок, никто иные, как наши прибившиеся ""коллеги"" по IT-индустрии, так называемые безопасники. Напомню, самые страшные болезни заходят через системы безопасности организма. Рак, СПИД, и аутоиммунные заболевания. И это произошло с нашей IT-индустрией, со всеми веб-сервисами и мобильными приложениями. Мы заражены. Мы больше не хозяева своих денег и данных. Все может накрыться медным тазом в произвольный момент. Логин и пароль были слишком простым способом входа? Кто-то ставит слабые пароли! И ради защиты детей тупых придумали второй фактор. Что может быть надежнее для этого, чем кусочек пластика который выходит из строя, невозможно восстановить в другой стране, и находится в зависимости от дополнительной, по сути, левой компании- оператора? Разве что e-sim, которые помимо такой же зависимости от глючных операторов в роуминге, так еще и превращаются в тыкву с поломкой телефона. Разбился экран в другой стране, добро пожаловать в бомжи! Но и этого религиозным фанатикам показалось мало, и они добавили алгоритмы определения входа из необычных мест, устройств и так далее... А что, сидите дома, холопы, это безопаснее! Эталонный клиент современных мер безопасности, это слабоумный, пускающий слюни, с трясущимися руками старик, забывающий все на свете, не помнящий как его зовут, и никогда не выходящий на прогулку дальше пяти метров от дома. Индустрия безопасности и безопасников, которая якобы должна спасать нас, сама превратилась в гигантский источник опасности, который ввергает нас в рабство, настоящую зависимость от странных и ненадежных факторов, технических проблем третьих компаний, прихоти ""умных"" алгоритмов, лени техподдержки, фазы луны. Конкурс рассказов Открываем сезон футурологии с крутыми призами 07.03.2025, 08:20 За что безопасники будут гореть в аду? / Хабр https://habr.com/ru/articles/888708/ 2/11 Где-то мы это уже слышали, не правда ли? Безопасники частных компаний просто повторили путь своих старших государственных братишек менее чем за пару десятилетий. Логин и пароль это лучшее что было. Логин и пароль надежно и удобно хранятся в специализированных БД, приложениях вроде KeePass, и так далее. Безопаснее с дополнительными факторами не стало. Стало страшнее и опаснее за хрупкие и ненадежные дополнительные факторы, которые периодически запрашивают. Вспомним, вторым фактором может выступать ключевая фраза, ответ на вопрос про любимое блюдо и так далее, практика, которую злобные гиганты намеренно ввергли в забытье. Я призываю здравомыслящих людей разбить оковы рабства. Давайте будем создавать приложения ради свободы, а не безопасности. Свобода или безопасность? 61.54% Свобода 8 38.46% Безопасность 5 Проголосовали 13 пользователей. Воздержались 4 пользователя. Теги: свобода, разработка, разработка приложений,  разработка программного обеспечения, разработка сайтов, облачные сервисы,  облачная инфраструктура Хабы: Разработка мобильных приложений, Разработка облачных платформ,  Исследования и прогнозы в IT, Программирование, IT-компании Только зарегистрированные пользователи могут участвовать в опросе. Войдите, пожалуйста. Редакторский дайджест Присылаем лучшие статьи раз в месяц +7 0 1 Конкурс рассказов Открываем сезон футурологии с крутыми призами 07.03.2025, 08:20 За что безопасники будут гореть в аду? / Хабр https://habr.com/ru/articles/888708/ 3/11 "
8,ИИ-агенты в Альфа-Банке_ нейросети создают автотесты без участия человека _ Хабр.pdf,Альфа-Банк,Лучший мобильный банк по версии Markswebb,404.32,2 часa назад,"Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем","404.32 Рейтинг Альфа-Банк Лучший мобильный банк по версии Markswebb Подписаться В Альфа-Банке мы внедрили ИИ-агентов, которые проектируют, разрабатывают и проверяют автотесты. При этом полностью автономно, как QA-инженеры, но в разы быстрее и точнее. Подобных примеров, когда ИИ разрабатывает автотесты от анализа требований до пул-реквеста, в нашей стране, пожалуй, ещё не было. Что умеют агенты? Анализировать контекст из Jira и Confluence, вычленяя суть задачи. Прогнозировать риски, зависимости и даже «пограничные» сценарии. Генерировать DTO для REST API и превращать ручные сценарии в Java-тесты за минуты. Сверять код с бизнес-логикой и техстандартами Альфы, защищая прод от случайных ошибок. Создавать вариативные проверки — от позитивных кейсов до сложных негативных условий. Автоматизировать рутину. В ИИ-команде QA есть несколько агентов, каждый работает над своей частью из перечная выше. Сейчас решение пилотируется в нескольких продуктовых командах, но результаты уже видны: AlfaTeam 2 часа назад ИИ-агенты в Альфа-Банке: нейросети создают автотесты без участия человека 1 мин 717 Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем* Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Зарплаты айтишников Миграция за ночь? Легко! Без сна и с адре… Альфа-Банк Лучший мобильный банк по версии Markswebb Сайт Хабр Карьера 14 Карма 0.3 Рейтинг @AlfaTeam Пользователь Сайт Facebook ВКонтакте Другие новости mefdayy 19 минут назад Nvidia и Broadcom тестируют передовой процесс производства чипов Intel 2 мин меньше ошибок в проде, предсказуемые дедлайны и высвобожденные ресурсы для творческих задач. «Одна команда ИИ-агентов экономит десятки часов работы, увеличивает скорость релизов и находит на 30% больше багов» Святослав Соловьев, Директор по генеративному ИИ в ИТ Альфа-Банка. В ближайшее время подробнее опишем как устроены агенты, какие технологии используем и как мы измеряем их эффективность. Теги: автоматизация, искусственный интеллект, тестирование, автотесты Хабы: Блог компании Альфа-Банк, Искусственный интеллект, Тестирование IT-систем Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта Комментарии 11 123 Подписаться "
9,"Инструменты наблюдаемости, о которых нужно знать в 2023 году.pdf",RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2376.9,4 часa назад,"Блог компании RUVDS.com, open source, хранение данных, облачные сервисы, мик","2376.9 Рейтинг RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Автор оригинала: Lahiru Hewawasam  4 часа назад Простой9 мин ru_vds Инструменты наблюдаемости, о которых нужно знать в 2023 году 752 Блог компании RUVDS.com ,  Open source* ,  Хранение данных* ,  Облачные сервисы* ,  Мик Перевод КАК СТАТЬ АВТОРОМ Когда организации переходят в облако, их системы тоже начинают стремиться к распределённым архитектурам. Один из самых распространённых примеров этого — использование микросервисов. Однако это также создаёт новые сложности с точки зрения наблюдаемости. Необходимо подбирать подходящие инструменты для мониторинга, отслеживания и трассировки этих систем при помощи анализа выходных результатов посредством метрик, логов и трассировок. Это позволяет командам разработчиков быстро выявлять первопричины проблем, устранять их и оптимизировать производительность приложений, ускоряя выпуск кода. В этой статье мы рассмотрим возможности, ограничения и важные особенности одиннадцати популярных инструментов наблюдаемости, что позволит вам выбрать наиболее подходящий для вашего проекта. Helios Helios — это решение по обеспечению наблюдаемости для разработчиков, предоставляющее информацию по всему потоку приложений. Оно включает в себя фреймворк распространения контекста OpenTelemetry и обеспечивает наблюдение за микросервисами, serverless-функциями, базами данных и сторонними API. Можно протестировать песочницу продукта или использовать его бесплатно, зарегистрировавшись здесь. ▍ Основные возможности • Обеспечение полного контроля: Helios предоставляет информацию о распределённой трассировке в полном контексте, показывает, как передаются данные через всё приложение в любом окружении. • Визуализация: позволяет пользователям собирать и визуализировать данные трассировок из множественных источников данных, чтобы исследовать и устранять потенциальные проблемы. • Многоязыковая поддержка: поддерживает множество языков и фреймворков, в том числе Python, JavaScript, Node.js, Java, Ruby, .NET, Go, C++ и Collector. • Обмен и многократное использование: вы с лёгкостью можете сотрудничать с участниками команды, обмениваясь через Helios трассировками, тестами и триггерами. Кроме того, Helios позволяет многократно использовать запросы и полезные нагрузки между участниками команды. • Автоматическая генерация тестов: автоматически генерирует тесты на основании данных трассировок. • Простота интеграций: интегрируется в существующую экосистему, включая логи, тесты, мониторинг ошибок и многое другое. • Воссоздание процессов: Helios всего за несколько кликов позволяет в точности воссоздавать рабочие процессы, в том числе HTTP-запросы, сообщения Kafka и RabbitMQ, а также вызовы Lambda. ▍ Популярные способы использования • Распределённая трассировка • Интеграция трассировок в многоязыковое приложение • Наблюдаемость serverless-приложения • Устранение неполадок в тестах • Информация о вызовах API • Анализ и выявление узких мест Prometheus Prometheus — это опенсорсный инструмент, широко используемый для обеспечения наблюдаемости в нативных облачных окружениях. Он может собирать и хранить данные временных последовательностей и предоставляет инструменты визуализации для анализа собранных данных. ▍ Основные возможности • Сбор данных: он может скрейпить метрики из различных источников, в том числе из приложений, сервисов и систем. Также он «из коробки» поддерживает множество форматов данных, в том числе логи, трассировки и метрики. • Хранилище данных: он сохраняет собранные данные в базе данных временных последовательностей, позволяя эффективно запрашивать и агрегировать данные с течением времени. • Система алертов: инструмент включает в себя встроенную систему алертов, которая может запускать алерты на основании запросов. • Исследование сервисов: он может автоматически распознавать и скрейпить метрики сервисов, работающих в различных окружениях, например, Kubernetes и в других системах управления контейнерами. • Интеграция с Grafana: инструмент имеет гибкую интеграцию с Grafana, позволяющей создавать дэшборды для отображения и анализа метрик Prometheus. ▍ Ограничения • Ограниченные возможности анализа первопричин: инструмент в первую очередь предназначен для мониторинга и алертов. Поэтому он не предоставляет встроенных возможностей аналитики первопричин проблем. • Масштабирование: хотя инструмент может обрабатывать множество метрик, это может привести к большой трате ресурсов, поскольку Prometheus хранит все данные в памяти. • Моделирование данных: содержит модель данных на основе пар «ключ-значение» и не поддерживает вложенных полей и join. ▍ Популярные способы применения • Сбор и хранение метрик • Система алертов • Исследование сервисов Grafana Grafana — это опенсорсный инструмент, в первую очередь используемый для визуализации и мониторинга данных. Он позволяет с лёгкостью создавать интерактивные дэшборды для визуализации и анализа данных из различных источников. ▍ Основные возможности • Визуализация данных: создаёт настраиваемые и интерактивные дэшборды для визуализации метрик и логов из различных источников данных. • Система алертов: позволяет настраивать алерты на основании состония метрик для информирования о потенциальных проблемах. • Выявление аномалий: позволяет настроить выявление аномалий для автоматического определения и отправки алертов в случае аномального поведения в метриках. • Анализ первопричин: позволяет углубиться в метрики для анализа первопричин, предоставляя подробную информацию с историческим контекстом. ▍ Ограничения • Хранение данных: архитектура инструмента не поддерживает долговременное хранение и для сохранения метрик и логов требует дополнительных инструментов наподобие Prometheus или Elasticsearch. • Моделирование данных: Grafana не предоставляет расширенных возможностей моделирования данных. То есть она не позволяет моделировать конкретные типы данных и выполнять сложные запросы. • Агрегирование данных: Grafana не содержит встроенных функций агрегирования. ▍ Популярные способы применения • Визуализация метрик • Система алертов • Выявление аномалий Elasticsearch, Logstash и Kibana (ELK) Стек ELK — это популярное опенсорсное решение, помогающее управлять логами и анализировать данные. Оно состоит из трёх компонентов: Elasticsearch, Logstash и Kibana. Elasticsearch — это движок распределённого поиска и аналитики, способный обрабатывать большие объёмы структурированных и неструктурированных данных; он позволяет хранить и индексировать большие массивы данных, а также выполнять поиск по ним. Logstash — это конвейер сбора и обработки данных, позволяющий собирать, обрабатывать и обогащать данные из множества источников, например, файлов логов. Kibana — это инструмент визуализации и исследования данных, позволяющий создавать интерактивные дэшборды и визуализации на основе данных, находящихся в Elasticsearch. ▍ Основные возможности • Управление логами: ELK позволяет собирать, обрабатывать, хранить и анализировать данные логов и метрики из множества источников, предоставляя централизованную консоль для поиска по логам. • Поиск и анализ: позволяет выполнять поиск и анализ релевантных данных логов, что критически важно для выявления и устранения первопричин проблем. • Визуализация данных: Kibana позволяет создавать настраиваемые дэшборды, которые способны визуализировать данные логов и метрики из множества источников данных. • Выявление аномалий: Kibana позволяет создавать алерты для аномальной активности в данных логов. • Анализ первопричин: стек ELK позволяет глубоко изучать данные логов, чтобы лучше понять первопричины, предоставляя подробные логи и исторический контекст. ▍ Ограничения • Трассировка: ELK нативно не поддерживает распределённую трассировку. Поэтому может понадобиться применение дополнительных инструментов наподобие Jaeger. • Мониторинг в реальном времени: архитектура ELK позволяет ему качественно выполнять задачи платформы управления логами и анализа данных. Однако в отчётности логов присутствует незначительная задержка, и пользователям приходится ждать. • Сложная настройка и поддержка: для платформы требуется сложный процесс настройки и поддержки. Кроме того, для управления большими объёмами данных и множественными источниками данных требуются специфические знания. ▍ Популярные способы применения • Управление логами • Визуализация данных • Комплаенс и безопасность InfluxDB и Telegraf InfluxDB и Telegraf — это опенсорсные инструменты, популярные благодаря своим возможностям по хранению и мониторингу данных временных последовательностей. InfluxDB — это база данных временных последовательностей, хранящая большие объёмы данных временных последовательностей и выполняющая запросы к ним при помощи своего языка запросов, напоминающего SQL. Telegraf — это хорошо известный агент сбора данных, способный собирать и отправлять метрики широкому выбору получателей, например, InfluxDB. Также он поддерживает многие источники данных. ▍ Основные возможности Комбинация из InfluxDB и Telegraf предоставляет множество возможностей, повышающих наблюдаемость приложений. • Сбор и хранение метрик: Telegraf позволяет собирать метрики из множества источников и отправлять их в InfluxDB для хранения и анализа. • Визуализация данных: InfluxDB можно интегрировать со сторонними инструментами визуализации наподобие Grafana для создания интерактивных дэшбордов. • Масштабируемость: архитектура InfluxDB позволяет обрабатывать большие объёмы данных временных последовательностей и выполнять горизонтальное масштабирование. • Поддержка множества источников данных: Telegraf поддерживает более двухсот плагинов ввода для сбора метрик. ▍ Ограничения • Ограниченные возможности алертинга: в обоих инструментах отсутствуют возможности алертинга и для его подключения необходима интеграция сторонних сервисов. • Ограниченный анализ первопроичин: в этих инструментах отсутствуют нативные возможности анализа первопричин и необходима интеграция сторонних сервисов. ▍ Популярные способы применения • Сбор и хранение метрик • Мониторинг Datadog Datadog — это популярная облачная платформа для мониторинга и аналитики. Она широко используется для получения информации о здоровье и производительности распределённых систем с целью заблаговременного устранения проблем. ▍ Основные возможности • Многооблачная поддержка: пользователи могут выполнять мониторинг приложений, работающих на облачных платформах нескольких поставщиков, например, AWS, Azure, GCP и так далее. • Карты сервисов: позволяют выполнять визуализацию зависимостей сервисов, местоположений, сервисов и контейнеров. • Аналитика трассировок: пользователи могут анализировать трассировки, предоставляя подробную информацию о производительности приложений. • Анализ первопричин: позволяет глубоко изучать метрики и трассировки, чтобы понять первопричину проблем, предоставляя подробную информацию с историческим контекстом. • Выявление аномалий: может настраивать систему выявления аномалий, которая автоматически выявляет аномальное поведение в метриках и создаёт алерты о нём. ▍ Ограничения • Затраты: Datadog — это облачный платный сервис, стоимость которого увеличивается при развёртывании крупномасштабных систем. • Ограниченная поддержка потребления, хранения и индексации логов: Datadog по умолчанию не предоставляет поддержку анализа логов. Необходимо отдельно приобретать поддержку потребления и индексации логов. Поэтому большинство организаций принимает решение хранить ограниченное количество логов, что может вызвать неудобства при устранении проблем, поскольку отсутствует доступ к полной истории проблемы. • Нехватка контроля за хранением данных: Datadog хранит данные на собственных серверах и не позволяет пользователям хранить данные локально или в дата-центрах компании. ▍ Популярные способы применения • Конвейеры наблюдаемости • Распределённая трассировка • Мониторинг контейнеров New Relic New Relic — это облачная платформа мониторинга и аналитики, позволяющая выполнять мониторинг приложений и систем в распределённом окружении. Она использует сервис «New Relic Edge» для распределённой трассировки и способна выполнять наблюдение за 100% трассировок приложения. ▍ Основные возможности • Мониторинг производительности приложений: предоставляет комплексное решение APM для мониторинга производительности приложений и устранения проблем. • Многооблачная поддержка: поддерживает мониторинг приложений на облачных платформах нескольких поставщиков, например, AWS, Azure, GCP и так далее. • Аналитика трассировок: позволяет анализировать трассировки, предоставляя подробную информацию о производительности системы и приложений. • Анализ первопричин: позволяет глубоко изучать матрики и трассировки для анализа первопричин проблем. • Управление логами: собирает, обрабатывает и анализирует данные логов из различных источников, обеспечивая всеобъемлющую картину логов. ▍ Ограничения • Ограниченная опенсорсная интеграция: New Relic — это платформа с закрытыми исходниками, поэтому её интеграция с опенсорсными инструментами может быть ограниченной. • Затраты: New Relic может быть более дорогим по сравнению с другими решениями при работе с крупномасштабными системами. ▍ Популярные способы применения • Мониторинг производительности приложений • Многооблачный мониторинг • Аналитика трассировок AppDynamics AppDynamics — это платформа мониторинга и аналитики, позволяющая отслеживать и визуализировать каждый компонент приложения, а также управлять ими. Кроме того, она позволяет выполнять анализ первопричин для выявления внутренних проблем, которые могут влиять на производительность приложения. ▍ Основные возможности • Сбор данных: пользователи могут собирать метрики и трассировки из множества источников: хостов, контейнеров, облачных сервисов и приложений. • Выявление аномалий: позволяет настраивать систему выявления аномалий, которая выявляет аномальное поведение и сообщает о нём при помощи алертов. • Аналитика трассировок: пользователи могут анализировать трассировки и получать подробную информацию о производительности. Мониторинг производительности приложений: предоставляет комплексное решение APM, позволяющее выполнять мониторинг и устранение проблем производительности приложения. • Ограниченные возможности настройки: по сравнению с другими инструментами, опции настройки не очень гибки, потому что пользователи не могут настраивать решение самостоятельно. ▍ Популярные способы применения • Мониторинг производительности приложений • Многооблачный мониторинг • Управление бизнес-транзакциями Выбор лучшего инструмента наблюдаемости Наблюдаемость — неотъемлемая часть разработки и эксплуатации современного ПО. Она помогает компаниям выполнять мониторинг здоровья и производительности систем и быстро решать проблемы ещё до того, как они станут критичными. В этой статье мы рассказали об одиннадцати лучших инструментах наблюдаемости, о которых должны знать разработчики при работе с распределёнными системами. Как видите, каждый инструмент имеет свои сильные стороны и ограничения. Поэтому чтобы найти подходящий для вас инструмент, следует сравнить его с требованиями вашей системы. Выбор наилучшего инструмента наблюдаемости для вашей организации зависит от конкретных потребностей, таких, как окружение, технологический стек, опыт разработчиков, профили пользователей, требования к мониторингу и устранению проблем, а также рабочие процессы. Надеюсь, эта статья была для вас полезной. Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх  ️ RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 327 Карма 576 Рейтинг @ru_vds Пользователь Комментарии 3 Публикации Теги:   ruvds_перевод , наблюдаемость , observability , graphana , prometheus , helios , логи , метрики , трассировка Хабы:   Блог компании RUVDS.com , Open source , Хранение данных , Облачные сервисы , Микросервисы +9 21 3 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ Ваш аккаунт Разделы Информация Услуги  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт ruvds.com Дата регистрации 18 марта 2016 Дата основания 27 июля 2015 Численность 11–30 человек Местоположение Россия Представитель ruvds Войти Регистрация Публикации Новости Хабы Компании Авторы Песочница Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
10,История российской науки_ напишем вместе_ _ Хабр.pdf,Хабр,"Экосистема для развития людей, вовлеченных в IT",353.18,10 часов назад,"Блог компании Хабр, Научно-популярное","4.58 Оценка 353.18 Рейтинг Хабр Экосистема для развития людей, вовлеченных в IT 8 февраля — день российской науки. Этот праздник отмечается в Академии Наук, в НИИ, вузах, исследовательских лабораториях по всей стране. Судьба российской науки непростая, всегда переплетённая с историей страны и ею же обусловленная: непростой путь к открытию МГУ, талантливые кулибины (и И.Кулибин) из глубинки, учёные в изгнании, Туполевская шарага, достижения института Гамалеи… Всё смешано, переплетено и влияло и влияет на весь мир. Величие российской науки прежде всего в людях, которые её создают, которые разрабатывают, изобретают, экспериментирую и точно знают, что делают.  В этот день всегда спорят медики, биологи, инженеры, программисты, филологи, физики, химики — чьи учёные научнее и главнее, кто определял и создавал будущее? Ответ простой: все, каждый — в своей сфере. Предлагаем сегодня собрать российские  открытия и учёных, которые вас вдохновляют, впечатляют, будоражат воображение и влияют (или повлияли) на вас. Напишем эту статью вместе? Exosphere 10 часов назад История российской науки: напишем вместе? Простой 5 мин 1.3K Блог компании Хабр, Научно-популярное Ретроспектива КАК СТАТЬ АВТОРОМ Технотекст Добавь свой раздел в статью про российскую науку Войти Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп Правила создания статьи Вы в комментариях рассказываете о том самом учёном и его изобретении, которые важны именно для вас, а мы добавляем ваши истории в общую статью и формируем самый крутой лонгрид об истории российской науки. Формат свободный, желательно (не обязательно!) отразить: ФИО учёного, его годы жизни основные изобретения краткая биография наиболее значимое изобретение (на ваш взгляд) чем вас вдохновил учёный, как повлиял на вас изображения приветствуются. Комментарии будут сохраняться и вы сможете их обсуждать — так, как и всегда. Автор самого заплюсованного описания получит приятный сюрприз от команды Хабра. Чур, википедию не копировать! Итак, поехали.  Николай Иванович Пирогов (1810 — 1881) великий российский врач, хирург Николай Пирогов в 14 лет поступил на первый курс медицинского факультета МГУ (тогда Московского университета). По окончании вуза он в числе семи лучших студентов отправился в Депт (Тарту) заниматься анатомическими исследованиями и готовиться к профессорской деятельности. Считается одним из лучших анатомов мира, родоначальник топографической анатомии. Именно он разрезал замороженное человеческое тело в четырёх плоскостях и создал атлас топографической анатомии, который стал незаменимым пособием для хирургов. Это был не просто учебный материал, но прежде всего возможность оперировать с минимальным травматизмом для пациента. Пирогов брался за любые операции, от ампутаций и извлечения камней до глазных манипуляций и сосудистых вмешательств. Именно он провёл в 1847 году первую свою операцию под наркозом, тщательно изучив свойства эфира и хлороформа. Затем в течение года прошло 300 подобных операций. Николай Пирогов — автор алгоритма сортировки раненных на поле брани, благодаря чему удалось спасти огромное количество жизней, а сам алгоритм до сих пор служит организации военной медицины. Именно он ввёл в практику российской медицины обеззараживание хлорной водой и настойкой йода. До сих пор хлорка, йодопирон, йодинол и йод служат хирургам и всем медикам верную службу. Николай Пирогов был первым учёным от медицины, о котором я узнала в возрасте 7 или 8 лет, уже будучи немного горящей «стать доктором». Жизнь повернулась самым причудливым образом, но изучение истории медицины сделало моё мышление структурным, комплексным и навсегда привило любовь к людям. Какие бы они ни были, они — люди, с болью, со стрессом, с заболеваниями и радостью выздоровления. Совершенно особенное ощущение. Художественный фильм «Пирогов» Гааз и Пирогов — лекторий «Достоевский». Хотя конкретно в этом фильме история Гааза потрясает гораздо больше. Семён Николаевич Корсаков (1787-1853) создатель первой в мире перфокарточной машины (табулятора) «для сравнения идей» Рассказывает @Motanto Его история меня когда-то поразила: только подумайте, кто-то что-то делал в программировании до Бэббиджа, да ещё и в Российской Империи. Ещё больше поразил тот факт, что Корсаков был обвинён современниками в пустой трате времени на изобретения, а его славные дела дошли до мирового научного сообщества и до нас с вами благодаря ""раскопкам"" учёных XX века, в частности благодаря математику и кибернетику Г.Н. Поварову. Потомственный дворянин, Семён Корсаков стал прародителем отечественной кибернетики. Он всегда видел своей задачей усиление работы разума с помочью различных механических устройств (чем вам не первобытное программирование?!). Кроме изобретённых им гомеоскопов и идеоскопа, он подарил миру идею перфокарт. Именно гомеоскопические перфорированные таблицы стали прототипом тех самых картонных карт. Устройства Корсакова помогали сравнивать таблицы и обрабатывать большие массивы данных, с помощью его устройств можно было решать задачи классификации. Корсаков видел бытовое применение своим изобретениям (реестры, каталоги, регистрации), ничего не патентовал и просто представил машины общественности. С.Н.Корсаков и машина для сравнения идей Однако комиссия из пяти ученых, возглавляемая молодым математиком академиком М. В. Остроградским, вынесла вердикт: «Господин Корсаков потратил чересчур много своих интеллектуальных сил, пытаясь научить других, как вовсе без оного интеллекта прожить». Кажется, это лучшее определение ИТ-сферы целиком ;-) Такая вот биг дата доэлектрической эры. Павел Петрович Аносов (10 июля 1796 — 25 мая 1851) горный инженер, учёный-металлург Рассказывает @Mimizavr Талантами Русь богата И вот, старики говорят, Что сталь дамасских булатов Рассек уральский булат. Павел Петрович Аносов - на мой взгляд, совершенно фантастическая фигура, благодаря которой в 19 веке очень сильно развилась металлургия на Урале. Будучи горным инженером по специальности, он занимался научной работой в сфере металлургии, стал крупным организатором горнозаводской промышленности на Урале, исследовал природу Южного Урала, вел геологическую разведку месторождений россыпного золота и железных руд, изобрел ряд устройств и механизмов (в том числе и эффективную золотопромывную машину), разработал технологию производства огнеупорных тиглей (первый в России), стал десятым губернатором Томской губернии. Павел Петрович известен, в первую очередь, как создатель русского булата. Булат — особый вид стали, очень прочный и упругий, с узором на поверхности, который является не украшением, а показателем качества металла. Впервые оружие из такой стали начали использовать воины Древней Индии. Такими мечами разрубали камни и легкие воздушные ткани. Несколько веков спустя центром по изготовлению булатных клинков стал Дамаск. Как мастера 12-15 веков могли варить такую удивительную сталь, оставалось загадкой, разгадать которую сумел лишь Павел Аносов. До Павла Аносова создать булат пытались многие, но никому это не удавалось. За несколько лет работы мастер испробовал 189 способов сталеварения, но ни один из них не не дал булат. И лишь в 190-ый раз смешивая компоненты, создавая необходимые температурные условия мастер открыл тайну. Игорь Васильевич Курчатов (1903–1960) физик, «отец» атомной бомбы СССР Говорит и показывает @Boomburum Игорь Васильевич Курчатов (1903–1960) — один из первопроходцев в области ядерной физики, «отец» атомной бомбы СССР. В числе его заслуг — создание первого в Европе атомного реактора, первой в СССР атомной бомбы и первой в мире термоядерной бомбы. Но были заслуги и в развитии мирного атома — под его руководством была сооружена первая в мире атомная электростанция (Обнинская АЭС).  Самый засекреченный ученый Самый засекреченный ученый Теги:  российская наука, российские учёные Хабы:  Блог компании Хабр, Научно-популярное Редакторский дайджест Присылаем лучшие статьи раз в месяц Хабр Экосистема для развития людей, вовлеченных в IT Хабр Карьера Facebook Twitter ВКонтакте Instagram 244 Карма 128.3 Рейтинг @Exosphere модератор «Хабра», куратор-эксперт ОПРОС Вы знаете, что такое IT-менторство и как оно работает? Знаю, я — ментор или у меня была консультация с ментором Знаю и хочу попробовать, но пока опыта нет Знаю и не хочу пробовать, менторство не решает мой запрос Не знаю, но мне интересно Не знаю и мне неинтересно Голосовать Воздержаться Проголосовал 1061 пользователь. Воздержались 219 пользователей. Электропочта Комментарии 8 ИНФОРМАЦИЯ Сайт habr.com Дата регистрации 9 августа 2008 Дата основания 26 мая 2006 Ваш аккаунт Войти Регистрация Разделы Статьи Новости Информация Устройство сайта Для авторов Услуги Корпоративный блог Медийная реклама Численность 51–100 человек Местоположение Россия Представитель Алексей ССЫЛКИ Хабр Карьера career.habr.com Хабр Q&A qna.habr.com Хабр Фриланс freelance.habr.com ВИДЖЕТ ВКОНТАКТЕ Хабы Компании Авторы Песочница Для компаний Документы Соглашение Конфиденциальность Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2024, Habr Хабр Подписан 1 друг Подписаться на новости БЛОГ НА ХАБРЕ 10 часов назад История российской науки: напишем вместе? 31 янв в 15:03 10 «золотых» советов авторам любых текстов 28 янв в 12:22 Истории, достойные экранизации, или День защиты персональных данных 31 дек 2023 в 02:19 Поздравление-загадка от Хабра 29 дек 2023 в 18:31 Анонимный Дед Мороз на Хабре: хвастаемся подарками 1.3K 8 8.5K 41 +41 7.6K 16 +16 4K 44 +44 4.1K 97 +97 "
11,Как системному аналитику написать хорошее резюме — 11 рекомендаций _ Хабр.pdf,рекомендаций,-,0.0,мар в 14:20,-,"look, use the source! 1. Указывайте количественно и качественно выраженные достижения Это самый главный и мощный пункт. Большинство людей пишут какие-то беспомощные аморфные функции и фразы про обязанности и участие — «состоял, привлекался, принимал участие». Это выглядит, как свидетель из Фрязино, а не мощный проектный специалист, который будет двигать проект вперёд. Systems_Education 20 мар в 14:20 Как системному аналитику написать хорошее резюме — 11 рекомендаций Простой 3 мин 1.8K Анализ и проектирование систем*, Карьера в IT-индустрии Туториал 0 27 22 КАК СТАТЬ АВТОРОМ Исследуем Духов Машин в новом Сезоне на Хабре Нанимающий руководитель смотрит прежде всего на результаты, а не на процесс. Если вы пишете только про поток, это в глазах читающего создаёт риски того, что вы цените процесс, а не результаты. (Процесс тоже важен, но про него отдельно). Освойте язык результатов, важных для команды, бизнеса нанимателя, бизнеса клиента. Как обычно пишут Функции и задачи: разрабатывал требования, общался с клиентами, командой, отвечал на вопросы, рисовал схемы… Как надо Достижения: разработал требования для 5 проектов в области X, Y, Z; помог клиентам найти более выгодное решение; сократил время на создание пакета требований в 2 раза относительно планового; сократил количество обсуждений по требованиям в разработке в 3 раза; уменьшил и удерживал количество циклов согласования с 4-х до 2-х; внедрил в команде практику использования таких-то диаграмм, что сократило длительность переписок и встреч; выстроил доверительные отношения с ключевыми клиентами. 2. Не допускайте ошибок Будьте предельно внимательны ко всем словам, типографике, аббревиатурам. Кадровикам-рекрутёрам будет пофиг, но вот ваши будущие коллеги будут очень пристально смотреть на ваш текст и всё, что будет резать глаз. Если вы допускаете опечатки, ошибки в документе, который представляет вас всему рынку труда, то в рабочих документах будете тем более. Никто не хочет бесплатно работать корректором. Отдельный вопрос — логика построения предложений. В резюме для этого мало места, но и тут некоторые умудряются нагородить странный порядок слов. Помните как мантру — «Мама мыла раму», Субъект-Действие-Объект. Никто не хочет бесплатно работать редактором. Писать понятные тексты — это одна из важнейших компетенций аналитика. 3. Только актуальный и релевантный опыт Не указывайте места работы старше 8 лет, если только не хотите зачем-то подчеркнуть разносторонность своего опыта — например, почему-то считаете, что то, что вы работали таксистом полезно для конкретных вакансий, которые вы ищете. 4. Указывайте в местах работы основную трудовую функцию, а не должность по трудовой Запись в трудовой никого не интересует. Слова «старший специалист отдела ВРИОКОМУНО» никому ни о чём не говорят, ну разве что вы точно были не младшим. Пишите суть деятельности — «аналитик», «проектировщик», «координатор», «конструктор». Лучше, если там будет фигурировать как минимум слово «инженер». 5. Указывайте область деятельности компании / подразделения Название компании тоже зачастую ничего не говорит читателю. Сразу после названия позиции первой строчкой укажите, чем занимается компания — «разработка рудных месторождений», «направление занимается кредитами для малого бизнеса» и т.д. 6. Указывайте объём управления Иногда бывает, что человек работал ведущим аналитиком или руководителем группы и не указывает, сколькими людьми управлял. Поверьте, между 2-мя людьми в подчинении и 7 есть разница, по крайней мере, при прочих равных. 7. Подкрепляйте софт-скиллы фактами Лучше не писать «обучаемость», «креативность», «коммуникативность», это всем набило оскомину и ничего не значит, кроме вашего желания, чтобы вас так воспринимали. Пишите с примерами: Высокая обучаемость: за 3 месяца досрочно освоила Enterprise Architect. Креативность: придумала несколько концепций решений, которые мы с командой смогли продать клиенту и они дали ему существенный эффект в бизнесе. 8. Указывайте, если готовы предоставить образцы документов по запросу Документы не обязательно должны быть рабочими, это могут быть и документы, созданные вами на учебном курсе (Но такая фраза сразу приятно выделит вас на фоне других кандидатов). А сейчас так и вообще, пример портфолио можно нагенерить и оформить за пару-тройку часов! 9. Не забывайте про ключевые слова Если у вас в резюме нет слов SQL, XML, UML, JSON, API, то совершенно неудивительно, что вас невозможно найти (до трудно забыть дело даже не дойдет). 10. Покажите масштаб личности Вполную вы сможете развернуться на встрече, но даже на уровне резюме люди хотят работать с интересными людьми, а не роботами, которые выполняют трудовые функции по договору. Дайте представление читателю о вашем жизненном кругозоре и интересах, какими вещами вы увлекаетесь. Причём желательно не просто «философия, психология», а более предметно «изучаю аналитическую философию и гештальт-психологию». Одно ключевое слово здесь может заставить нанимающего менеджера позвать вас на интервью. 11. Создавайте копии резюме с профильным позиционированием Это совершенно бесплатно. Если вы знаете, работа в каких сферах/темах проектов вам более интересна, создайте копии резюме с соответствующими названиями: Системный аналитик в интеграции; Системный аналитик IoT; и т.д. Если хотите получить более предметную помощь по своему резюме и поиску работы — идите к менторам. Теги:  резюме, системный аналитик, трудоустройство, поиск работы 32 Карма 0 Рейтинг @Systems_Education Пользователь Сайт Сайт Facebook Telegram Telegram Реклама Комментарии 22 Публикации RationalAnswer 9 часов назад Как обнулялся Credit Suisse: разбираем траекторию погружения на дно швейцарского гига-банка 12 мин kesn 10 часов назад Хабы:  Анализ и проектирование систем, Карьера в IT-индустрии Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 12K Обзор +75 26 49 АМА Байки погромиста. Если кто-то скажет, что программирование — это скучно Простой 12 мин Tutelka 9 часов назад Обещания — настоящие и не очень Простой 7 мин alizar 9 часов назад Удивительное рядом. Как устроен буфер обмена в Windows и Linux Простой 6 мин unxed 3 часа назад Linux-порт Far Manager: новости весны 2023 Средний 4 мин Робот Марвин ждёт тех, кто его удивит в Сезон ML на Хабре Событие Показать еще МИНУТОЧКУ ВНИМАНИЯ Разместить 11K Мнение +75 33 9 2.7K Мнение +34 24 10 4.6K +31 46 10 1.7K Дайджест +29 14 7 ВАКАНСИИ Системный аналитик от 150 000 до 200 000 ₽ · SpectrumData · Екатеринбург Системный аналитик от 150 000 до 200 000 ₽ · Сбер · Екатеринбург Системный аналитик\ бизнес-аналитик до 250 000 ₽ · Главный радиочастотный центр · Москва Системный аналитик (senior) от 200 000 до 300 000 ₽ · Heaven 11 · Можно удаленно Системный аналитик (внутренняя безопасность) от 200 000 до 250 000 ₽ · Сбер · Москва Больше вакансий на Хабр Карьере Летопись data-driven-подхода, от статистики до ML Спецпроект Kubernetes без хайпа: полезные на практике посты о кубе Турбо Реклама ЧИТАЮТ СЕЙЧАС На LinkedIn обнаружили резюме человека, которому катастрофически не везёт с работодателями Как обнулялся Credit Suisse: разбираем траекторию погружения на дно швейцарского гига-банка «Призрачные вакансии» как новый HR-инструмент: растёт количество объявлений, по которым не ищут сотрудников Как получить доступ к chatGPT в России 2.4K 7 12K 48 4.1K 3 903K 233 РЕКЛАМА Байки погромиста. Если кто-то скажет, что программирование — это скучно Робот Марвин ждёт тех, кто его удивит в Сезон ML на Хабре Событие ИСТОРИИ РАБОТА Cистемный аналитик 528 вакансий Все вакансии Реклама 11K 9 Пункты сбора вторсырья на картах Электровел с ИИ от Acer Кейсы ML в бизнесе Полезная подборка о зрении Публичная консультация с экспертом: рекрутеры Реклама Реклама Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
12,Как создать аппаратный эмулятор CD-ROM без паяльника _ Хабр.pdf,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2394.92,20 мар в 14:00,"Блог компании RUVDS.com, Системное администрирование*, Разработка под Linux*, Разработка на Raspberry Pi*,","2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO- образов этих дисков остаётся актуальным. Многие операционные системы поставляются в виде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк старых персональных компьютеров. artyomsoft 20 мар в 14:00 Как создать аппаратный эмулятор CD-ROM без паяльника Средний 19 мин 9.2K Блог компании RUVDS.com, Системное администрирование*, Разработка под Linux*, Разработка на Raspberry Pi*,  КАК СТАТЬ АВТОРОМ Вызовы для k8s. Чего ждать операторам в будущем Существует множество решений, как можно установить операционную систему с ISO- образа без записи его на оптический носитель. Я уже затрагивал тему ISO-образов в моих статьях: «Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно знать о внешних загрузочных дисках». В этой статье я хочу рассказать о ещё одном способе, который, как оказывается, вшит в ядро Linux. Если ваш одноплатный компьютер имеет USB OTG-разъём, и на него возможна установка Linux, то вы c большой долей вероятности можете сделать из одноплатника аппаратный эмулятор привода оптических дисков. Меня этот способ заинтересовал. Я проверил его сам и, получив положительный результат у себя, решил поделиться с вами. Я сам узнал много интересного, систематизировал свои знания, поэтому надеюсь, что чтение будет познавательно и интересно для вас. Как всегда, если вы хотите посмотреть, что получится в итоге, уточнить детали, вы всегда можете найти исходный код в моём репозитории на GitHub. При написании статьи я поставил себе следующие цели: 1. Аппаратный эмулятор CD-ROM должен быть реализован без использования паяльника и макетных плат. 2. Реализация должна быть понятна человеку, имеющему лишь базовые представления о Linux, USB и Bluetooth. 3. Решение должно быть таким, чтобы его можно было с небольшими изменениями реализовать на различных одноплатных компьютерах. 4. Побудить интерес читателя к изучению используемых в статье технологий. 5. Изложить материал, необходимый для решения задачи лаконично и просто. Не уверен, что у меня это получилось из-за большого объёма темы. Буду признателен, если вы в отзывах напишете своё мнение. Оглавление Суть решения Проверка решения на практике От проверки идеи до реализации Операционная система Linux USB Bluetooth Сборка и модификация дистрибутива Raspberry Pi OS Реализация Как пользоваться эмулятором Особенности моего эмулятора Выводы Суть решения Решение заключается в том, что, модифицируя операционную систему Linux на одноплатном компьютере (встраиваемой системе), можно получить из него устройство, которое будет распознаваться компьютером как внешний оптический привод USB или флеш-накопитель. В ядро Linux включена поддержка эмуляции CD-ROM и эмуляции флеш-накопителя. Но это не значит, что любую встраиваемую систему можно превратить в них. Для этого ещё необходимо, чтобы встраиваемая система имела USB OTG-контроллер или USB- контроллер периферийного устройства. Проверка решения на практике Я делал эмулятор оптических дисков, используя Raspberry Pi Zero 2 W. Но вы можете использовать и другие одноплатные компьютеры. Естественно, вам тогда придётся самим разбираться с некоторыми проблемами, которые с большой долей вероятностью у вас возникнут. У меня других одноплатных компьютеров кроме Raspberry Pi не было, поэтому привожу алгоритм, как делал я. 1. Скачать образ Raspberry OS Light с сайта raspberrypi.org. 2. Записать образ на SD-карту. Я использовал программу balenaEtcher. 3. Добавить строку dtoverlay=dwc2  в файле config.txt на SD-карте. 4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл userconf.txt нужен, чтобы установить пароль для пользователя, ssh — чтобы включить SSH, wpa_supplicant.conf — чтобы указать точку доступа и пароль для Wi-Fi. 5. Вставить SD-карту в Raspberry Pi Zero 2 W. 6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму компьютера. 7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero 2 W подключится к Wi-Fi-сети. 8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и подключиться к нему по протоколу SFTP. Я использовал приложение WinSCP. 9. Записать ISO-образы, которые вы хотите эмулировать, в файловую систему Raspberry Pi. 10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать при помощи приложения PuTTY. 11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB CD-ROM, ввести команду: $ sudo modprobe g_mass_storage cdrom=y removable=y stall=n file=/full/filename/w После чего у вас на компьютере распознается внешний USB CD-ROM, в который вставлен диск. 12. Для прекращения эмуляции ввести команду: $ sudo modprobe -r g_mass_storage Приведённый файл userconf.txt устанавливает для пользователя pi пароль «raspberry». Файл ssh — это пустой файл, который не содержит никаких данных. От проверки идеи до реализации Приведённая выше последовательность шагов позволяет вам посмотреть работу эмуляции в действии. Однако это решение обладает рядом недостатков. 1. Чтобы загрузить и выбрать образ для эмуляции, необходимо наличие Wi-Fi. Файл userconf.txt Файл wpa_supplicant.conf 2. При перезагрузке Raspberry Pi необходимо заново монтировать образ. 3. Если нужно эмулировать образ для загрузки с него операционной системы, понадобится ещё один компьютер для управления Raspberry Pi. 4. Эмулировать можно образ размером максимум 2 Gib. Если вам интересно, как избавиться от этих недостатков, у вас есть время и интерес разобраться в этом вопросе, то предлагаю продолжить чтение. Краткое содержание моей реализации следующее: 1. Для общения с Raspberry Pi будет использоваться Bluetooth. 2. Чтобы работа устройства была возможна в автономном режиме (без подключений Wi- Fi и Bluetooth), управляющий скрипт оформляется в виде службы Systemd. 3. Для управления по Bluetooth будет использоваться приложение Serial Bluetooth Terminal c Google Play. 4. Для эмуляции оптических дисков с образов размером больше 2Gib необходимо внести небольшие изменения в модуль ядра Linux и выполнить перекомпиляцию. Приведу кратко, что вам нужно знать, чтобы лучше понять суть того, что мы будем делать. Операционная система Linux ▍ Ядро Linux Ядро Linux содержит в себе абстракции для работы с устройствами там, где оно запускается. Реализуются эти абстракции в специальных программах, называемых драйверами. В ОС Linux драйвер может находиться непосредственно в файле ядра, а может быть оформлен в виде отдельного модуля. В большинстве случаев предпочтителен второй способ, так как модули можно динамически удалять и добавлять. Например, если устройство не подключено к системе или с ним не осуществляется работа, драйвер нам не нужен, и его можно выгрузить из памяти или не загружать вообще. При загрузке ядра ему необходима информация об устройствах, которые присутствуют в системе, чтобы корректно загрузить драйверы (модули) для них. Эта информация может передаваться из различных источников. Например, в архитектуре x86 это будет ACPI. В архитектуре ARM это Device Tree. ▍ Device Tree и Overlays Иногда Device Tree нужно модифицировать, чтобы можно было загрузить корректно драйвера для устройств. Делается это при помощи подключения overlays. Они содержат информацию, что необходимо изменить в исходном Device Tree. ▍ Headless-режим работы Raspberry Pi Очень часто в различных статьях и самоучителях по работе с Raspberry Pi необходимо подключение монитора, клавиатуры и мыши. Но на самом деле есть возможность работать с ним в так называемом headless-режиме. В этом режиме вы работаете с Raspberry Pi при помощи эмулятора терминала. Соединение его с Raspberry Pi может быть UART, USB, Bluetooth, Ethernet, Wi-Fi. Главная сложность заключается в том, как можно работать в этом режиме с самого начала, сразу после записи образа операционной системы на SD-карту, если у вас нет лишнего монитора и клавиатуры. Как активировать SSH, настроить Wi-Fi на использование определённой точки доступа? В Raspberry Pi OS такая возможность есть. Достаточно разместить определённые файлы в разделе FAT32 на SD-карте и загрузиться с неё. Raspberry Pi OS сделает необходимые настройки сама. ▍ Файловые системы, блочные устройства, разделы, монтирование Меня всегда восхищала идея, что в Linux всё является файлом. Правильно используя средства Linux, можно практически без программирования выполнять сложные задачи. Блочное устройство — это некоторый файл, в который можно записывать и считывать данные блоками байтов различной длины. Что будет происходить при этом, зависит от того, с чем реально ассоциирован этот файл. Например, если он ассоциирован с жёстким диском, то тогда будут читаться/записываться данные на жёсткий диск, не обращая внимание на разделы и файловые системы. Если он ассоциирован с разделом жёсткого диска, то будут читаться/записываться данные, не обращая внимание на файловую систему. При помощи команды losetup можно добиться того, что он будет ассоциироваться с обычным файлом на диске, что позволит создавать образы разделов и дисков. Ещё полезной командой Linux является команда kpartx, которая создаёт блочные устройства из файла образа диска. Каждое из устройств будет ассоциировано с образом раздела, который хранится в этом файле. Форматирование раздела в Linux выполняется одной командой. В качестве параметра необходимо передать имя файла блочного устройства. Например, для создания файловой системы exFAT на блочном устройстве /dev/mmcblk0p3: $ mkfs.exfat /dev/mmcblk0p3 Чтобы можно было работать с файлами файловой системы, размещённой на блочном устройстве, нужно примонтировать файловую систему к корневой файловой системе при помощи команды mount. $ mkdir -p /mnt/data $ mount -t auto /dev/mmcblk0p3 /mnt/data Обратите внимание, что директория, куда будет производиться монтирование, должна существовать до того, как вы будете монтировать. Если нужно размонтировать файловую систему, используется команда umount. $umount /mnt/data Чтобы посмотреть, какие у вас есть блочные устройства, и куда они примонтированы, можно использовать команду: $ lsblk ▍ Systemd Загрузка Linux происходит в несколько этапов. Сначала загружается ядро операционной системы, затем ядро запускает процесс init. Задача процесса init загрузить и инициализировать процессы пространства пользователя и находиться в памяти до перезагрузки или выключения компьютера (устройства). За долгие годы существования Linux было написано множество реализаций init. На данный момент во многих Linux- дистрибутивах используется реализация, называемая systemd. Её мы и будем использовать. Минимум команд, которые необходимо знать для работы с systemd. Команда Назначение systemctl start sn.service запустить службу systemctl stop sn.service остановить службу systemctl status sn.service посмотреть статус службы systemctl enable sn.service включить службу (служба будет автоматически запущена при следующей загрузке Linux) systemctl disable sn.service выключить службу journalctl -u sn.service -b посмотреть логи службы, начиная с момента последней загрузки Linux ▍ Терминалы, PuTTY, sshd, agetty Для администрирования ОС Linux из Windows часто используют эмулятор терминала PuTTY. Он позволяет подключаться к компьютеру или устройству c ОС Linux с помощью различных соединений (Ethernet, Wi-Fi, эмулируемого последовательного порта на Bluetooth или USB) и работать удалённо с консолью Linux в Windows. Чтобы такое было возможно, в ОС Linux должна быть запущена специальная программа, которая будет взаимодействовать с PuTTY. Это может быть sshd в случае SSH-соединения или agetty в случае последовательного порта. При подключении через последовательный порт по умолчанию вы увидите чёрно-белый экран без поддержки манипулятора мышь. Чтобы добавить поддержку мыши и цветного экрана, необходимо изменить значение переменной окружения TERM в файле /usr/lib/systemd/system/serial-getty@.service. [Service] Environment=TERM=xterm USB Чтобы два USB-устройства могли работать друг с другом, необходимо наличие у каждого из них USB-контроллера. USB-контроллер в конкретный момент времени может работать в режиме хоста (Host) или режиме периферийного устройства (Device). Если одно из взаимодействующих устройств работает в режиме Host, то другое должно работать в режиме Device. Существуют следующие виды USB-контроллеров: Host — всегда работает в режиме Host. Device — всегда работает в режиме Device. OTG — может работать или в режиме хоста или в режиме периферийного устройства. Переключение режимов может быть аппаратным (при помощи особой распайки кабеля OTG кабель переводит в контроллер режим хоста) или программным Режим хоста подразумевает посылку команд, а режим периферийного устройства — их обработку. ▍ OTG USB-контроллер Возьмём Android-телефон с OTG-контроллером. Это означает, что при сопряжении по USB с компьютером (для записи файлов с компьютера на телефон), он будет играть роль периферийного устройства, а при сопряжении по USB с периферийным устройством (мышью, клавиатурой, сетевой картой, флэш-накопителем, монитором, принтером) телефон будет играть роль хоста. Обычно USB-контроллер периферийного устройства или USB OTG-контроллер присутствуют во встраиваемых устройствах. Также они могут быть интегрированы в однокристальную систему (SoC). Но по факту на устройстве может отсутствовать физический USB-разъём для подключения. Например, на всех Raspberry Pi установлена SoC, которая имеет OTG-контроллер, но фактически физический разъём для него есть только в Raspberry Pi Zero (Zero W, Zero 2 W) и в Raspberry Pi 4. ▍ Дескрипторы USB Каждое USB-устройство имеет дескрипторы. Дескрипторы — это информация о USB- устройстве, которая используется операционной системой для корректного выбора драйвера для устройства. Мне понравилось описание, которое приведено на сайте Microsoft. ▍ Создание USB-устройств в Linux Ядро Linux содержит модули, которые позволяют создавать виртуальные USB-устройства. Это может быть Mass Storage, последовательный порт, сетевая карта. Загрузив и настроив эти модули, вы можете сделать так, чтобы компьютером ваш одноплатник распознавался одним или несколькими такими устройствами. Если вам достаточно одного устройства, то вы можете загрузить модуль для этого устройства, опционально передав ему параметры для конфигурации при помощи команды modprobe. Когда отпадёт необходимость в этом устройстве, его можно выгрузить при помощи команды modprobe -r. Чтобы на одном физическом порту у вас распознавалось несколько устройств одновременно, нужно использовать модуль libcomposite и сконфигурировать эти устройства при помощи создания структур в файловой системе ConfigFS в директории /sys/kernel/config/usb_gadget. Такие устройства называются композитными USB-устройствами. Вы, скорее всего, встречались с такими, например, если у вас беспроводная клавиатура и мышь, а для них используется один приёмопередатчик. В нашем случае мы создадим композитное USB-устройство, которое будет последовательным портом и устройством хранения. Последовательный порт мы будем использовать для подключения к нашему эмулятору оптических дисков через PuTTY. Изначально я хотел, что бы это была сетевая карта и SSH, но карта требует настройки в операционной системе компьютера, поэтому для простоты отказался от этой идеи в пользу последовательного порта. ▍ Создание композитного USB-устройства при помощи ConfigFS 1. Загружаем модуль libcomposite. modprobe libcomposite 2. Заполняем дескрипторы для устройства. $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ mkdir -p $usb_dev $ echo 0x0137 > $usb_dev/idProduct $ echo 0x0100 > $usb_dev/bcdDevice $ echo 0x0200 > $usb_dev/bcdUSB $ echo 0xEF  > $usb_dev/bDeviceClass $ echo 0x02 > $usb_dev/bDeviceSubClass $ echo 0x01 > $usb_dev/bDeviceProtocol $ mkdir -p $usb_dev/strings/0x409 $ echo ""abababababababa"" > $usb_dev/strings/0x409/serialnumber $ echo ""Linux Foundation"" > $usb_dev/strings/0x409/manufacturer $ echo ""USB CD-ROM Emulator"" > $usb_dev/strings/0x409/product 3. Создаём конфигурацию. mkdir -p $usb_dev/configs/c.1 mkdir -p $usb_dev/configs/c.1/strings/0x409 echo ""acm+usb"" > $usb_dev/configs/c.1/strings/0x409/configuration echo ""0x80"" > $usb_dev/configs/c.1/bmAttributes echo 250 > $usb_dev/configs/c.1/MaxPower 4.Создаём и подключаем функцию acm (последовательный порт через USB). $ mkdir -p $usb_dev/functions/acm.usb0 $ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1 5. Создаём и подключаем функцию mass_storage. Mass_storage в данном случае — это эмуляция CD-ROM для ISO-образа /home/pi/1.iso. $ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0 $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/removable $ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua $ echo 0 > $usb_dev/functions/mass_storage.usb0/stall $ echo ""/home/pi/1.iso"" > $usb_dev/functions/mass_storage.usb0/lun.0/file $ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/configs/c.1 6. Активируем созданное устройство. $ ls /sys/class/udc > $usb_dev/UDC ▍ Удаление композитного USB-устройства при помощи ConfigFS 1. Деактивируем устройство. $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ echo """"> $usb_dev/UDC 2. Удаляем функцию mass_storage. $ rm $usb_dev/configs/c.1/mass_storage.usb0 $ rmdir $usb_dev/functions/mass_storage.usb0 3. Удаляем функцию acm. $ rm $usb_dev/configs/c.1/acm.usb0 $ rmdir $usb_dev/functions/acm.usb0 4. Удаляем конфигурацию. $ rmdir $usb_dev/configs/c.1/strings/0x409 $ rmdir $usb_dev/configs/c.1 5. Удаляем устройство. $ rmdir $usb_dev/strings/0x409 $ rmdir $usb_dev 6. Выгружаем загруженные устройством модули. $ modprobe -r usb_f_mass_storage $ modprobe -r usb_f_acm $ modprobe -r libcomposite Cтруктура файловой системы для созданного эмулятора CD-ROM Bluetooth Тема Bluetooth очень объёмная, и её невозможно изложить в одной статье, поэтому приведу только тот минимум, который позволяет понять, как мы будем использовать Bluetooth. Bluetooth — технология, которая позволяет связывать устройства без проводов по радиоканалу. На данный момент существует множество версий спецификации Bluetooth. Спецификация Bluetooth освещает множество вопросов. Чтобы передать данные с одного устройства на другое, необходимо наличие на обоих устройствах контроллеров и стеков Bluetooth. Bluetooth-контроллер — аппаратное устройство, обычно выполненное в виде микросхемы или части более сложной микросхемы, позволяющее получать/передавать данные по радиоканалу в соответствии со спецификацией Bluetooth. Bluetooth-стек — программная реализация протоколов, описанных в спецификации Bluetooth. Протоколы Bluetooth, предназначенные для решения определённых задач, группируются в профили Bluetooth. Мы будем использовать два профиля Bluetooth: 1. Generic Access Profile (GAP), который поддерживается всеми Bluetooth-устройствами. 2. Serial Port Profile (SPP), который подразумевает использование последовательного порта поверх соединения Bluetooth. ▍ Поддержка Bluetooth операционными системами Bluetooth-контроллеры могут иметь различные аппаратные интерфейсы для доступа. Это может быть UART, USB, PCIe. В случае операционной системы многие детали скрываются, и можно о них не думать. С контроллером можно работать на низком уровне через драйвер или уже используя высокоуровневые библиотеки и приложения, предоставляемые стеком Bluetooth, например, в Linux широко распространён стек BlueZ. ▍ BlueZ Стек Bluetooth BlueZ состоит из двух частей. Одна часть представлена модулями ядра Linux, уже включена в ядро. Если она отсутствует, то её нужно включить и перекомпилировать ядро. Вторая часть представлена приложениями для пространства пользователя. Приложения позволяют конфигурировать и работать со стеком Bluetooth. На данный момент многие приложения считаются устаревшими, и разработчики BlueZ рекомендуют использовать более новые приложения и интерфейс D-Bus для работы со стеком. Но, как мне кажется, именно те старые, устаревшие приложения позволяют лучше понять работу Bluetooth, поэтому в учебных целях я буду использовать их, для чего нужно будет инициализировать BlueZ в режиме совместимости. $ bluetoothd --noplugin=sap -C ▍ Протоколы Bluetooth Я не буду утомлять вас различными схемами, диаграммами, которые вы легко можете найти в интернете. Расскажу только о тех протоколах, с которыми нам предстоит столкнуться и нужно будет сконфигурировать. ▍ Service Discovery Protocol (SDP) При помощи протокола SDP можно определить, какие приложения (сервисы) находятся на хосте, и с ними возможен обмен данными через Bluetooth Чтобы можно было увидеть сервис с другого устройства, его необходимо зарегистрировать в SDP database. Например, если мы хотим зарегистрировать службу, представляющую эмуляцию последовательного порта в Bluetooth, это можно сделать следующей командой: $ sdptool add SP Чтобы можно было посмотреть службы, зарегистрированные у вас на хосте, нужно ввести команду: $ sdptool browse local ▍ Radio Frequency Communications (RFCOMM) Протокол RFCOMM позволяет создавать виртуальное соединение по последовательному порту между двумя хостами. На одном из хостов создаётся сервер, которому выделяется канал RFCOMM, второй из хостов подключается к нему, указывая MAC-адрес и номер канала Канал RFCOMM немного напоминает порт в UDP или TCP, но если у них и у источника и у получателя есть порты, то у RFCOMM для источника и получателя один и тот же канал. Поэтому невозможно создать несколько подключений на один и тот же канал. В Linux можно использовать команду rfcomm для создания процесса, который будет слушать определённый канал RFCOMM и при соединении запускать другой процесс. $ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcomm0 /dev/rfcomm0 В данном случае на Bluetooth-контроллере hci0 RFCOMM будет прослушиваться канал 1 и запускаться процесс cdemu-cmd с двумя параметрами командной строки /dev/rfcomm0 и /dev/rfcomm0. ▍ Утилита bluetoothctl Утилита Bluetoothctl позволяет сопрягать устройство, на котором вы её запустили с другим устройством. Вы можете сделать устройство видимым для обнаружения другими устройствами, а также найти другое устройство и выполнить с ним сопряжение. Более подробно расписано в документации к утилите, которая доступна по команде: $ man bluetoothctl ▍ Serial Bluetooth Terminal Для отладки приложений, использующих Bluetooth, удобно использовать приложение для Android Serial Bluetooth Terminal. Это приложение позволяет работать с Bluetooth- устройствами, у которых доступен профиль SPP. В нашем случае мы будем использовать его как визуальный интерфейс для работы с нашим эмулятором оптических дисков. Сборка и модификация дистрибутива Raspberry Pi OS Чтобы сделать полноценный аппаратный эмулятор оптических дисков, нам придётся немного модифицировать исходный дистрибутив Linux. Это подразумевает перекомпиляцию ядра, изменение нескольких конфигурационных файлов и добавление своего программного кода. Для меня это было удобно сделать при помощи Docker. ▍ Кросс-компиляция ядра Linux Кросс-компиляция позволяет на компьютере с одной архитектурой получать исполняемые файлы для другой архитектуры. Мы можем компилировать ядро Linux для Raspberry Pi на Raspberry Pi, а можем, используя кросс-компиляцию, сделать это на обычном компьютере с архитектурой x86, что существенно сократит время компиляции из-за большего быстродействия компьютера. Подробно о том, как выполнять кросс-компиляцию Raspberry Pi OS, можно почитать тут. ▍ Chroot и запуск бинарных файлов другой архитектуры Команда Linux chroot позволяет запускать процессы с изменённой корневой системой. Это кажется немного запутанным, но суть в следующем. В качестве параметра команде передаётся путь к корневой директории. В результате запуска команды через chroot запущенный процесс будет считать, что корнем файловой системы является та директория, которую передали в качестве параметра. Применений у команды chroot несколько, например, её можно использовать, чтобы запустить команду apt для Raspberry Pi в Docker-контейнере. Интересно, что Docker Desktop для Windows позволяет запускать исполняемые файлы для архитектуры ARM. В Linux-версии Docker такое сделать можно, но нужна дополнительная настройка. Реализация Созданный мной проект состоит из следующих файлов: 1. Dockerfile и скрипт, который выполняется в Docker-контейнере. 2. Файлы, которые необходимо добавить или обновить в исходном дистрибутиве: cdemu — основная логика работы эмулятора оптических дисков, написанная на языке bash; cdemu-cmd — bash-скрипт для обработки команд от пользователя и передачи их эмулятору; bash-utils.sh — bash-скрипт co вспомогательными функциями; cdemu-bluetooth-ui.service — systemd-служба, которая запускает интерпретатор команд на создаваемом RFCOMM-соединении телефона и Raspberry Pi; cdemu.service — systemd-служба, которая запускает эмулятор оптических дисков при загрузке; bluetooth.service — изменённая служба systemd для инициализации bluetooth; serial-getty@.service — изменённая служба systemd для запуска agetty на создаваемом соединении на последовательном порту; firstboot.service — служба systemd для запуска скрипта при первой загрузке операционной системы. Я её позаимствовал из проекта raspberian-firstboot; config.txt — изменённый файл конфигурации для загрузки Raspberry Pi. Содержит подключение overlay dwc. Это необходимо, чтобы USB-контроллер мог работать в device mode; fistboot.sh — скрипт, который запускается службой systemd firstboot.service; userconf.txt — файл, который необходим, чтобы установить пароль для пользователя pi. В последних версиях Raspberry Pi OS пользователь pi не активирован по умолчанию, поэтому необходимо наличие этого файла; ssh — файл необходим, чтобы активировать ssh, который отключён по умолчанию; wpa_supplicant.conf — файл, необходимый, если вы хотите настроить Raspberry Pi на работу с вашей точкой доступа. Листинги файлов не привожу, так как это ещё больше раздует и так большую статью. Ознакомиться вы с ними можете здесь. Как пользоваться эмулятором 1. Собираем Docker-образ. docker build -t raspi-image .  2. Собираем образ RaspberryPi OS. docker run --privileged -v c:\temp:/build --name raspi-image -it --rm raspi-image 3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi. 4. Подключаем Rapsberry Pi Zero 2 W к компьютеру. 5. Через некоторое время у вас появится съёмный накопитель. 6. На этот съёмный накопитель, содержащий файл Readme.txt, копируем образы, которые хотим эмулировать. 7. Находим виртуальный COM-порт, созданный после подключения Raspberry Pi к компьютеру. 8. Подключаемся к Raspberry Pi через с помощью Putty через виртуальный COM-порт. 9. Запускаем интерактивное приложение для управления эмулятором. $ sudo cdemu-cmd 10. Если хотим сделать управление с телефона, то выполняем сопряжение телефона и Raspberry Pi. Для чего вводим в эмуляторе терминала команду: $ sudo bluetoothctl 11. Делаем Raspberry Pi доступным для обнаружения: discoverable on 12. Находим его на телефоне и выполняем полключение. После чего соглашаемся с PIN- кодом на телефоне и Raspberry Pi. yes 13. Выходим из bluetoothctl. exit 14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем подключение к Raspberry Pi из него. Теперь можно посылать команды созданному эмулятору CD-ROM. Команды, которые можно посылать эмулятору: 1. hdd — переключение в режим эмуляции внешних жёстких дисков. 2. cdrom — переключение в режим эмуляции внешних приводов оптических дисков. 3. list — вывести список доступных ISO-образов, которые можно эмулировать. 4. insert <порядковый номер> — поместить ISO-образ для эмуляции. 5. eject — извлечь ISO-образ из эмулятора. 6. help — показать список доступных команд в текущем режиме. Особенности моего эмулятора Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы размером больше 2 Gib. Я просмотрел исходный код драйвера в файле drivers/usb/gadget/function/storage_common.c и предположил, что нет оснований не применять патч к ядру Linux от Adam Bambuch, который просто удаляет одно условие. Образы эмулировались нормально и при снятии ISO-образа с эмулируемого CD-ROM он был идентичен исходному. Поверял по хеш-коду для файла ISO-образа. Не пойму, почему есть это ограничение в Linux и почему его до сих пор не убрали? Если вы знаете ответ, ответьте в комментариях. Моя реализация не требует никаких дополнительных деталей. Нужен только Raspberry Pi Zero 2 W, один или два кабеля USB и адаптер питания, если будете использовать два кабеля USB. Один для питания, второй для передачи данных. Хоть и использование дополнительного кабеля и адаптера добавляет громоздкости, это решает проблему перезагрузки Raspberry Pi, если компьютер или ноутбук отключает ненадолго питание при перезагрузке. Кроме того, я не использую Python, только bash. Выводы Полученное программно-аппаратное решение, хоть и обладает рядом недостатков по сравнению с карманом Zalman (не поддерживается USB 3.0, нет интерактивного меню на самом устройстве), позволит вам установить практически любую операционную систему на широкий спектр компьютеров путём простого копирования ISO-образа. Решение является прототипом, но вместе с тем позволяет углубить знания по многим темам, или получить, если вы были с ними не знакомы. Так как основной целью была разработка прототипа, я запускал bluetoothd в режиме совместимости, и почти всю логику написал на bash. Я хотел показать возможность превратить встраиваемое устройство с операционной системой Linux в аппаратный эмулятор флеш-накопителя или привода оптических дисков, приложив минимум усилий. Надеюсь, что это удалось. Чтобы уместить всё в одной статье, я лишь поверхностно коснулся тех тем, которые необходимы для понимания. Если вас заинтересовало, вы можете самостоятельно изучить их углублённо. Объём статьи не позволяет осветить все интересности, с которыми я столкнулся при разработке эмулятора, и решения, которые применял и проверял. Приведу лишь несколько из них. Например, я долго боролся с зависанием при удалении составного устройства. Помогло использование службы serial-getty вместо getty, хотя во многих статьях упоминалась getty. Я долго разбирался, как можно сделать сопряжение через Bluetooth между Raspberry и телефоном, использовал команду bt-agent, но потом всё-таки отказался от неё в пользу bluetoothctl. При переключении эмулятора в режим HDD для записи ISO-образов изначально я открывал для доступа всю SD-карту и хранил ISO-образы в отдельном разделе, но потом посчитал, что для безопасности лучше хранить образ диска с ISO-образами в отдельном файле и открывать доступ только к нему, хоть это и снизило скорость записи, но пользователя не обескураживают появляющиеся несколько дисков. Разработанный прототип есть куда улучшать. Можно, например, создать более минималистичный дистрибутив Linux, который будет содержать только то, что реально используется для эмуляции, или создать более удобное графическое приложение для Android для работы с эмулятором. А можно упростить работу с Bluetooth, напрямую работая с драйверами bluetooth или используя интерфейс D-Bus для работы с Bluetooth- устройствами. Или вообще всё-таки взяться за паяльник и сделать устройство, более похожее по функционалу на карман Zalman. Но главное, вы увидели, что это реально сделать, а когда видишь положительный результат, это вдохновляет на большее творчество. В процессе тестирования и отладки программного кода было замечено, что на Lenovo X1 Extreme Gen 2 эмулятор CD-ROM дисков великолепно определялся в Windows 10, но отказывался определяться в BIOS. Эмпирически было определено, что помогает отключение режима экономии энергии процессора в BIOS. Также ноутбук отключал питание на usb при перезагрузке, поэтому понадобилось дополнительное питание Raspberry Pi. Интересно, но на ASUS K53E и Gigabyte BRIX всё работает без проблем. Решение с небольшими модификациями можно реализовать на Raspberry Pi 4. Но если вы поняли суть решения, вы его сможете повторить и на других одноплатных компьютерах, которые имеют выведенные USB-порты для OTG или USB-контроллеры периферийных устройств. RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 84 Карма 82.1 Рейтинг @artyomsoft Пользователь Комментарии 17 Dockerfile на данный момент только выполняется в Docker Desktop для Windows. В Linux он работать не будет. В заключение хочу сказать, что существует ещё один способ эмулировать оптические диски, который я не пробовал, но знаю о его существовании из ваших комментариев к одной из моих статей — это программа DriveDroid для Android. Я ей не пользовался, так как для её работы нужно получать права root на телефоне. Но, скорее всего, из-за ограничений в ядре Linux программа поддерживает ISO-образы до 2 Gib и/или работает только с гибридными ISO-образами. Если я не прав, буду рад увидеть ваши опровержения в комментариях. Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх 🕹️ Теги:  linux kernel, usb, bluetooth, bluez, linux modules, эмуляция, iso, cd-rom, systemd,  agetty, ruvds_статьи Хабы:  Блог компании RUVDS.com, Системное администрирование,  Разработка под Linux, Разработка на Raspberry Pi, DIY или Сделай сам Редакторский дайджест Присылаем лучшие статьи раз в месяц Электропочта Публикации RationalAnswer 9 часов назад Как обнулялся Credit Suisse: разбираем траекторию погружения на дно швейцарского гига-банка 12 мин kesn 10 часов назад Байки погромиста. Если кто-то скажет, что программирование — это скучно Простой 12 мин Tutelka 9 часов назад Обещания — настоящие и не очень Простой 7 мин alizar 9 часов назад Удивительное рядом. Как устроен буфер обмена в Windows и Linux Простой 6 мин ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 12K Обзор +75 26 49 11K Мнение +75 33 9 2.7K Мнение +34 24 10 4.7K +31 46 10 unxed 3 часа назад Linux-порт Far Manager: новости весны 2023 Средний 4 мин Показать еще 1.8K Дайджест +29 14 7 ИНФОРМАЦИЯ Сайт ruvds.com Дата регистрации 18 марта 2016 Дата основания 27 июля 2015 Численность 11–30 человек Местоположение Россия Представитель ruvds ССЫЛКИ VPS / VDS сервер от 130 рублей в месяц. ruvds.com Дата-центры RUVDS в Москве, Санкт-Петербурге, Казани, Екатеринбурге, Новосибирске, Лондоне, Франкфурте, Цюрихе, Амстердаме ruvds.com Помощь и вопросы ruvds.com Партнерская программа RUVDS ruvds.com VPS (CPU 1x2ГГц, RAM 512Mb, SSD 10 Gb) — 190 рублей в месяц ruvds.com VPS Windows от 523 рублей в месяц. Бесплатный тестовый период 3 дня. ruvds.com VDS в Цюрихе. Дата-центр TIER III — швейцарское качество по низкой цене. Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr ruvds.com Антивирусная защита виртуального сервера. Легкий агент для VPS. ruvds.com VPS в Лондоне. Дата-центр TIER III — английская точность за рубли. ruvds.com VPS с видеокартой на мощных серверах 3,4ГГц ruvds.com ПРИЛОЖЕНИЯ RUVDS Client Приложение для мониторинга и управления виртуальными серверами RUVDS с мобильных устройств. Android iOS ВИДЖЕТ БЛОГ НА ХАБРЕ 5 часов назад Как устроено индексирование баз данных 1.7K 0 9 часов назад Удивительное рядом. Как устроен буфер обмена в Windows и Linux вчера в 14:00 Будни техпода. Размещение игрового сервера на VDS вчера в 08:45 Ностальгические игры: Parasite Eve 20 мар в 18:00 Инструменты наблюдаемости, о которых нужно знать в 2023 году 4.7K 10 2.9K 12 3.4K 4 3.2K 4 "
13,Как создать аппаратный эмулятор CD-ROM.pdf,RUVDS.com,VDS/VPS-хостинг. Скидка 15% по коду HABR15,2394.92,20 мар в 12:00,"Блог компании RUVDS.com, системное администрирование, разработка под linux, ра","2394.92 Рейтинг RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15  20 мар в 12:00 Средний19 мин artyomsoft Как создать аппаратный эмулятор CD-ROM без паяльника 9.2K Блог компании RUVDS.com ,  Системное администрирование* ,  Разработка под Linux* ,  Ра КАК СТАТЬ АВТОРОМ Несмотря на то, что постепенно оптические диски уходят в прошлое, использование ISO-образов этих дисков остаётся актуальным. Многие операционные системы поставляются в виде ISO-образов, а администраторам необходимо поддерживать разношёрстный парк старых персональных компьютеров. Существует множество решений, как можно установить операционную систему с ISO-образа без записи его на оптический носитель. Я уже затрагивал тему ISO-образов в моих статьях: «Раскрываем секреты загрузочных ISO-образов» и «Что вам нужно знать о внешних загрузочных дисках». В этой статье я хочу рассказать о ещё одном способе, который, как оказывается, вшит в ядро Linux. Если ваш одноплатный компьютер имеет USB OTG-разъём, и на него возможна установка Linux, то вы c большой долей вероятности можете сделать из одноплатника аппаратный эмулятор привода оптических дисков.  Меня этот способ заинтересовал. Я проверил его сам и, получив положительный результат у себя, решил поделиться с вами.  Я сам узнал много интересного, систематизировал свои знания, поэтому надеюсь, что чтение будет познавательно и интересно для вас. Как всегда, если вы хотите посмотреть, что получится в итоге, уточнить детали, вы всегда можете найти исходный код в моём репозитории на GitHub. При написании статьи я поставил себе следующие цели: 1. Аппаратный эмулятор CD-ROM должен быть реализован без использования паяльника и макетных плат. 2. Реализация должна быть понятна человеку, имеющему лишь базовые представления о Linux, USB и Bluetooth. 3. Решение должно быть таким, чтобы его можно было с небольшими изменениями реализовать на различных одноплатных компьютерах. 4. Побудить интерес читателя к изучению используемых в статье технологий. 5. Изложить материал, необходимый для решения задачи лаконично и просто. Не уверен, что у меня это получилось из- за большого объёма темы. Буду признателен, если вы в отзывах напишете своё мнение. Оглавление • Суть решения • Проверка решения на практике • От проверки идеи до реализации • Операционная система Linux • USB • Bluetooth • Сборка и модификация дистрибутива Raspberry Pi OS • Реализация • Как пользоваться эмулятором • Особенности моего эмулятора • Выводы Суть решения Решение заключается в том, что, модифицируя операционную систему Linux на одноплатном компьютере (встраиваемой системе), можно получить из него устройство, которое будет распознаваться компьютером как внешний оптический привод USB или флеш- накопитель. В ядро Linux включена поддержка эмуляции CD-ROM и эмуляции флеш-накопителя. Но это не значит, что любую встраиваемую систему можно превратить в них. Для этого ещё необходимо, чтобы встраиваемая система имела USB OTG-контроллер или USB- контроллер периферийного устройства. Проверка решения на практике Я делал эмулятор оптических дисков, используя Raspberry Pi Zero 2 W. Но вы можете использовать и другие одноплатные компьютеры. Естественно, вам тогда придётся самим разбираться с некоторыми проблемами, которые с большой долей вероятностью у вас возникнут. У меня других одноплатных компьютеров кроме Raspberry Pi не было, поэтому привожу алгоритм, как делал я. 1. Скачать образ Raspberry OS Light с сайта raspberrypi.org. 2. Записать образ на SD-карту. Я использовал программу balenaEtcher. 3. Добавить строку dtoverlay=dwc2  в файле config.txt на SD- карте. 4. Записать файлы ssh и wpa_supplicant.conf на SD-карту. Файл userconf.txt нужен, чтобы установить пароль для пользователя, ssh — чтобы включить SSH, wpa_supplicant.conf — чтобы указать точку доступа и пароль для Wi-Fi. 5. Вставить SD-карту в Raspberry Pi Zero 2 W. 6. Подключить USB-разъём Raspberry Pi Zero 2 W к USB-разъёму компьютера. 7. Подождать, пока выполнится первая загрузка и Raspberry Pi Zero 2 W подключится к Wi-Fi-сети. 8. Найти IP-адрес Raspberry Pi Zero 2 W в Wi-Fi-сети и подключиться к нему по протоколу SFTP. Я использовал приложение WinSCP. 9. Записать ISO-образы, которые вы хотите эмулировать, в файловую систему Raspberry Pi. 10. Подключиться через SSH к Raspberry Pi W 2. Это можно сделать при помощи приложения PuTTY. 11. Для того, чтобы ваш Raspberry Pi Zero 2 W стал внешним USB CD-ROM, ввести команду: После чего у вас на компьютере распознается внешний USB CD- ROM, в который вставлен диск. 12. Для прекращения эмуляции ввести команду: Приведённый файл userconf.txt устанавливает для пользователя pi пароль «raspberry». Файл ssh — это пустой файл, который не содержит никаких данных. От проверки идеи до реализации Приведённая выше последовательность шагов позволяет вам посмотреть работу эмуляции в действии. Однако это решение обладает рядом недостатков. $ sudo modprobe g_mass_storage cdrom=y removable=y stall $ sudo modprobe -r g_mass_storage Файл userconf.txt Файл wpa_supplicant.conf 1. Чтобы загрузить и выбрать образ для эмуляции, необходимо наличие Wi-Fi. 2. При перезагрузке Raspberry Pi необходимо заново монтировать образ. 3. Если нужно эмулировать образ для загрузки с него операционной системы, понадобится ещё один компьютер для управления Raspberry Pi. 4. Эмулировать можно образ размером максимум 2 Gib. Если вам интересно, как избавиться от этих недостатков, у вас есть время и интерес разобраться в этом вопросе, то предлагаю продолжить чтение. Краткое содержание моей реализации следующее: 1. Для общения с Raspberry Pi будет использоваться Bluetooth. 2. Чтобы работа устройства была возможна в автономном режиме (без подключений Wi-Fi и Bluetooth), управляющий скрипт оформляется в виде службы Systemd. 3. Для управления по Bluetooth будет использоваться приложение Serial Bluetooth Terminal c Google Play. 4. Для эмуляции оптических дисков с образов размером больше 2Gib необходимо внести небольшие изменения в модуль ядра Linux и выполнить перекомпиляцию. Приведу кратко, что вам нужно знать, чтобы лучше понять суть того, что мы будем делать. Операционная система Linux ▍ Ядро Linux Ядро Linux содержит в себе абстракции для работы с устройствами там, где оно запускается. Реализуются эти абстракции в специальных программах, называемых драйверами. В ОС Linux драйвер может находиться непосредственно в файле ядра, а может быть оформлен в виде отдельного модуля. В большинстве случаев предпочтителен второй способ, так как модули можно динамически удалять и добавлять. Например, если устройство не подключено к системе или с ним не осуществляется работа, драйвер нам не нужен, и его можно выгрузить из памяти или не загружать вообще. При загрузке ядра ему необходима информация об устройствах, которые присутствуют в системе, чтобы корректно загрузить драйверы (модули) для них. Эта информация может передаваться из различных источников. Например, в архитектуре x86 это будет ACPI. В архитектуре ARM это Device Tree. ▍ Device Tree и Overlays Иногда Device Tree нужно модифицировать, чтобы можно было загрузить корректно драйвера для устройств. Делается это при помощи подключения overlays. Они содержат информацию, что необходимо изменить в исходном Device Tree. ▍ Headless-режим работы Raspberry Pi Очень часто в различных статьях и самоучителях по работе с Raspberry Pi необходимо подключение монитора, клавиатуры и мыши. Но на самом деле есть возможность работать с ним в так называемом headless-режиме. В этом режиме вы работаете с Raspberry Pi при помощи эмулятора терминала. Соединение его с Raspberry Pi может быть UART, USB, Bluetooth, Ethernet, Wi-Fi. Главная сложность заключается в том, как можно работать в этом режиме с самого начала, сразу после записи образа операционной системы на SD-карту, если у вас нет лишнего монитора и клавиатуры. Как активировать SSH, настроить Wi-Fi на использование определённой точки доступа? В Raspberry Pi OS такая возможность есть. Достаточно разместить определённые файлы в разделе FAT32 на SD-карте и загрузиться с неё. Raspberry Pi OS сделает необходимые настройки сама. ▍ Файловые системы, блочные устройства, разделы, монтирование Меня всегда восхищала идея, что в Linux всё является файлом. Правильно используя средства Linux, можно практически без программирования выполнять сложные задачи. Блочное устройство — это некоторый файл, в который можно записывать и считывать данные блоками байтов различной длины. Что будет происходить при этом, зависит от того, с чем реально ассоциирован этот файл. Например, если он ассоциирован с жёстким диском, то тогда будут читаться/записываться данные на жёсткий диск, не обращая внимание на разделы и файловые системы. Если он ассоциирован с разделом жёсткого диска, то будут читаться/записываться данные, не обращая внимание на файловую систему. При помощи команды losetup можно добиться того, что он будет ассоциироваться с обычным файлом на диске, что позволит создавать образы разделов и дисков.  Ещё полезной командой Linux является команда kpartx, которая создаёт блочные устройства из файла образа диска. Каждое из устройств будет ассоциировано с образом раздела, который хранится в этом файле. Форматирование раздела в Linux выполняется одной командой. В качестве параметра необходимо передать имя файла блочного устройства. Например, для создания файловой системы exFAT на блочном устройстве /dev/mmcblk0p3: Чтобы можно было работать с файлами файловой системы, размещённой на блочном устройстве, нужно примонтировать файловую систему к корневой файловой системе при помощи команды mount. Обратите внимание, что директория, куда будет производиться монтирование, должна существовать до того, как вы будете монтировать. Если нужно размонтировать файловую систему, используется команда umount. Чтобы посмотреть, какие у вас есть блочные устройства, и куда они примонтированы, можно использовать команду: ▍ Systemd Загрузка Linux происходит в несколько этапов. Сначала загружается ядро операционной системы, затем ядро запускает процесс init. Задача процесса init загрузить и инициализировать процессы пространства пользователя и находиться в памяти до перезагрузки или выключения компьютера (устройства). За долгие годы существования Linux было написано множество реализаций init. На данный момент во многих Linux-дистрибутивах используется реализация, называемая systemd. Её мы и будем использовать. $ mkfs.exfat /dev/mmcblk0p3 $ mkdir -p /mnt/data $ mount -t auto /dev/mmcblk0p3 /mnt/data $umount /mnt/data $ lsblk Минимум команд, которые необходимо знать для работы с systemd. Команда Назначение systemctl start sn.service запустить службу systemctl stop sn.service остановить службу systemctl status sn.service посмотреть статус службы systemctl enable sn.service включить службу (служба будет автоматически запущена при следующей загрузке Linux) systemctl disable sn.service выключить службу journalctl -u sn.service -b посмотреть логи службы, начиная с момента последней загрузки Linux ▍ Терминалы, PuTTY, sshd, agetty Для администрирования ОС Linux из Windows часто используют эмулятор терминала PuTTY. Он позволяет подключаться к компьютеру или устройству c ОС Linux с помощью различных соединений (Ethernet, Wi-Fi, эмулируемого последовательного порта на Bluetooth или USB) и работать удалённо с консолью Linux в Windows. Чтобы такое было возможно, в ОС Linux должна быть запущена специальная программа, которая будет взаимодействовать с PuTTY. Это может быть sshd в случае SSH- соединения или agetty в случае последовательного порта. При подключении через последовательный порт по умолчанию вы увидите чёрно-белый экран без поддержки манипулятора мышь. Чтобы добавить поддержку мыши и цветного экрана, необходимо изменить значение переменной окружения TERM в файле /usr/lib/systemd/system/serial-getty@.service.  USB Чтобы два USB-устройства могли работать друг с другом, необходимо наличие у каждого из них USB-контроллера. USB- контроллер в конкретный момент времени может работать в режиме хоста (Host) или режиме периферийного устройства (Device). Если одно из взаимодействующих устройств работает в режиме Host, то другое должно работать в режиме Device. Существуют следующие виды USB-контроллеров: • Host — всегда работает в режиме Host. • Device — всегда работает в режиме Device. • OTG — может работать или в режиме хоста или в режиме периферийного устройства. Переключение режимов может быть аппаратным (при помощи особой распайки кабеля OTG кабель переводит в контроллер режим хоста) или программным Режим хоста подразумевает посылку команд, а режим периферийного устройства — их обработку.  ▍ OTG USB-контроллер Возьмём Android-телефон с OTG-контроллером. Это означает, что при сопряжении по USB с компьютером (для записи файлов с компьютера на телефон), он будет играть роль периферийного устройства, а при сопряжении по USB с периферийным устройством (мышью, клавиатурой, сетевой картой, флэш-накопителем, монитором, принтером) телефон будет играть роль хоста. [Service] Environment=TERM=xterm Обычно USB-контроллер периферийного устройства или USB OTG- контроллер присутствуют во встраиваемых устройствах. Также они могут быть интегрированы в однокристальную систему (SoC). Но по факту на устройстве может отсутствовать физический USB-разъём для подключения. Например, на всех Raspberry Pi установлена SoC, которая имеет OTG-контроллер, но фактически физический разъём для него есть только в Raspberry Pi Zero (Zero W, Zero 2 W) и в Raspberry Pi 4. ▍ Дескрипторы USB Каждое USB-устройство имеет дескрипторы. Дескрипторы — это информация о USB-устройстве, которая используется операционной системой для корректного выбора драйвера для устройства. Мне понравилось описание, которое приведено на сайте Microsoft. ▍ Создание USB-устройств в Linux Ядро Linux содержит модули, которые позволяют создавать виртуальные USB-устройства. Это может быть Mass Storage, последовательный порт, сетевая карта. Загрузив и настроив эти модули, вы можете сделать так, чтобы компьютером ваш одноплатник распознавался одним или несколькими такими устройствами.  Если вам достаточно одного устройства, то вы можете загрузить модуль для этого устройства, опционально передав ему параметры для конфигурации при помощи команды modprobe. Когда отпадёт необходимость в этом устройстве, его можно выгрузить при помощи команды modprobe -r. Чтобы на одном физическом порту у вас распознавалось несколько устройств одновременно, нужно использовать модуль libcomposite и сконфигурировать эти устройства при помощи создания структур в файловой системе ConfigFS в директории /sys/kernel/config/usb_gadget. Такие устройства называются композитными USB-устройствами. Вы, скорее всего, встречались с такими, например, если у вас беспроводная клавиатура и мышь, а для них используется один приёмопередатчик. В нашем случае мы создадим композитное USB-устройство, которое будет последовательным портом и устройством хранения. Последовательный порт мы будем использовать для подключения к нашему эмулятору оптических дисков через PuTTY. Изначально я хотел, что бы это была сетевая карта и SSH, но карта требует настройки в операционной системе компьютера, поэтому для простоты отказался от этой идеи в пользу последовательного порта. ▍ Создание композитного USB-устройства при помощи ConfigFS 1. Загружаем модуль libcomposite. 2. Заполняем дескрипторы для устройства. modprobe libcomposite $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ mkdir -p $usb_dev $ echo 0x0137 > $usb_dev/idProduct $ echo 0x0100 > $usb_dev/bcdDevice $ echo 0x0200 > $usb_dev/bcdUSB $ echo 0xEF  > $usb_dev/bDeviceClass $ echo 0x02 > $usb_dev/bDeviceSubClass $ echo 0x01 > $usb_dev/bDeviceProtocol 3. Создаём конфигурацию. 4.Создаём и подключаем функцию acm (последовательный порт через USB). 5. Создаём и подключаем функцию mass_storage. Mass_storage в данном случае — это эмуляция CD-ROM для ISO-образа /home/pi/1.iso. 6. Активируем созданное устройство. $ mkdir -p $usb_dev/strings/0x409 $ echo ""abababababababa"" > $usb_dev/strings/0x409/serialnumb $ echo ""Linux Foundation"" > $usb_dev/strings/0x409/manufactu $ echo ""USB CD-ROM Emulator"" > $usb_dev/strings/0x409/produc mkdir -p $usb_dev/configs/c.1 mkdir -p $usb_dev/configs/c.1/strings/0x409 echo ""acm+usb"" > $usb_dev/configs/c.1/strings/0x409/configur echo ""0x80"" > $usb_dev/configs/c.1/bmAttributes echo 250 > $usb_dev/configs/c.1/MaxPower $ mkdir -p $usb_dev/functions/acm.usb0 $ ln -s $usb_dev/functions/acm.usb0 $usb_dev/configs/c.1 $ mkdir -p $usb_dev/functions/mass_storage.usb0/lun.0 $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/cdrom $ echo 1 > $usb_dev/functions/mass_storage.usb0/lun.0/remova $ echo 0 > $usb_dev/functions/mass_storage.usb0/lun.0/nofua $ echo 0 > $usb_dev/functions/mass_storage.usb0/stall $ echo ""/home/pi/1.iso"" > $usb_dev/functions/mass_storage.us $ ln -s $usb_dev/functions/mass_storage.usb0 $usb_dev/config ▍ Удаление композитного USB-устройства при помощи ConfigFS 1. Деактивируем устройство. 2. Удаляем функцию mass_storage. 3. Удаляем функцию acm. 4. Удаляем конфигурацию. 5. Удаляем устройство. $ ls /sys/class/udc > $usb_dev/UDC $ usb_dev=/sys/kernel/config/usb_gadget/cdemu $ echo """"> $usb_dev/UDC $ rm $usb_dev/configs/c.1/mass_storage.usb0 $ rmdir $usb_dev/functions/mass_storage.usb0 $ rm $usb_dev/configs/c.1/acm.usb0 $ rmdir $usb_dev/functions/acm.usb0 $ rmdir $usb_dev/configs/c.1/strings/0x409 $ rmdir $usb_dev/configs/c.1 $ rmdir $usb_dev/strings/0x409 $ rmdir $usb_dev 6. Выгружаем загруженные устройством модули. Cтруктура файловой системы для созданного эмулятора CD-ROM Bluetooth Тема Bluetooth очень объёмная, и её невозможно изложить в одной статье, поэтому приведу только тот минимум, который позволяет понять, как мы будем использовать Bluetooth. Bluetooth — технология, которая позволяет связывать устройства без проводов по радиоканалу. На данный момент существует множество версий спецификации Bluetooth. Спецификация Bluetooth освещает $ modprobe -r usb_f_mass_storage $ modprobe -r usb_f_acm $ modprobe -r libcomposite множество вопросов. Чтобы передать данные с одного устройства на другое, необходимо наличие на обоих устройствах контроллеров и стеков Bluetooth. Bluetooth-контроллер — аппаратное устройство, обычно выполненное в виде микросхемы или части более сложной микросхемы, позволяющее получать/передавать данные по радиоканалу в соответствии со спецификацией Bluetooth. Bluetooth-стек — программная реализация протоколов, описанных в спецификации Bluetooth. Протоколы Bluetooth, предназначенные для решения определённых задач, группируются в профили Bluetooth. Мы будем использовать два профиля Bluetooth: 1. Generic Access Profile (GAP), который поддерживается всеми Bluetooth-устройствами. 2. Serial Port Profile (SPP), который подразумевает использование последовательного порта поверх соединения Bluetooth. ▍ Поддержка Bluetooth операционными системами Bluetooth-контроллеры могут иметь различные аппаратные интерфейсы для доступа. Это может быть UART, USB, PCIe. В случае операционной системы многие детали скрываются, и можно о них не думать. С контроллером можно работать на низком уровне через драйвер или уже используя высокоуровневые библиотеки и приложения, предоставляемые стеком Bluetooth, например, в Linux широко распространён стек BlueZ.  ▍ BlueZ Стек Bluetooth BlueZ состоит из двух частей. Одна часть представлена модулями ядра Linux, уже включена в ядро. Если она отсутствует, то её нужно включить и перекомпилировать ядро. Вторая часть представлена приложениями для пространства пользователя. Приложения позволяют конфигурировать и работать со стеком Bluetooth. На данный момент многие приложения считаются устаревшими, и разработчики BlueZ рекомендуют использовать более новые приложения и интерфейс D-Bus для работы со стеком. Но, как мне кажется, именно те старые, устаревшие приложения позволяют лучше понять работу Bluetooth, поэтому в учебных целях я буду использовать их, для чего нужно будет инициализировать BlueZ в режиме совместимости.  ▍ Протоколы Bluetooth Я не буду утомлять вас различными схемами, диаграммами, которые вы легко можете найти в интернете. Расскажу только о тех протоколах, с которыми нам предстоит столкнуться и нужно будет сконфигурировать.  ▍ Service Discovery Protocol (SDP) При помощи протокола SDP можно определить, какие приложения (сервисы) находятся на хосте, и с ними возможен обмен данными через Bluetooth Чтобы можно было увидеть сервис с другого устройства, его $ bluetoothd --noplugin=sap -C необходимо зарегистрировать в SDP database. Например, если мы хотим зарегистрировать службу, представляющую эмуляцию последовательного порта в Bluetooth, это можно сделать следующей командой: Чтобы можно было посмотреть службы, зарегистрированные у вас на хосте, нужно ввести команду: ▍ Radio Frequency Communications (RFCOMM) Протокол RFCOMM позволяет создавать виртуальное соединение по последовательному порту между двумя хостами. На одном из хостов создаётся сервер, которому выделяется канал RFCOMM, второй из хостов подключается к нему, указывая MAC- адрес и номер канала Канал RFCOMM немного напоминает порт в UDP или TCP, но если у них и у источника и у получателя есть порты, то у RFCOMM для источника и получателя один и тот же канал. Поэтому невозможно создать несколько подключений на один и тот же канал. В Linux можно использовать команду rfcomm для создания процесса, который будет слушать определённый канал RFCOMM и при соединении запускать другой процесс. В данном случае на Bluetooth-контроллере hci0 RFCOMM будет $ sdptool add SP $ sdptool browse local $ rfcomm -r watch hci0 1 /usr/local/bin/cdemu-cmd /dev/rfcom прослушиваться канал 1 и запускаться процесс cdemu-cmd с двумя параметрами командной строки /dev/rfcomm0 и /dev/rfcomm0. ▍ Утилита bluetoothctl Утилита Bluetoothctl позволяет сопрягать устройство, на котором вы её запустили с другим устройством. Вы можете сделать устройство видимым для обнаружения другими устройствами, а также найти другое устройство и выполнить с ним сопряжение. Более подробно расписано в документации к утилите, которая доступна по команде:  ▍ Serial Bluetooth Terminal Для отладки приложений, использующих Bluetooth, удобно использовать приложение для Android Serial Bluetooth Terminal. Это приложение позволяет работать с Bluetooth-устройствами, у которых доступен профиль SPP. В нашем случае мы будем использовать его как визуальный интерфейс для работы с нашим эмулятором оптических дисков. Сборка и модификация дистрибутива Raspberry Pi OS Чтобы сделать полноценный аппаратный эмулятор оптических дисков, нам придётся немного модифицировать исходный дистрибутив Linux. Это подразумевает перекомпиляцию ядра, изменение нескольких конфигурационных файлов и добавление своего программного кода. Для меня это было удобно сделать при помощи Docker. ▍ Кросс-компиляция ядра Linux Кросс-компиляция позволяет на компьютере с одной архитектурой $ man bluetoothctl получать исполняемые файлы для другой архитектуры. Мы можем компилировать ядро Linux для Raspberry Pi на Raspberry Pi, а можем, используя кросс-компиляцию, сделать это на обычном компьютере с архитектурой x86, что существенно сократит время компиляции из-за большего быстродействия компьютера. Подробно о том, как выполнять кросс-компиляцию Raspberry Pi OS, можно почитать тут. ▍ Chroot и запуск бинарных файлов другой архитектуры Команда Linux chroot позволяет запускать процессы с изменённой корневой системой. Это кажется немного запутанным, но суть в следующем. В качестве параметра команде передаётся путь к корневой директории. В результате запуска команды через chroot запущенный процесс будет считать, что корнем файловой системы является та директория, которую передали в качестве параметра.  Применений у команды chroot несколько, например, её можно использовать, чтобы запустить команду apt для Raspberry Pi в Docker-контейнере. Интересно, что Docker Desktop для Windows позволяет запускать исполняемые файлы для архитектуры ARM. В Linux-версии Docker такое сделать можно, но нужна дополнительная настройка. Реализация Созданный мной проект состоит из следующих файлов: 1. Dockerfile и скрипт, который выполняется в Docker-контейнере. 2. Файлы, которые необходимо добавить или обновить в исходном дистрибутиве: • cdemu — основная логика работы эмулятора оптических дисков, написанная на языке bash; • cdemu-cmd — bash-скрипт для обработки команд от пользователя и передачи их эмулятору; • bash-utils.sh — bash-скрипт co вспомогательными функциями; • cdemu-bluetooth-ui.service — systemd-служба, которая запускает интерпретатор команд на создаваемом RFCOMM- соединении телефона и Raspberry Pi; • cdemu.service — systemd-служба, которая запускает эмулятор оптических дисков при загрузке; • bluetooth.service — изменённая служба systemd для инициализации bluetooth; • serial-getty@.service — изменённая служба systemd для запуска agetty на создаваемом соединении на последовательном порту; • firstboot.service — служба systemd для запуска скрипта при первой загрузке операционной системы. Я её позаимствовал из проекта raspberian-firstboot; • config.txt — изменённый файл конфигурации для загрузки Raspberry Pi. Содержит подключение overlay dwc. Это необходимо, чтобы USB-контроллер мог работать в device mode; • fistboot.sh — скрипт, который запускается службой systemd firstboot.service; • userconf.txt — файл, который необходим, чтобы установить пароль для пользователя pi. В последних версиях Raspberry Pi OS пользователь pi не активирован по умолчанию, поэтому необходимо наличие этого файла; • ssh — файл необходим, чтобы активировать ssh, который отключён по умолчанию; • wpa_supplicant.conf — файл, необходимый, если вы хотите настроить Raspberry Pi на работу с вашей точкой доступа. Листинги файлов не привожу, так как это ещё больше раздует и так большую статью.  Ознакомиться вы с ними можете здесь. Как пользоваться эмулятором 1. Собираем Docker-образ. 2. Собираем образ RaspberryPi OS. 3. Записываем образ на SD-карту. Вставляем её в Raspberry Pi. 4. Подключаем Rapsberry Pi Zero 2 W к компьютеру. 5. Через некоторое время у вас появится съёмный накопитель. 6. На этот съёмный накопитель, содержащий файл Readme.txt, копируем образы, которые хотим эмулировать. 7. Находим виртуальный COM-порт, созданный после подключения Raspberry Pi к компьютеру. 8. Подключаемся к Raspberry Pi через с помощью Putty через виртуальный COM-порт. 9. Запускаем интерактивное приложение для управления эмулятором. 10. Если хотим сделать управление с телефона, то выполняем сопряжение телефона и Raspberry Pi. Для чего вводим в эмуляторе терминала команду: docker build -t raspi-image . docker run --privileged -v c:\temp:/build --name raspi-image $ sudo cdemu-cmd $ sudo bluetoothctl 11. Делаем Raspberry Pi доступным для обнаружения: 12. Находим его на телефоне и выполняем полключение. После чего соглашаемся с PIN-кодом на телефоне и Raspberry Pi. 13. Выходим из bluetoothctl. 14. Запускаем на телефоне Serial Bluetooth Terminal и выполняем подключение к Raspberry Pi из него. Теперь можно посылать команды созданному эмулятору CD-ROM. Команды, которые можно посылать эмулятору: 1. hdd — переключение в режим эмуляции внешних жёстких дисков. 2. cdrom — переключение в режим эмуляции внешних приводов оптических дисков. 3. list — вывести список доступных ISO-образов, которые можно эмулировать. 4. insert <порядковый номер> — поместить ISO-образ для эмуляции. 5. eject — извлечь ISO-образ из эмулятора. 6. help — показать список доступных команд в текущем режиме. Особенности моего эмулятора discoverable on yes exit Интересно, но в Linux по умолчанию нельзя эмулировать ISO-образы размером больше 2 Gib. Я просмотрел исходный код драйвера в файле drivers/usb/gadget/function/storage_common.c и предположил, что нет оснований не применять патч к ядру Linux от Adam Bambuch, который просто удаляет одно условие. Образы эмулировались нормально и при снятии ISO-образа с эмулируемого CD-ROM он был идентичен исходному. Поверял по хеш-коду для файла ISO-образа. Не пойму, почему есть это ограничение в Linux и почему его до сих пор не убрали? Если вы знаете ответ, ответьте в комментариях. Моя реализация не требует никаких дополнительных деталей. Нужен только Raspberry Pi Zero 2 W, один или два кабеля USB и адаптер питания, если будете использовать два кабеля USB. Один для питания, второй для передачи данных. Хоть и использование дополнительного кабеля и адаптера добавляет громоздкости, это решает проблему перезагрузки Raspberry Pi, если компьютер или ноутбук отключает ненадолго питание при перезагрузке. Кроме того, я не использую Python, только bash. Выводы Полученное программно-аппаратное решение, хоть и обладает рядом недостатков по сравнению с карманом Zalman (не поддерживается USB 3.0, нет интерактивного меню на самом устройстве), позволит вам установить практически любую операционную систему на широкий спектр компьютеров путём простого копирования ISO-образа.  Решение является прототипом, но вместе с тем позволяет углубить знания по многим темам, или получить, если вы были с ними не знакомы. Так как основной целью была разработка прототипа, я запускал bluetoothd в режиме совместимости, и почти всю логику написал на bash. Я хотел показать возможность превратить встраиваемое устройство с операционной системой Linux в аппаратный эмулятор флеш- накопителя или привода оптических дисков, приложив минимум усилий. Надеюсь, что это удалось. Чтобы уместить всё в одной статье, я лишь поверхностно коснулся тех тем, которые необходимы для понимания. Если вас заинтересовало, вы можете самостоятельно изучить их углублённо. Объём статьи не позволяет осветить все интересности, с которыми я столкнулся при разработке эмулятора, и решения, которые применял и проверял. Приведу лишь несколько из них. Например, я долго боролся с зависанием при удалении составного устройства. Помогло использование службы serial-getty вместо getty, хотя во многих статьях упоминалась getty. Я долго разбирался, как можно сделать сопряжение через Bluetooth между Raspberry и телефоном, использовал команду bt-agent, но потом всё-таки отказался от неё в пользу bluetoothctl.  При переключении эмулятора в режим HDD для записи ISO-образов изначально я открывал для доступа всю SD-карту и хранил ISO- образы в отдельном разделе, но потом посчитал, что для безопасности лучше хранить образ диска с ISO-образами в отдельном файле и открывать доступ только к нему, хоть это и снизило скорость записи, но пользователя не обескураживают появляющиеся несколько дисков. Разработанный прототип есть куда улучшать. Можно, например, создать более минималистичный дистрибутив Linux, который будет содержать только то, что реально используется для эмуляции, или создать более удобное графическое приложение для Android для работы с эмулятором. А можно упростить работу с Bluetooth, напрямую работая с драйверами bluetooth или используя интерфейс D-Bus для работы с Bluetooth-устройствами. Или вообще всё-таки взяться за паяльник и сделать устройство, более похожее по функционалу на карман Zalman. Но главное, вы увидели, что это реально сделать, а когда видишь положительный результат, это вдохновляет на большее творчество. В процессе тестирования и отладки программного кода было замечено, что на Lenovo X1 Extreme Gen 2 эмулятор CD-ROM дисков великолепно определялся в Windows 10, но отказывался определяться в BIOS. Эмпирически было определено, что помогает отключение режима экономии энергии процессора в BIOS. Также ноутбук отключал питание на usb при перезагрузке, поэтому понадобилось дополнительное питание Raspberry Pi. Интересно, но на ASUS K53E и Gigabyte BRIX всё работает без проблем. Решение с небольшими модификациями можно реализовать на Raspberry Pi 4. Но если вы поняли суть решения, вы его сможете повторить и на других одноплатных компьютерах, которые имеют выведенные USB-порты для OTG или USB-контроллеры периферийных устройств. Dockerfile на данный момент только выполняется в Docker Desktop для Windows. В Linux он работать не будет. В заключение хочу сказать, что существует ещё один способ эмулировать оптические диски, который я не пробовал, но знаю о его существовании из ваших комментариев к одной из моих статей — это программа DriveDroid для Android. Я ей не пользовался, так как для её работы нужно получать права root на телефоне. Но, скорее всего, из-за ограничений в ядре Linux программа поддерживает ISO- образы до 2 Gib и/или работает только с гибридными ISO-образами. Если я не прав, буду рад увидеть ваши опровержения в комментариях. RUVDS.com VDS/VPS-хостинг. Скидка 15% по коду HABR15 Telegram ВКонтакте Twitter 84 Карма 82.1 Рейтинг @artyomsoft Пользователь Комментарии 17 Публикации Telegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх  ️ Теги:   linux kernel , usb , bluetooth , bluez , linux modules , эмуляция , iso , cd-rom , systemd , agetty , ruvds_статьи Хабы:   Блог компании RUVDS.com , Системное администрирование , Разработка под Linux , Разработка на Raspberry Pi , DIY или Сделай сам +84 98 17 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт ruvds.com Дата регистрации 18 марта 2016 Дата основания 27 июля 2015 Численность 11–30 человек Местоположение Россия Представитель ruvds Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
14,"Кто ты, SwiftData _ Хабр.pdf",Т-Банк,-,521.91,6 фев 2024 в 18:00,"Блог компании Т-Банк, Разработка под iOS*, Разработка мобильных приложений*, Swift*, Разработка под macOS","521.91 Рейтинг Т-Банк Подписаться Azon 6 фев 2024 в 18:00 Кто ты, SwiftData Средний 15 мин 4.4K Блог компании Т-Банк, Разработка под iOS*, Разработка мобильных приложений*, Swift*, Разработка под macOS* Мнение +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Скрытая угроза для хакеров: как 2FA рушит их планы Привет! Я Андрей Зонов, стафф-инженер в Тинькофф и большой фанат CoreData. Моя любовь в CoreData началась на старте карьеры, когда я попал в первый Enterprise-проект. Это были времена iOS 4.3., CoreData не имела parent-контекстов и методов perform. Как-то так сложилось, что за свою карьеру я много фиксил классические проблемы в CoreData, и с появлением SwiftData мне стало интересно, остались ли проблемы в SwiftData и что нам дает этот фреймворк.  В статье разберем основные концепции и киллер-фичи по SwiftData. Пройдемся по основным отличиям и тому, как можно мигрировать с CoreData на актуальную SwiftData. Копнем внутрь SwiftData, узнаем, как она устроена под капотом, и подведем итоги стоит ли вообще переезжать на новый фреймворк Apple.  Концепция @Model  Основная и первая киллер-фича SwiftData — это переворот концепции. Если в CoreData мы создавали файлик .xcdatamodeld и рисовали схемки в визуальном интерфейсе, то так же как и со SwiftUI, в SwiftData появляется единый источник правды — это код. Определяем схему Entity прямо в коде: final class Post {         var id: String    var message: String    var timestamp: Date    var imageURLs: [URL] Больше не нужно думать об архитектуре нашего компонента —  нужно ему хранилище или нет. Если на слое модели уже есть какой-то класс, например класс публикации Post, то нужно просто добавить макрос @model, чтобы сделать класс персистентным и добавить возможность сохранения его в Storage. Кажется, что именно макросов ждала CoreData со времен iOS 5. Потому что макросы дают ту самую compile-time-безопасность и возможность валидации схем, которой не хватало в CoreData.  Но это не все, что умеет SwiftData-модель. Если у исходного класса были ссылки на другие классы, то добавление макроса @model создаст связи с сущностями, ссылки на которые были в исходном классе. Например, связь с аттачментами или с автором-пользователем. В случае с автором мы получаем связь типа один к одному, а в случае с аттачментами — один ко многим. Связи устанавливаются из кода: @Model final class Post {         var id: String    var message: String    var timestamp: Date        var attachments: [Attachment]    var author: User В коде появились новые макросы. Атрибут Unique. Если добавить его к полю ID, то можно помочь SwiftData не создавать дублирующую запись, а найти ту, что есть, и обновить поля, которые не совпадают. То есть при добавлении новой публикации в Store, если уже в Store есть публикация с идентичным ID, SwiftData сделает не insert, а upsert. @Model final class Post {      @Attribute(.unique)    var id: String    var message: String    var timestamp: Date        var attachments: [Attachment]    var author: User Макрос Relationship. Тут вы встретите  знакомое из Core Data слово — Cascade. Он позволяет при удалении публикации автоматически удалять все attachments, связанные с ней.  Здесь же можно выставить инверсные зависимости. Если у attachment есть несколько полей, названных или ссылающихся на пост, мы можем подсказать SwiftData, какое именно является инверсной зависимостью.  @Model final class Post {    @Attribute(.unique)    var id: String    var message: String    var timestamp: Date    @Relationship(deleteRule: .cascade, inverse: \Attachment.post)    var attachments: [Attachment]    var author: User Макрос Transient. Apple активно использует этот макрос в реализации самой SwiftData. Если ваш класс модели имеет свойство, которое вы не хотите писать в постоянное хранилище, достаточно аннотировать каждое из этих свойств макросом @Transient. Например, это может быть калькулируемое поле, зависимое от сегодняшней даты. @Model final class Post {    @Attribute(.unique)    var id: String    var message: String    var timestamp: Date    @Relationship(deleteRule: .cascade, inverse: \Attachment.post)    var attachments: [Attachment]    var author: User    @Transient    var isDraft: Bool = false Концепция Model Container Следующая концепция SwiftData — это Model Container или контейнер моделей. Если до этого мы рассмотрели модели, то Model Container — тот самый мостик между схемой и хранилищем. Model Container получает в конструкторе набор моделей, с которыми ему нужно работать.  Model Container достаточно умный, чтобы заметить, что у сущности Post есть связи с attachment и user, такая конфигурация будет валидной. Model Container поймет, что у поста есть связи, и самостоятельно добавит их в схему базы данных.  Контейнер настраивает хранилище, конфигурации и план миграции с предыдущих схем на актуальные. Например, в SwiftData можно настроить in-memory тип хранилища и не хранить персистентность. Можно настроить обратную ситуацию, когда у нас есть persistent store, но мы не можем в него писать — readOnly. Тут же настраиваются такие нюансы, как Cloud Kit или прямой путь к .sqlite-файлу. Контейнер связывает нашу схему с физическим хранилищем на диске. Кроме контейнера у моделей есть контекст. Контейнер — компаньон контекста. Контекст в SwiftData, в отличие от CoreData, связан именно с контейнером.  let container = try ModelContainer(for: Post.self) let context = container.mainContext let newContext = ModelContext(container) newContext.autosaveEnabled = false      let post = Post(message: ""Привет!"") newContext.insert(post) try newContext.save() Сам контекст мы получаем из контейнера. Apple пыталась это внедрить уже в последних версиях CoreData, но здесь это вышло абсолютно на новый уровень. Для создания контекста нужно передать контейнер, в рамках которого он будет работать. И при создании модели сущности Post мы должны явно вызвать метод insert у контекста и save.  Я пропустил autosave enabled, потому что вся SwiftData работает на дефолтах и по дефолту все контексты автосохраняемы. Если мы выключаем автосохранение у созданного контекста, то не нужно вызывать метод save.  Когда вызывается метод save:  с Swift UI все очевидно: когда перерисовывается view, проверяется, есть ли изменения в контексте. И если они есть, то триггерится сохранение; с UIKit каждый раз, когда приложение UI Application меняет метод своего жизненного цикла, то есть переходит в foreground или в background, вы переключаете приложение или закрываете, явно вызывается save. Мы можем быть уверены, что сохранение будет перед тем, как приложение выгрузится из памяти. Даже если приложение просто висит на экране у пользователя, через какое-то время на RunLoop срабатывает периодический таймер, и если есть изменения, также триггерится save.  Важно, что контекст связан с контейнером, но не с моделью, поэтому в SwiftData возможно создание объекта с контекстом сразу. Модель самостоятельно работает без контекста, и мы можем ее использовать не только на слое модели, конструировать ее в отрыве от контейнера и таким образом не думать о персистентности на ранних этапах конструирования модуля.  Контекст — это своеобразный слепок данных, выгруженных из хранилища. Именно контекст следит за тем, какие данные в хранилище обновляются, возможно с помощью других контекстов, и сообщает об этом интерфейсу. И также при изменении моделей в самом контексте он проводит эти данные в Store.  У одного хранилища может быть несколько слепков, но у каждого контекста может быть только одно хранилище. Связь — одно хранилище ко многим контекстам. На уровне контекста настраивается автосейв, и если автосейв выключен, очевидно, что можно сделать так же, как и в CoreData, undo и redo.  Концепция Predicate Следующая концепция и киллер-фича SwiftData — это предикаты. Предикаты работают на основе макросов, и они имеют очевидное преимущество над предикатами CoreData — Compile- Time-Safety. Кроме того, предикаты — это pure Swift, который преобразуется в select-запросы. Базовая работа с коллекциями может быть преобразована в select-запрос с Join-ом. Предикаты их поддерживают, но скрывают. В compile-time предикаты подскажут, возможно ли собрать select-запрос, и если нет, то почему. Такая возможность есть из-за того, что предикаты реализованы через макросы.  У предиката есть компаньон — FetchDescriptor. Он нужен для того, чтобы выполнить fetch- запрос, его название говорит «я описываю fetch-запрос». Мы передаем предикат в FetchDescriptor и после этого передаем дескриптор в контекст.  FetchDescriptor нужен, потому что внутри SQL есть select, where и sortby. Sortby реализованы через key-value-кодинг. Key-value-кодинг опять в Swifty-way, безопасный и удобный. FetchDescriptor имеет в конструкторе эти два аргумента — predicate и sortby, и оба аргумента опциональны.  FetchDescriptor может иметь просто пустой конструктор, тогда он получит все записи из нашей таблицы, например для пагинации. На уровне дескриптора можно настроить исключение тех данных, которые еще не сохранены. И опять же, мы возвращаемся к тому, что у контекстов есть автосейв. Автосейв можно выключить. Если мы выключили автосейв, то, соответственно, мы, возможно, хотим фетчить данные из контекста, но только те, что уже были сохранены, и исключать те, что не были сохранены.  let today = Date() let kittyFeed = #Predicate<Post> {     $0.timestamp < today &&     $0.message.contains(""котики"") } var descriptor = FetchDescriptor<Post>(     predicate: kittyFeed,     sortBy: [SortDescriptor(\.timestamp)] ) descriptor.includePendingChanges = false let posts = try newContext.fetch(descriptor) Например, есть публикация. Мы не хотим сохранять публикацию до того, как она будет иметь базовый тайтл. ID она имеет. Если мы фетчим весь список публикаций, то пока публикация является драфтом. Как только мы ее переведем из драфта, сможем ее отобразить. Таким образом одним свойством includePendingChanges мы уже получаем хорошо работающую фичу.  И также здесь есть оптимизация выделения оперативной памяти для фетч-запроса. Мы можем указать, какие именно поля самих сущностей мы хотим запросить и какие поля сущностей внутри связи нам нужны.  Таким образом через те же механизмы, что были доступны в CoreData, мы можем оптимизировать размер выделяемой оперативной памяти для хранения фетч-запроса и в тот же момент максимально быстро получать доступ к тем полям, которые нам действительно нужны.  Предикаты — мощные compile-time киллер фичи. Они поддерживают подзапросы, джойны и транслируются явно в SQL-запросы. На уровне фетч-дескриптора мы можем настроить такие вещи, как offset-limit. Он работает с фолтом и префетчем.  Улучшение синтаксиса предикатов Улучшение синтаксиса предикатов Интеграция со SwiftConcurrency Сложно объяснить весь блеск SwiftConcurrency в связке со SwiftData, не объяснив всю нищету того, как работала многопоточная CoreData. Несмотря на то, что я ее очень люблю, уровень сложности и количество проблем, которые она приносила, — это просто невыносимо. CoreData работает так: один поток — один контекст. Соблюдай правила — ничего не будет болеть. Непонятные типы хранилища XML, которые идут нам еще с macOS, недоступны в iOS, но все еще есть в списке. Полностью весь движок на Objective-C runtime, который может в любой момент выстрелить — и мы даже не поймем, что произошло. И концептуально с iOS 5 CoreData не менялась.  Последнее время люди делились на тех, кто не понимает CoreData, и теми, кто не понимает Realm. Как только появился Swift с Codable, все такие: будем хранить все в файлах! И кажется, это не совсем то, чего хочет от нас Apple. Давайте посмотрим, как работает многопоточность в CoreData. func update(post: CoreData.Post, message: String) async throws {     guard let context = post.managedObjectContext else {         throw CoreData.CoreError.noContext     }     try await context.perform {         post.message = message         try context.save()     } } Многопоточность в CoreData работает предсказуемо. Всегда одному потоку соответствует один контекст и объекты между потоками передаются через Object ID.  Это происходит из-за того, что Manage Object и Manage Context не потокобезопасны. Казалось бы, Apple вам в iOS 5 добавила такие классные штуки, как Context Perform и Parent Context. Но мы не можем делать хорошо CoreData, потому что это требует кучу когнитивной нагрузки и полного понимания того, как работает многопоточность в iOS.  Я часто видел, как разработчики брали конкурентную очередь, ассоциировали ее с контекстом и у них все отлично работало. Они считали, что удовлетворяют правилу «один поток, один контекст»: и у них, и у тестировщиков все отлично работало. А у пользователей все дико крашилось. Потому что разработчики забывали, что в релизной конфигурации под нагрузкой конкурентная очередь исполняется на разных потоках.  Если в параллель начинается работа над одним объектом внутри одного контекста из разных потоков, то мы получаем в лучшем случае неконсистентные данные, а в худшем — краш на сейве. И это отловить было очень сложно. И казалось бы, можно делать вот так, но это будет некрасиво. И все пытаются сделать красиво, но крашатся.  В SwiftData мы получили, что теперь контексты связаны с Queue явно. И через Queue они связаны с очередями. Ушел непонятный тип хранилища XML, и у нас полностью нативная поддержка Swift. Вспоминаем предикаты, макросы, все в Compile Time Safety.  SwiftConcurrency со SwiftData связаны так: контекст SwiftData ассоциирует с Queue. Каждый раз, когда мы создаем контекст, у нас SwiftData запоминает, в какой Queue создан этот контекст, и запоминает это в рамках актора.  Если мы создаем в background-потоке private-контекст, он ассоциируется с private Queue. Если в главном потоке, он ассоциируется с main Queue, которая serial.  SwiftData предоставляет ModelContainer. ModelContainer внутри имеет main-контекст, который атрибутирован main-актором.  С main-актором все знакомы, понимаем, что будет, если мы попробуем main-контекст использовать на background-потоках. По бэкграунду есть отдельный актор — ModelActor. Он содержит протокол, ModelContainer и ModelExecutor.  ModelExecutor — это макрос. Мы можем атрибуцировать этим макросом свой актор и посмотреть, что внутри конструктора будет создаваться контекст. Так контекст будет ассоциироваться с Queue и передаваться в default serial ModelExecutor.  Есть подсказка к тому, как работает @ModelActor, — serial. Но кто это такой? Что за default serial ModelExecutor? Он создается только в конструкторе ModelActor, принимает контекст, который мы создали прямо там же рядом, в этой же Queue, и, как говорит нам документация, он безопасно работает с хранилищем.  DefaultSerialModelExecutor работает серийно на нужной очереди. Это именно то же самое, что было в CoreData с методами perform, только теперь это работает очевидно. И SwiftConcurrency будет бить по рукам, если мы будем использовать это некорректно. Наконец-то ? @ModelActor actor MobiusModelActor {     let modelExecutor: any ModelExecutor     let modelContainer: ModelContainer     init(modelContainer: ModelContainer) {        let modelContext = ModelContext(modelContainer)        modelExecutor = DefaultSerialModelExecutor(modelContext: modelContext)        self.modelContainer = modelContainer     } } CoreData и SwiftData вместе CoreData и SwiftData могут жить вместе и писать в один стор. SwiftData может читать из сторов, в которые в этот же момент пишет CoreData. А может работать и наоборот — записанные данные из SwiftData будут доступны в CoreData. Apple дала классный инструмент, который генерирует из наших Core Data Model SwiftData- классы. Но нужно эти SwiftData-классы положить в другой namespace. Например, в другой модуль или обернуть их в enum.  Я советую их оборачивать в структуру или в enum, потому что при следующей миграции будет очень удобно, что есть несколько enum для каждой версии SwiftData.  И значит, SwiftData автоматически вам сделает миграцию.  Это все замечательно работает, если у вас минимальный таргет iOS 17. Но кажется, что ничего волшебного в SwiftData нет, чего не было в CoreData.  В своем проекте я измерил производительность — и оказалось, что ни прироста, ни деградации производительности нет. Имеет ли смысл вообще поднимать таргет до iOS 17 или продавать идею миграции на SwiftData? Есть ли пофит, кроме безопасности?  Как вы думаете, насколько SwiftData лучше CoreData? Ноль. В тех местах, где утыкается CoreData, — там же утыкается SwiftData. Все идентично до нюансов. Детали SwiftData Изначально я мерил performance на CoreData-проекте и увидел, что performance идентичный. Записал полностью логи SQL-запросов и мигрировал этот проект полностью на SwiftData. Удалил CoreData для честности эксперимента: может, они на интеропе тормозят. И прогнал то же самое.  Записываем логи CoreData примерно так: @Model final class MobiusEntity1 {     var timestamp: Date     var oneToOne: MobiusEntity2     var oneToMany: [MobiusEntity3]     init(timestamp: Date) {         self.timestamp = timestamp         self.oneToOne = MobiusEntity2(timestamp: timestamp)         self.oneToMany = [MobiusEntity3(timestamp: timestamp)]     } } В launch-аргументы пишем com.apple.coredata.sqldebug. Ключ у SwiftData такой же. И логи — coredata.sql.begin, coredata.sql.insert. Логи идентичные, файлы идентичные.  Вывод: внутри SwiftData engine.coredata. Как, блин, они это сделали? И плохо ли это?  Неделю я был расстроен, потом отошел и начал думать. Внутри у нас SQLite. Все проблемы CoreData всегда были именно в проблемах SQLite, потому что при любых сложностях и работе с диском мы упираемся в его скорость.  SQLite — классный фреймворк и самая популярная встраиваемая база данных. SQLite на 100% покрыт тестами — это общеизвестный факт. Каждый релиз прогоняется 5 миллионов тестов — все виды тестирования. С 2009 года SQLite полностью покрыт тестами — это написанный на C оптимизированный код, который можно читать. Единственный код на C, который можно читать, из тех, что я встречал. И кроме того, этот код в паблик-домене. То есть он доступен, мы его можем читать, мы его можем использовать: открытая лицензия.  Но вопрос: как они это сделали? Если есть SwiftData, которая использует актуальные функции языка и под капотом CoreData, давайте сделаем свою SwiftData с блэкджеком и поддержкой iOS 14. У нас везде макросы.  Хочется понять, что происходит под капотом. Раскрываем макрос: @Model final class Post {         @Attribute(.unique)    var id: String    var message: String    var timestamp: Date       @Transient    var _$backingData: any BackingData<Post> = Post.createBackingData() } Видим: BackingData, PostCreateBackingData. Что такое BackingData? Как нам говорит открытая документация, это какой-то внутренний, ненужный класс. Там есть приватный класс, который реализует протокол BackingData, DefaultBackingData.  Если мы знаем класс, то знаем Objective-C, идем в рефлексию, находим очень странное property:  public extension BackingData {     var managedObject: NSManagedObject? {         guard let object = getMirrorChildValue(             of: self,             childName: ""_managedObject"") as? NSManagedObject else {             return nil         }         return object     } } Если мы его приведем к ManagedObject, то получаем, что каждый раз, когда создается SwiftData- класс, под капотом создается CoreData-сущность без контекста. Но конструктор специфический, я его не сразу узнал, потому что раньше Entity приходилось строкой писать.  За счет конструктора мы можем в SwiftData оторвать модели от контекста, потому что так же, как было с Alloc и Nid, мы отдельно создаем модельку и отдельно insert-им ее в контекст.  Мы можем убедиться в том, что у созданного объекта есть ассоциированный managed-объект. То есть мы создали SwiftData-объект POST и у него же можем за счет нашей extension получить Cordat-объект.  Мы поняли то, что CoreData — это всего лишь текущая реализация в SwiftData и BackingData позволяет Apple подменить ее другими реализациями. То есть если сейчас BackingData имеет только DefaultBackingData-класс, то потом может быть NoSQLBackingData и это будет иметь свои преимущества в других use-кейсах.  За счет макросов мы поняли, как Apple смогла добиться независимости модели от контекста несмотря на движок CoreData под капотом, но это не дало ответ, как реализовать то же самое самим с таргетом ниже iOS 17. Мы все еще не можем сделать свою SwiftData на основе CoreData, потому что акторы — те, что нам предоставляет SwiftData, — доступны в конкретной реализации iOS 17, потому что весь обзервинг работает через новый обзерв, который тоже доступен только в iOS 17. И, видимо, именно из-за этих концептуальных ограничений и полной совместимости с актуальными SwiftUI они так и сделали.  Мы можем разобраться с концептуальными киллер-фичами SwiftData и принести их в CoreData для упрощенной миграции в будущем. Во-первых, хочется получить Code-First-подход. Во- вторых, хочется использовать макросы для Compile-Time-Safety. Хочется отвязать ManagedObjects от ManagedObjects-контекстов, чтобы иметь чистый CRUD и дальше SwiftData была идентична при миграции. И хочется использовать наконец-таки SwiftConcurrency для потока безопасности в CoreData.  И тут мне на ум пришел open source framework CoreStore. Он уже умеет Code-First-подход и выглядит вот так: final class Post: CoreStoreObject {    @Field.Stored(“latitude"")    var id: String    @Field.Stored(“latitude"")    var message: String        @Field.Relationship(“attachments"")    var attachments: [Attachment]    @Field.Relationship(""author"")    var author: User } После того этот фреймворк нам предоставляет DataStack. То есть Model у нас имел Store и мы описывали списком все модели. Здесь мы не сможем оптимизировать до одной и опустить связи, но по концепции это идентично. Мы просто перечисляем все сущности, и через генерики Compile-Time-Safety достигается по фетчингу.  Пока я не видел ни одной реализации и не смог сам быстро реализовать похожие предикаты Compile-Time-Safe, но именно CoreStore уже содержит через Key-Value и через свой DSL безопасный, понятный фетчинг, где у нас есть var через Key-Value. Он понимает контекст, то есть из Favorite мы сравниваем с Bool, а PublishDate мы сравниваем с Date. И это будет валидно, и если мы попробуем сравнить не те типы данных, оно нам скажет, что что-то идет не так.  В CoreStore-библиотеке сортировка работает через Key-Value, предсказуемо и будет валидироваться в Compile-Time. Базовое Compile-Time мы получаем в Safety, и кажется, что, если пойти чуть глубже, то можно попробовать реализовать и Macro-Predicator на основе уже готового API для этого DSL. Но я до этого пока не дошел. Самое важное, что хочется получить в CoreData, что есть в SwiftData, — это потокобезопасность. Кажется, что за счет того, что у нас есть понимание, как сделаны акторы в SwiftData, мы можем принести ту же логику в CoreData. Нам ничего не мешает сделать Serial Executor на Serial Queue, который будет просто вызывать контекст perform, а он уже работает через Serial Execution, и это все обернуть в актор.  Если мы не сможем использовать по-другому CoreData, если мы заставим всех использовать ее через акторы, то мы получим потокобезопасность в CoreData. То же самое, что мы имеем в SwiftData. А выстрелить в ногу с CoreData можно будет всегда. И в целом, можно сказать, что мы пришли к цели.  В итоге мы получили:  1. Code First-подход — самое простое и базовое, что нам поможет потом в миграции.  2. Макросы и Compile Time Safety.  3. Интеграцию SwiftConcurrency. Да, не в том формате, в котором было это в SwiftData. Но если соблюдать те же самые правила и те же самые подходы, то, во-первых, мы сможем обучить наших коллег использовать те концепции, которые к нам придут со SwiftData. А во- вторых, мы сможем ограничивать и снижать когнитивную нагрузку при работе с многопоточной.  Подводя итоги  SwiftData меня разочаровала только при первом приближении, а потом я понял, что она действительно стоит того. Это не новый фреймворк, это та CoreData, которую мы ждали с iOS 5. Это правильная CoreData с использованием Swift, современных подходов к проектированию, программированию и всех самых главных возможностей.  За счет того, что Apple дала нам тулинг, мы можем достаточно просто мигрировать существующие проекты с громадными .xcdatamodel-ами на SwiftData.  В SwiftData реализовано все лучше, включая миграции. И за счет того, что мы имеем под капотом CoreData, а еще под капотом SwiftData, это работает предсказуемо. И мы будем уверены в том, что сможем найти решение, потому что мы сможем включить SQL debug и посмотреть, что за запросы идут.  Каждый раз, когда мы утыкались в производительность CoreData, помогал именно SQL debug. Мы просто смотрели логи и пытались оптимизировать запросы.  За счет того, что внутри SQLite есть тулинг для просмотра красивых схем вашей базы данных, для автоматического анализа, для профилирования ваших select-запросов. Есть даже инструменты, правда платные, которые самостоятельно могут профилировать все ваши запросы и давать советы по оптимизации. И это очень классно, что под капотом в целом у нас остается надежный проверенный инструмент, который оптимизирован донельзя. Это C. Но тем не менее в SwiftData появились классные фичи, которые в CoreData не получится встроить.  Т-Банк Компания 18 Карма 0 Рейтинг Андрей @Azon Автор iOS Broadcast, Разработчик в Тинькофф Публикации yanakuklina 12 часов назад Как мы автоматизировали обжарку кофе и доставляем 100 тысяч заказов в месяц почти без ошибок Простой 10 мин El_Gato_Grande 12 часов назад Need for Speed: Most Wanted. Как легендарная игра уничтожила всю серию Появились автосохранение, миграции, и SwiftData работает через макросы. Теперь у нас есть concurrency и есть макросы, которые нам позволяют работать.  Заходите в комментарии, если есть вопросы или истории, которыми хочется поделиться! Теги: coredata, swiftdata, swift, swiftui, ios Хабы: Блог компании Т-Банк, Разработка под iOS, Разработка мобильных приложений, Swift,  Разработка под macOS Комментарии 8 +8 ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 5.7K Кейс +66 33 19 +19 Подписаться +13 39 8 +8 Подписаться 10 мин kulyanin 4 часа назад Б/У жёсткие диски Western Digital под видом новых в крупном ритейлере Простой 3 мин MaFrance351 12 часов назад Кто сообщает картам, где едет ваш транспорт Средний 8 мин mikhailsudakov 12 часов назад История Ultimate Play the Game — легендарного разработчика игр для ZX Spectrum 26 мин EgorKotkin 5 часов назад Как Кремниевая долина превращает бум ИИ в новый пузырь доткомов 8 мин viktoshaf 7 часов назад Настройка BGP с Anycast: высокая доступность и отказоустойчивость Простой 10 мин 17K +55 10 60 +60 7K Из песочницы +48 21 53 +53 3.4K Обзор +35 22 22 +22 2K +29 9 17 +17 1.7K +28 5 10 +10 2K Туториал vorobeevich 7 часов назад GigaChat 2.0 в API Средний 14 мин VBart 9 часов назад Стильный современный «autoindex» в Angie/nginx без sms и сторонних модулей Простой 24 мин Wolfurud 11 часов назад Как я создавал Telegram-бота с помощью ChatGPT Простой 11 мин Показать еще ВАКАНСИИ КОМПАНИИ «Т-БАНК» QA-инженер (mobile) Т-Банк · Уфа Site Reliability Engineer (SRE) в команду ETL-платформы Т-Банк · Можно удаленно Frontend-разработчик Angular (middle+/senior) Т-Банк · Нижний Новгород · Можно удаленно Site Reliability Engineer (SRE) +26 10 0 3.5K Обзор +26 15 12 +12 890 Туториал +26 17 1 +1 5.2K Кейс +24 39 3 +3 Т-Банк · Можно удаленно .NET-разработчик Т-Банк · Пермь Больше вакансий на Хабр Карьере ИНФОРМАЦИЯ Сайт l.tbank.ru Дата регистрации 26 октября 2011 Дата основания 18 ноября 2005 Численность свыше 10 000 человек Местоположение Россия ВИДЖЕТ БЛОГ НА ХАБРЕ 12 мар в 19:30 Модели T-lite и T-pro: training report 10 мар в 20:28 3.8K 6 +6 ServerSocket для IPS в Android и примеры межпроцессного взаимодействия 7 мар в 17:05 Java Digest #22 3 мар в 19:10 Scala Digest. Выпуск 26 28 фев в 19:00 Что в черном ящике, или Как разработчику понять, что требует оптимизации в БД PostgreSQL 1.1K 3 +3 2.4K 0 680 0 8.4K 16 +16 Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2025, Habr "
15,"Математика наклона в картах, или как мы сделали небо _ Хабр.pdf",2ГИС,Главные по городской навигации,143.84,13 фев 2024 в 13:01,"Блог компании 2ГИС, Canvas*, Геоинформационные сервисы*, Математика*, WebGL","143.84 Рейтинг 2ГИС Главные по городской навигации Подписаться Недавно в карте 2ГИС появились небо и туман, которые можно увидеть, увеличив масштаб и наклон. В статье рассказываю, для чего нам понадобились эти фичи, с какими сложностями столкнулись в процессе исследований и как в итоге реализовали нужную функциональность.  Изначально карта не была готова к тому, чтобы её сильно наклоняли: при угле наклона, превышающему 45°, можно было наблюдать картинку, как на скриншоте ниже ↓ ndim321 13 фев 2024 в 13:01 Математика наклона в картах, или как мы сделали небо Средний 8 мин 8.8K Блог компании 2ГИС, Canvas*, Геоинформационные сервисы*, Математика*, WebGL* Кейс ✏️ Технотекст 2023 +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Травка зеленеет, солнышко блестит, а скидки растут Дальний край карты располагался слишком близко к камере, карта заканчивалась и начиналась пустота, окрашенная в бежевый цвет. Крупные объекты, такие как парки, торговые центры, высотки, обрезались. Это убивало реалистичность карты.  К тому же в 2023-м в 2ГИС появились первые иммерсивные модели зданий. И для сценариев, где пользователи захотят детальнее рассмотреть новые 3D-модели, также стала необходима проработка механики работы с большим наклоном. Что такое наклон карты Сначала разберёмся, что такое наклон и к чему приводит его изменение. Нам понадобится немного математики. В двумерном пространстве рассмотрим прямую A и точку C, которую будем называть камерой. Направление камеры — это ненулевой вектор, начинающийся в камере. Hc - это перпендикуляр опущенный из камеры на прямую A. Тогда наклон (α) — это угол между Hc, и направлением камеры. Для простоты считаем, что Введём ещё одно определение: видимая область (X) — это длина отрезка, отсекаемого от прямой А направлением камеры и перпендикуляром. Тогда При увеличении угла наклона будет увеличиваться и размер видимой области, а при α = 90° размер области X равен бесконечности. Также обратим внимание на производную по α: Можно сделать вывод, что даже малое приращение угла α сильно увеличивает видимую камерой область X. Теперь рассмотрим то, как работает видимая область нашей карты. Усложним случай, рассмотренный выше: вместо прямой A камера будет смотреть на плоскость. Видимая область двумерная, а именно, трапеция, высота которой прямо пропорциональна tg(α). Пример видимой области карты Пример видимой области карты Именно эту видимую область пользователь видит при просмотре карты, а мы должны заполнить её данными. Главный вывод на текущий момент: увеличение наклона ведёт к быстрому увеличению видимой области — следовательно, нам нужно значительно больше данных. Сколько нужно данных Данные до пользователя мы доставляем, используя векторные тайлы, которые располагаются в квадратной сетке. В зависимости от величины масштаба карты размер и детальность тайлов меняются. Для меньшего масштаба применяются более крупные тайлы с меньшей детальностью, а для большего масштаба — наоборот. Чтобы заполнить всю видимую область данными, нужно найти все тайлы, которые видны из неё, то есть где хотя бы одна точка тайла находится внутри видимой области. Зелёная трапеция — видимая область. Синие квадраты — тайлы, которые нужно показать. Здесь размер тайла — 1,2 км² Зелёная трапеция — видимая область. Синие квадраты — тайлы, которые нужно показать. Здесь размер тайла — 1,2  км² Как мы уже выяснили, увеличение наклона приводит к быстрому увеличению размера видимой области, соответственно, быстро возрастает число тайлов, которые нужно загрузить по сети, обработать и нарисовать. Замостить тайлами всю видимую область для произвольного положения камеры мы не можем. Что же тогда делать? Попытка замостить видимую область По мере увеличения расстояния до тайла его полезность для пользователя снижается: он занимает всё меньший размер видимой области, детали сливаются и становятся нечитаемыми. Это натолкнуло нас на идею для исследования: можно попробовать использовать подход аналогичный LOD’ам в играх. А именно, использовать тайлы меньшей детальности по мере увеличения расстояния до камеры. В таких тайлах меньше данных, и они покрывают большую площадь, что может обеспечить покрытие большей части видимой области без создания каши из объектов вдали. Пример видимой области, заполненной тайлами разного размера. Красной стрелкой обозначена граница детальности Пример видимой области, заполненной тайлами разного размера. Красной стрелкой обозначена граница детальности Просто так использовать более крупные тайлы не получилось, мы столкнулись с двумя проблемами.  1. Для объектов, которые попадают в несколько тайлов, может возникнуть ситуация, когда они разрезаются границей, где начинаются тайлы меньшей детальности. Вот так, например, выглядит Москва Сити, если посмотреть на неё с определенного ракурса: Москва Сити пропал потому что в тайле меньшей детальности нет данных о высотах зданий Москва Сити пропал потому что в тайле меньшей детальности нет данных о высотах зданий 2. При изменении видимой области (например, вращении и перемещении камеры) на границе смены детальности можно увидеть мелькание объектов, так как они отсутствуют в менее детальных тайлах: Первую проблему решить в общем виде можно только на этапе подготовки векторных тайлов. На уровне движка мы могли только изменять удаление границы детальности от камеры. К сожалению, удаление границы детальности от камеры значительно увеличивает количество загружаемых тайлов и объектов, которые нужно нарисовать. Проблему номер два мы пытались решить, изменив геометрию области видимости. Теперь это была не трапеция, а сектор окружности с центром в центре карты определенного радиуса. Использование такой геометрии для видимой области усложнило бы определение видимых тайлов.  Пока что мы отказались от замощения видимой области тайлами меньшей детальности. Это требует больше ресурсов от пользовательских устройств и усложняет работу движка. И хоть замощение видимой области тайлами улучшает иммерсивность, в большинстве случаев это не так важно для пользователя, так как тайлы находятся вдали от камеры.  После всех проведенных исследований мы решили не изменять логику работы видимой области. Так как на текущий момент очень сложно добиться приемлемого результата за предсказуемые ресурсы. Единственное незначительное изменение, которое было сделано — максимальный размер видимой области на крупных масштабах был увеличен до размера тайла 15 зума. Это не оказывает влияния на работу карты при небольших наклонах и зумах, но позволяет хоть немного замостить видимую область вдали. Таким образом  карта стала выглядеть как на картинке выше. Единственное, что выглядело не очень — участок, выделенный красным прямоугольником. На краю карты простирается полотно светло-бежевого цвета, которое никак не напоминало небосвод. Поэтому для большей реалистичности мы решили отрисовать небо. Небо У нас было несколько экспериментов с градиентами. Обсуждали даже реалистичные модели, которые используют в играх. Они учитывают разные типы рассеивания на частицах в атмосфере, но пока от них отказались в угоду производительности. Даже при максимальном наклоне небо занимает всего лишь небольшой участок экрана. Самое примитивное решение — залить всё однотонным цветом, что мы в итоге и сделали. Дёшево, сердито, но эффективно. Реализуется это так: перед отрисовкой объектов карты на canvas рисуем прямоугольник нужного цвета на весь экран. В результате мы почти достигли нужного результата, но выглядит слишком примитивно. Начали работать с туманом. Туман У тумана на карте две цели. Первая — он должен скрыть некрасивую границу тайлов. Вторая — потенциально оптимизировать то, что он скрывает: либо совсем скрывать все здания за ним, либо рисовать их сильно менее детальными. Мы не можем использовать отдельный проход для создания тумана, потому что наша карта богата прозрачными 3D-объектами. Поэтому мы попробовали смешать цвета объектов с цветом тумана в зависимости от расстояния от этих объектов до камеры. Как функцию распределения интенсивности тумана мы выбрали обычную линию — это самый быстрый вариант. В играх часто используют нелинейные зависимости, потому что они дают более реалистичное распределение, но в картах нам пока это не нужно. Получается с какого-то расстояния туман становится видимым и при удалении от камеры он постепенно заслоняет собой объекты. Немного поигравшись с границами на близком зуме, мы получили примерно такую картинку. Слева — маска, по которой применяем туман, а справа — как это выглядит в итоге. Мы здесь немного увеличили настройки тумана, чтобы его было лучше видно Слева — маска, по которой применяем туман, а справа — как это выглядит в итоге. Мы здесь немного увеличили  настройки тумана, чтобы его было лучше видно Для этого в вершинном шейдере конкретного объекта нам нужно рассчитать расстояние от камеры до объекта в мировой системе координат и степень скрытия туманом: uniform mat4 u_mat4_model; uniform vec3 u_camera_position; uniform float u_fog_start_distance; varying vec3 v_relative_to_fog_position; void calculate_fog_position(vec3 object_position) {   // мировые координаты объекта   vec3 world_position = u_mat4_model * object_position;   // относительная величина, которая определяет степень скрытия объекта туманом   v_relative_to_fog_position = (world_position - u_camera_posision) / u_fog_start_distance; } Во фрагментном шейдере мы смешиваем цвет объекта и цвет тумана в зависимости от  v_relative_to_fog_position: uniform float u_float_fog_start; // граница начала тумана uniform float u_float_fog_end; // граница конца тумана (объекты дальше нее полностью скрыты  uniform vec3 u_vec3_fog_color; // цвет тумана varying vec3 v_relative_to_fog_position; vec4 apply_fog(vec4 color) {    float fog_factor = (length(v_relative_to_fog_position) - u_float_fog_start) / (u_float_fo    color.rgb = mix(color.rgb, u_vec3_fog_color, fog_factor); } Для некоторых объектов мы не смешиваем их цвета с цветом тумана, а увеличиваем их прозрачность (например POI). В этом подходе есть две очевидные и одна неочевидная проблемы. 1. Туман не влияет на небо, потому что оно не учитывается в глубине при рендеринге. Иными словами, мы видим слишком жёсткий переход на горизонте. 2. Иконки и некоторые другие 2D-объекты явно выбиваются из общей картины, но к их цвету нельзя просто применить цвет тумана, так же как к остальным объектам, поскольку они рисуются поверх всех остальных слоёв. 3. Неочевидно то, что если мы уменьшим масштаб карты, то монотонный туман окутает вообще всё. Первую проблему мы решили, добавив в шейдер неба информацию о тумане. uniform float u_float_fog_horizon_blend; // коэффициент блендинга тумана с небом varying vec3 v_relative_to_fog_position; float calculate_blending_factor(vec3 dir, float blend_factor) {   // рассчитывает вертикальный градиент, который обеспечивает переход туман-небо } vec4 apply_fog_to_sky(vec4 color) {    float depth = length(v_relative_to_fog_position);    vec3 dir = v_relative_to_fog_position / depth;    float blending_factor = calculate_blending_factor(dir, u_float_fog_horizon_blend);    color.rgb = mix(color.rgb, u_fog_color.rgb, blending_factor); } A ещё учли цвет неба при применении тумана ко всем объектам на карте. Со второй проблемой разобрались, снизив прозрачность иконок по тому же закону, по какому мы затуманиваем другие объекты. На наш взгляд, получилось довольно гармонично. Лишние иконки не мешают при навигации по карте, а читаемость того, что находится вблизи, не пострадала. А вот неочевидная проблема заслуживает отдельного параграфа. Туман на разных масштабах Предположим, мы настроили расстояние, которое нас устраивает и на котором растёт интенсивность тумана. И сделали это на близком масштабе. Картинка выглядит хорошо. При вращении камеры тоже не возникает проблем. Казалось бы, всё готово. Но это не так. Стоит отдалить камеру, уменьшив масштаб, как туман окутывает весь экран. Мы этого совсем не хотим — у нас всё-таки карта, а не Сайлент Хилл. Нам в первую очередь важна удобная навигация. Вот как эта проблема выглядит сбоку.  Всё, что находится за пределами сферы, полностью скрыто в тумане. Слева — вид, который мы получаем на предыдущих скринах. А справа расстояние до всех объектов настолько велико, что туман даёт 100-процентную заливку Всё, что находится за пределами сферы, полностью скрыто в тумане. Слева — вид, который мы получаем на  предыдущих скринах. А справа расстояние до всех объектов настолько велико, что туман даёт 100-процентную  заливку Изначально все наши идеи крутились вокруг более сложной функции тумана. Наиболее многообещающим вариантом казалось снижение интенсивности в конусе под камерой. Но мы не будем приводить все неудачные попытки решить эту проблему. В итоге пришли к очень простому варианту, который полностью покрыл наши требования: мы привязали распределение тумана к расстоянию до точки вращения камеры. Это выглядит примерно так. Что получилось В итоге получилось реализовать продуктовые требования: теперь карты на 2gis.ru можно рассматривать под большим наклоном и заодно лицезреть голубое небо. Особенно приятно, что мы смогли добиться желаемого эффекта и не потерять производительность карты. Сейчас большой наклон и туман работает на 2gis.ru, а так же доступен в нашем MapGL JS API. Следующим этапом опубликуем их в наших мобильных приложениях. 2ГИС Главные по городской навигации 12 Карма 0 Рейтинг @ndim321 Пользователь Если у вас остались вопросы, буду рад ответить в комментариях. А если захотите развивать трёхмерную карту в браузере с нами — в команде Web-карты сейчас как раз открыта вакансия. Теги: webgl, наклон, glsl Хабы: Блог компании 2ГИС, Canvas, Геоинформационные сервисы, Математика, WebGL Подписаться +59 48 17 +17 Подписаться Публикации yanakuklina 12 часов назад Как мы автоматизировали обжарку кофе и доставляем 100 тысяч заказов в месяц почти без ошибок Простой 10 мин El_Gato_Grande 12 часов назад Need for Speed: Most Wanted. Как легендарная игра уничтожила всю серию 10 мин kulyanin 4 часа назад Б/У жёсткие диски Western Digital под видом новых в крупном ритейлере Простой 3 мин MaFrance351 12 часов назад Кто сообщает картам, где едет ваш транспорт Средний 8 мин Комментарии 17 +17 ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 5.7K Кейс +66 33 19 +19 17K +55 10 60 +60 7K Из песочницы +46 21 53 +53 3.4K Обзор mikhailsudakov 12 часов назад История Ultimate Play the Game — легендарного разработчика игр для ZX Spectrum 26 мин EgorKotkin 5 часов назад Как Кремниевая долина превращает бум ИИ в новый пузырь доткомов 8 мин viktoshaf 7 часов назад Настройка BGP с Anycast: высокая доступность и отказоустойчивость Простой 10 мин vorobeevich 7 часов назад GigaChat 2.0 в API Средний 14 мин VBart 9 часов назад Стильный современный «autoindex» в Angie/nginx без sms и сторонних модулей Простой 24 мин +35 22 22 +22 2K +29 9 17 +17 1.7K +28 5 10 +10 2K Туториал +26 10 0 3.5K Обзор +26 15 12 +12 890 Туториал +26 17 1 +1 Wolfurud 11 часов назад Как я создавал Telegram-бота с помощью ChatGPT Простой 11 мин Нужен ли «ванильный» Kubernetes, когда есть платформа «Штурвал»: отвечают разработчики Турбо Показать еще МИНУТОЧКУ ВНИМАНИЯ ВОПРОСЫ И ОТВЕТЫ Почему не освобождается куча при уничтожении Unitywebgl приложения? Unity · Средний · 1 ответ Автоматизация подбора координат текселей? Машинное обучение · Средний · 1 ответ Как сделать, чтобы игра(Web app) растягивалаь на весь экран? Unity · Простой · 0 ответов Где и как найти переменные локальной игры на html5? Chrome · Простой · 1 ответ Как в Unity WEBGL передать фокус со страницы в приложение? Unity · Средний · 0 ответов 5.2K Кейс +24 39 3 +3 Девушка с розовыми волосами и Слизень на планете Рбах IT-комикс Kubernetes без боли: пробуем бесплатный «Штурвал CE» Турбо «Люк, я твой фактор!» — защищаем подключения с MFA Турбо Больше вопросов на Хабр Q&A ИНФОРМАЦИЯ Сайт 2gis.ru Дата регистрации 9 августа 2008 Дата основания 25 апреля 1999 Численность 1 001–5 000 человек Местоположение Россия ПРИЛОЖЕНИЯ 2ГИС 2ГИС — это офлайн-карты, справочник с контактами организаций и удобный навигатор по городу. Android iOS Отелло Отелло – надёжный сервис бронирования отелей, гостиниц, апартаментов, хостелов, гостевых домов, баз отдыха и других объектов размещения. iOS Android ССЫЛКИ Вакансии компании job.2gis.ru 2ГИС — справочник и карта 2gis.ru Продукты для разработчиков dev.2gis.ru 2ГИС Технологии — доклады о технологиях, продуктах и дизайне techno.2gis.ru Новое в 2ГИС delivered.2gis.ru Работа для разработчиков С++ cpp.2gis.ru Телеграм-канал t.me ВКОНТАКТЕ БЛОГ НА ХАБРЕ 12 мар в 16:51 Конфигурируемая тайловая разрезка: ускоряем отрисовку карты изменением данных 5 мар в 19:45 «Доставили»: как мы превратили релиз-ноуты в продуктовый блог 30 янв в 15:58 Менеджер данных: как новая роль изменила подход к работе с ML 23 янв в 17:05 Забудь про XPath и CSS-селекторы: путь от стандартных локаторов к кастомным 26 дек 2024 в 19:47 Как генерация тестовых данных вернула доверие к тестам ИСТОРИИ 1.1K 4 +4 1.9K 2 +2 3.7K 10 +10 3K 8 +8 2.9K 11 +11 Годнота от компаний Чего ждать от ИИ: угадываем будущее в новом сезоне С праздником весны! Blue Ghost М1 прилунился Всегда котов! IT с п мета Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2025, Habr "
16,На каком железе анализировать огромный вал информации_ _ Хабр.pdf,МТС,Про жизнь и развитие в IT,2233.32,8 фев 2019 в 15:00,"Блог компании МТС, Apache*, Big Data*, Hadoop*, IT-инфраструктура","2233.32 Рейтинг МТС Про жизнь и развитие в IT Подписаться Мы – Big Data в МТС и это наш первый пост. Сегодня расскажем о том, какие технологии позволяют нам хранить и обрабатывать большие данные так, чтобы всегда хватало ресурсов для аналитики, и затраты на закупки железа не уходили в заоблачные дали. О создании центра Big Data в МТС задумались в 2014 году: появилась необходимость масштабирования классического аналитического хранилища и BI-отчетности над ним. На тот момент движок для обработки данных и BI были SASовские – так сложилось исторически. И хотя потребности бизнеса в хранилище были закрыты, со временем функционал BI и ad-hoc- аналитики поверх аналитического хранилища разросся настолько, что нужно было решать вопрос увеличения производительности, учитывая, что с годами количество пользователей увеличилось в десятки раз и продолжало расти. В результате конкурса в МТС появилась MPP-система Teradata, покрывающая потребности телекома на тот момент. Это стало толчком к тому, чтобы попробовать что-то более популярное и open source’вое. info_habr 8 фев 2019 в 15:00 На каком железе анализировать огромный вал информации? 8 мин 8.2K Блог компании МТС, Apache*, Big Data*, Hadoop*, IT-инфраструктура* +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Сезон футурологии на Хабре: и будущее, и призы уже здесь На фото — команда Big Data МТС в новом офисе «Декарт» в Москве Первый кластер был из 7 нод. Этого было достаточно для проверки нескольких бизнес-гипотез и набивания первых шишек. Старания не прошли даром: Big Data в МТС существует уже три года и сейчас анализ данных задействован почти во всех функциональных направлениях. Команда выросла из трех человек в две сотни. Мы хотели иметь легкие процессы разработки, быстро проверять гипотезы. Для этого нужно три вещи: команда со стартаповским мышлением, легковесные процессы разработки и развитая инфраструктура. Про первое и второе много где можно почитать и послушать, а вот про развитую инфраструктуру стоит рассказать отдельно, ведь здесь важны legacy и источники данных, которые есть в телекоме. Развитая data-инфраструктура – это не только построение data lake, детального слоя данных и слоя витрин. Это еще и инструменты, и интерфейсы доступа к данным, изоляция вычислительных ресурсов под продукты и команды, механизмы доставки данных до потребителей – как в real-time, так и в batch-режиме. И многое-многое другое. Все эта работа у нас выделилась в отдельное направление, которое занимается разработкой коммунальных сервисов и инструментов по работе с данными. Это направление называется ИТ- платформой Big Data. Откуда в МТС берется Big Data В МТС достаточно много источников данных. Один из основных — базовые станции, мы обслуживаем абонентскую базу более 78 млн абонентов в России. Также у нас много сервисов и услуг, которые не относятся к телекому и позволяют получать более разносторонние данные (электронная коммерция, системная интеграция, интернет вещей, облачные сервисы и др. — весь “нетелеком” приносит уже около 20% от всей выручки). Кратко нашу архитектуру можно представить в виде такого графика: Как видно на графике, Dаta Sources могут отдавать информацию в режиме реального времени. Мы используем stream layer – можем обрабатывать информацию real-time, извлекать из нее какие-то события, которые нам интересны, и на этом строить аналитику. Для того, чтобы обеспечить такой процессинг событий, — мы разработали достаточно стандартную реализацию (с точки зрения архитектуры) с использованием Apache Kafka, Apache Spark и кода на языке Scala. Информация, получаемая в результате такого анализа, может потребляться как внутри МТС, так и в будущем вовне: бизнесу часто интересен сам факт совершения определенных действий абонентов. Есть еще режим загрузки данных пачками – batch layer. Обычно загрузка происходит раз в час по расписанию, в качестве планировщика у нас используется Apache Airflow, а сами процессы batch-загрузки у нас реализованы на python. В этом случае в Data Lake загружается значительно бОльший объем данных, необходимый для наполнения Big Data историческими данными, на которых должны обучаться наши Data Science-модели. В результате формируется профиль абонента в историческом разрезе на основе данных о его сетевой активности. Это позволяет получить предсказательную статистику и строить модели поведения человека, даже создавать его психологический портрет – есть у нас такой отдельный продукт. Эта информация очень полезна, например, для маркетинговых компаний. Также у нас есть большой объем данных, которые составляют классическое хранилище. То есть мы агрегируем информацию по различным событиям – как пользовательским, так и сетевым. Все эти обезличенные данные также помогают более точно предсказывать интересы пользователей и события, важные для компании – например, прогнозировать возможные сбои оборудования и вовремя устранять неполадки. Hadoop Если смотреть в прошлое и вспоминать, как вообще появились большие данные, то нужно отметить, что в основном накапливание данных производилось в маркетинговых целях. Нет такого четкого определения, что такое большие данные – это гигабайт, терабайт, петабайт. Невозможно провести черту. Для одних большие данные – это десятки гигабайт, для других – петабайты. Так сложилось, что с течением времени во всем мире накопилось очень много данных. И для того, чтобы провести какую-то более-менее значимую аналитику этих данных, обычных хранилищ, которые развивались с 70-х годов прошлого века, оказывалось уже недостаточно. Когда начался вал информации в 2000-е, 10-е годы и когда появилось очень много устройств, которые имели выход в интернет, когда появился интернет вещей, эти хранилища уже просто концептуально не справлялись. В основе этих хранилищ лежала реляционная теория. То есть были реляции разных форм, которые взаимодействовали друг с другом. Была система описания, как строить и проектировать хранилища. Когда не справляются старые технологии – появляются новые. В современном проблема аналитики больших данных решается двумя путями: Создание своего фреймворка, которое позволяет обрабатывать большие объемы информации. Обычно это распределенное приложение из многих сотнях тысяч серверов – как у Google, Яндекс, которые создали свои распределенные базы данных, позволяющие работать с таким объемом информации. Развитие технологии Hadoop – фреймворк для распределенных вычислений, распределенная файловая система, которая может хранить и обрабатывать очень большой объем информации. Инструменты Data Science в основном совместимы с Hadoop и такая совместимость открывает много возможностей для продвинутого анализа данных. Многие компании, в том числе и мы, идут в сторону open source Hadoop экосистемы. Центральный Hadoop-кластер у нас находится в Нижнем Новгороде. Там аккумулируется информация практически со всех регионов страны. По объему сейчас туда можно загрузить около 8,5 петабайт данных. Также в Москве у нас есть отдельные RND кластеры, где мы проводим эксперименты. Так как у нас в разных регионах около тысячи серверов, где мы проводим аналитику, а также планируется расширение – то возникает вопрос о правильном выборе оборудования для распределенных аналитических систем. Можно купить оборудование, достаточное для хранения данных, но которое окажется непригодным для аналитики – просто потому, что не будет хватать ресурсов, количества ядер CPU и свободной оперативной памяти на узлах. Важно найти баланс, чтобы получить хорошие возможности для аналитики и не очень высокую стоимость оборудования. Компания Intel предложила нам разные варианты, как можно оптимизировать работу с распределенной системой для того, чтобы аналитику по нашему объему данных можно было получать за приемлемые деньги. Intel развивает технологию твердотельных накопителей NAND SSD. Она в сотни раз быстрее, чем обычный HDD. Чем нам это хорошо: SSD, особенно с NVMe интерфейсом, обеспечивает достаточно быстрый доступ к данным. Плюс Intel выпустила серверные твердотельные накопители Intel Optane SSD на основе нового типа энергонезависимой памяти Intel 3D XPoint. Они справляются с интенсивными смешанными нагрузками на систему хранения, и обладают бОльшим ресурсом, чем обычные NAND SSD. Чем нам это хорошо: Intel Optane SSD позволяет стабильно работать под большими нагрузкам с малой задержкой. Мы изначально рассматривали NAND SSD как замену традиционных жестких дисков, потому что у нас очень большой объем данных перемещается между жестким диском и оперативной памятью – и нам нужно было оптимизировать данные процессы. Первый тест Первый тест мы провели в 2016 году. Мы просто взяли и попытались заменить HDD на быстрый NAND SSD. Для этого мы заказали семплы нового накопителя Intel – на тот момент это был DC P3700. И прогнали стандартный тест Hadoop – экосистемы, которая позволяет оценить, как изменяется перфоманс в разных условиях. Это стандартизированные тесты TeraGen, TeraSort, TeraValidate. TeraGen позволяет «нагенерить» искусственных данных определенного объема. Мы для примера взяли 1 ГБ и 1 ТБ. С помощью TeraSort мы отсортировали этот объем данных в Hadoop. Это довольно ресурсоемкая операция. И последний тест – TeraValidate – позволяет убедиться, что данные отсортированы в нужном порядке. То есть мы проходим по ним второй раз. В качестве эксперимента мы взяли машинки только с SSD – то есть Hadoop установили только на SSD без использования жестких дисков. Во втором варианте SSD мы использовали для хранения временных файлов, HDD – для хранения основных данных. И в третьем варианте жесткие диски использовались и для того, и для другого. Результаты этих экспериментов нас не очень обрадовали, потому что разница в показателях эффективности не превышала 10-20%. То есть мы поняли, что Hadoop не очень дружит с SSD в плане хранения, потому что изначально система была создана для хранения больших данных на HDD, и никто ее особенно не оптимизировал под быстрые и дорогие SSD. А так как стоимость SSD на тот момент была достаточно высокой, мы решили пока в эту историю не идти и обойтись жесткими дисками. Второй тест Затем у Intel появились новые серверные Intel Optane SSD на базе памяти 3D XPoint. Они вышли в конце 2017 года, но семплы нам были доступны ранее. Характеристики памяти 3D XPoint позволяют использовать Intel Optane SSD в качестве расширения оперативной памяти в серверах. Так как мы уже поняли, что решить проблему производительности IO Hadoop на уровне устройств блочного хранения будет непросто, то решили попробовать новый вариант – расширение оперативной памяти с помощью технологии Intel Memory Drive Technology (IMDT). И в начале этого года мы одни из первых в мире её протестировали. Чем нам это хорошо: это дешевле, чем оперативная память, что позволяет собирать сервера, которые обладают терабайтами оперативной памяти. А так как оперативная память достаточно быстро работает – в нее можно загружать большие дата-сеты и анализировать их. Напомню, что особенность нашего аналитического процесса в том, что мы несколько раз обращаемся к данным. Для того чтобы какой-то анализ сделать, мы должны как можно больше данных загрузить в память и «прокрутить» несколько раз какую-то аналитику этих данных. Английская лаборатория Intel в г. Суиндон выделила нам кластер из трех серверов, который в ходе тестов мы сравнили с нашим тестовым кластером, расположенными в МТС. Как видно из графика, по итогам теста мы получили достаточно хорошие результаты. Тот же TeraGen показал увеличение производительности практически в два раза, TeraValidate – на 75%. Это очень хорошо для нас, потому что, как я уже сказал, мы несколько раз обращаемся к данным, которые у нас находятся в памяти. Соответственно, если мы получаем такой прирост по производительности, это нам особенно хорошо поможет в анализе данных, особенно в реал- тайме. Мы провели три теста при разных условиях. 100 ГБ, 250 ГБ и 500 ГБ. И чем больше мы использовали памяти, тем Intel Optane SSD с Intel Memory Drive Technology лучше показывали себя. То есть чем больше данных мы анализируем, тем больше получаем эффективности. Аналитика, которая проходила на большем количестве узлов, может проходить на меньшем их количестве. И еще мы получаем достаточно большой объем памяти на наших машинах, что очень хорошо для Data Science-задач. По итогам тестов мы приняли решение закупить эти накопители для работы в МТС. Если вам тоже приходилось выбирать и тестировать железо для хранения и обработки большого объема данных – нам будет интересно почитать, с какими трудностями вы при этом столкнулись и к каким результатам в итоге пришли: пишите в комментариях. Авторы: Руководитель центра компетенций прикладной архитектуры департамента Big Data МТС Григорий Коваль  Руководитель трайба управления данными департамента Big Data МТС Дмитрий Шостко Теги: мтс, it-инфраструктура, it-архитектура, hardware, hadoop, apache spark Хабы: Блог компании МТС, Apache, Big Data, Hadoop, IT-инфраструктура grigory_koval @ zloi_diman @ МТС Про жизнь и развитие в IT 31 Карма 0 Рейтинг @info_habr Пользователь Публикации yanakuklina 12 часов назад Как мы автоматизировали обжарку кофе и доставляем 100 тысяч заказов в месяц почти без ошибок Простой 10 мин El_Gato_Grande 12 часов назад Need for Speed: Most Wanted. Как легендарная игра уничтожила всю серию 10 мин kulyanin 4 часа назад Б/У жёсткие диски Western Digital под видом новых в крупном ритейлере Комментарии 2 +2 ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 5.7K Кейс +66 33 19 +19 17K +55 10 60 +60 Подписаться +9 28 2 +2 Подписаться Простой 3 мин MaFrance351 12 часов назад Кто сообщает картам, где едет ваш транспорт Средний 8 мин mikhailsudakov 12 часов назад История Ultimate Play the Game — легендарного разработчика игр для ZX Spectrum 26 мин EgorKotkin 5 часов назад Как Кремниевая долина превращает бум ИИ в новый пузырь доткомов 8 мин viktoshaf 7 часов назад Настройка BGP с Anycast: высокая доступность и отказоустойчивость Простой 10 мин vorobeevich 7 часов назад GigaChat 2.0 в API Средний 14 мин 7K Из песочницы +46 21 53 +53 3.4K Обзор +35 22 22 +22 2K +29 9 17 +17 1.7K +28 5 10 +10 2K Туториал +26 10 0 3.5K Обзор VBart 9 часов назад Стильный современный «autoindex» в Angie/nginx без sms и сторонних модулей Простой 24 мин Wolfurud 11 часов назад Как я создавал Telegram-бота с помощью ChatGPT Простой 11 мин Показать еще ВАКАНСИИ КОМПАНИИ «МТС» Разработчик 1С [Центр 1С] МТС · Москва · Можно удаленно Стажер Frontend разработчик [МТС Докс] МТС · Можно удаленно Ведущий системный архитектор [МТС Франчайзинг] МТС · Можно удаленно Middle инженер технической поддержки [МТС Партнер] МТС · Краснодар Junior Frontend VUE разработчик [МТС Докс] МТС · Можно удаленно Больше вакансий на Хабр Карьере +26 15 12 +12 890 Туториал +26 17 1 +1 5.2K Кейс +24 39 3 +3 ИНФОРМАЦИЯ Сайт www.mts.ru Дата регистрации 16 августа 2016 Дата основания 16 апреля 1993 Численность свыше 10 000 человек Местоположение Россия ССЫЛКИ Вакансии тут career.habr.com IT-сообщество True Tech t.me Про IT в МТС mts-digital.ru Сайт цифровой экосистемы mts.ru БЛОГ НА ХАБРЕ 8 часов назад Не только хардкор: что читают R&D-инженеры, когда пытаются отдохнуть 13 часов назад Code, maturity, tools: как мы развиваем QA-практики в МТС 12 мар в 17:00 Миссия выполнима: стандартизировать производственный процесс в крупной компании и учесть запросы всех продуктовых команд 12 мар в 12:00 Bobcat Miner 300 — жизнь после забвения. Как я получил мощный одноплатник почти бесплатно 518 4 +4 255 0 414 0 6.9K 15 +15 11 мар в 17:00 Развиваем soft и точечно прокачиваем hard skills: подборка книг от СТО. Часть 3 1.8K 2 +2 Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2025, Habr "
17,На светлом-светлом Хабре появилась тёмная-тёмная тема _ Хабр.pdf,Хабр,"Экосистема для развития людей, вовлеченных в IT",540.17,4 апр 2024 в 10:05,"Блог компании Хабр, Веб-дизайн*, Веб-разработка*, Habr, Usability","4.71 Оценка 540.17 Рейтинг Хабр Экосистема для развития людей, вовлеченных в IT Подписаться Привет, Хабр! Мы приготовили для вас долгожданный подарок (из заголовка вы уже поняли, какой), но мы вам его просто так, как говорится, не отдадим ? Boomburum 4 апр 2024 в 10:05 На светлом-светлом Хабре появилась тёмная-тёмная тема Простой 6 мин 23K Блог компании Хабр, Веб-дизайн*, Веб-разработка*, Habr, Usability* Ретроспектива КАК СТАТЬ АВТОРОМ Зарплаты айтишников Найдено: весенние мероприятия для айти… Глазосберегательные технологии TL;DR для нетерпеливых: переходите на dark.habr.io и следуйте инструкциям Как мы делали тёмную тему У вас бывает такое, что устали и ни на что уже нет сил, но вдруг открывается второе дыхание и вы способны сделать марш‑бросок, перевернуть горы?! У нас в компании к концу года такое периодически случается. И нет, речь не о горящих дедлайнах и попытках доделать всё недоделанное. Мы обычно берём и хакатоним что‑то — вдобавок ко всему тому, что и так делаем. Так, например, в конце декабря был реанимирован и запущен АДМ. Похожим образом отдел разработки сделал Чёрные списки, которые мы анонсировали в конце марта. А ещё мы решили захакатонить — ни много, ни мало — тёмную тему! Ту самую, которую вы просили сделать чуть ли не с момента появления Хабра. Ту, которую многие энтузиасты пытались создать через userCSS и прочие расширения. Ту, которую мы теперь официально внедряем на Хабр и будем поддерживать. Немного предварительных ласк о том, как мы устроили тёмную пользователям Хабра: Сергей Продакт всея Хабра То, что тёмная тема нужна, я понимал не как продакт, а как один из пользователей Хабра. У меня самого болели глаза вечерами, но ничего не поделаешь, работа есть работа. А у меня ещё и есть некоторые проблемы со светлыми интерфейсами — глаза очень чувствительны к яркому свету, так что 24/7 nightmode на всех мониторах — это прям моё. В общем, тёмная тема была в том числе моей личной болью и целью, одной из больших вех работы в Хабре и над Хабром. +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп Подходить к тёмной теме мы долгое время боялись, поскольку Хабр настолько огромный, что не то что поддерживать, а сделать её без косяков не представлялось возможным. А ещё и «старый Хабр», с которым тёмная тема вообще была чем‑то из области фантастики (учитывая сложность разработки и поддержки), на что бизнес не был готов тратить деньги. Так что подступались осторожно и издалека. В 2023 мы, наконец, системно подошли к решению этой проблемы, начав работать над общей цветовой палитрой всех сервисов, что открывало возможность сделать для Хабра вторую палитру, тёмную. И всё бы хорошо, но приоритет у этой задачи на 2023 год был не самый высокий и её приходилось отодвигать на второй план в угоду более важным фичам и проектам. К декабрю 2023 палитра была готова примерно на 50%, что не оставляло надежд на релиз в уходящем году. Одним декабрьским вечером случилось очередное локальное «доколе» — после стендапа мы остались с дизайнером и разработчиком, чтобы обсудить и прикинуть, что мы можем сейчас сделать и какие трудности предстоит преодолеть. Решили, что на MVP нам нужно недели две — но были сильно ненулевые шансы, что на выходе будет большая куча багов и катить это в прод будет нельзя, но по крайней мере, это будет отправной точкой, после которой задний ход уже не дать. На том и порешили. Степан Чёрный пояс по дизайну, человек-борода Наша работа над дизайн‑системой началась давно, каждый раз мы пытались приступить к ней, но что‑то всегда становилось на пути: от событий в мире до более важных коммерческих задач — они просто не давали нам вдумчиво сесть и поработать над сайтом. Но в середине 2023 года мы поймали себя на мысли, что времени никогда не бывает достаточно, поэтому просто нужно взять и начать. Не совру, если скажу, что работы там был вагон и маленькая тележка: одних только цветов на сайте и в макетах было более 300! Многие из них были так похожи друг на друга, что иногда даже дизайнеры сами терялись, где использовать тот или иной цвет. Благодаря нашим усилиям и куче экспериментов удалось сократить палитру в несколько раз, до 40 цветов. И, что самое важное, составили таблицу перехода цветов для разработчиков, чтобы всё было как по маслу. На одном из декабрьских стендапов наш продакт Сергей в очередной раз грустно вздохнул, что у нас нет тёмной темы, а пользователи всё просят и просят. Я подумал: «Мы же теперь можем сделаем это относительно быстро!». Оказалось, что фронтенд‑разработчики с нами согласны. И уже на следующий день мы встретились с Денисом (создатель Хабра — прим.), озвучили свои планы — постараемся сделать быстро, но возможны баги, которые вскоре исправим. И нам дали зелёный свет! Мы вдохновились, но боялись не успеть к Новому году. Забегая вперёд — так оно и вышло:) В тот же вечер я взялся за работу: начал подбирать цвета для тёмной темы и натягивать их на макеты. Несколько концертов на YouTube, несколько чашек кофе и пицца в помощь — и вуаля! Я перекрасил все макеты, создал палитры для тёмной и светлой темы и подготовил разработчикам json‑файл. Закончив в 6 утра, я решил, что сделал всё возможное и отправился спать. Было трудно поработать ещё 11 часов после рабочего дня, но я осознавал, что макеты нужны для быстрого ответа на вопрос от фронтов «Какой цвет должен быть тут?» С нетерпением все ждали, когда разработчики натянут на сайт обе палитры и пофиксят баги. Внезапный Boomburum: были пользователи, которые думали, что тёмную тему можно «сделать за вечер» путём инвертирования цветов — они же правы? Правы же??  Слушай, ну для кого‑то такой подход, может быть, и приемлем, но для меня было очевидно, что надо идти другим путём. Для быстрой демонстрации инвертировать цвета можно с помощью плагинов в Figma буквально за минуту, но получается треш — вырвиглазные сочетания, чернющий фон, ослепляющий белый текст — такое сильно бьёт по глазам. Поэтому при составлении палитры для тёмной темы я брал несколько макетов и в ручном режиме подбирал, каким должен быть фон, каким должен быть текст и т. п. Когда получился основной костяк из 10–15 цветов, все остальные уже подгонял под них, чтобы они нормально сочетались и не противоречили друг другу. Эту основу из 15 цветов показал на встрече с Денисом, чтобы сложилось понимание и единое видение, а не попытка продать кота в мешке. Это было отличное решение. Из того, что ещё запомнилось: на Хабре несколько сотен хабов и у каждого своя иконка. У большинства хабов были чёрные аватарки, которые сливались с тёмным фоном. Решение — перерисовано 135 иконок. Теперь красота, выглядит это примерно так: Катя Фронтендных дел мастер Степан выше уже рассказал про 300+ цветов, раскиданных за 18 лет по всему проекту — такому количеству оттенков серого позавидует даже Кристиан Грей ? Всё это меня удивило, когда я пришла в Хабр, но сейчас я очень рада, что удалось всё наконец‑то причесать. Это первые шаги к цельному UI-киту. В декабре мы перевели оставшиеся внутренние пакеты на единую палитру и начали перекрашивать интерфейс. Первую альфа‑версию мы раскатили в конце декабря (последняя запись в чейнджлоге 2023 не даст соврать), но очень скоро поняли, что новогоднего подарка для пользователей не получится — коллеги оформляли тонны багов в самых неожиданных местах, поэтому мы приняли решение больше никому не показывать и проработать получше в новом году. Для понимания масштабов Хабра — поток багов от коллег стал затихать только к концу февраля. Но в итоге сделали это! Несмотря на то, что с момента «давайте пилить тёмную тему» до её публичного релиза прошёл целый квартал, мы постарались за это время отловить большинство багов во всевозможных местах. Но не исключаем того, что где‑то что‑то не заметили, и поэтому очень рассчитываем на вашу помощь — пишите в комментариях или в личку, если заметили, что где‑то на сайте что‑то не так. Как включить тёмную тему Историю с запуском тёмной темой решила поддержать компания Yandex Cloud, вместе с которой мы сделали «рубильник» для включения тёмной темы: ищите блок «Хабр × Yandex Cloud» в правой колонке на главной странице Хабра и жмите «Изменить настройки темы». Ну или переходите по ссылке ниже на божественной красоты лендинг: открывайте, читайте и следуйте инструкциям — помимо активации фичи вас ждёт несколько увлекательных активностей с заданиями, за выполнение которых у вас будет шанс выиграть классный мерч от Yandex Cloud и Хабра: dark.habr.io Есть более короткий путь до активации тёмной темы (который будет использоваться позже), но пока я очень прошу не сообщать о нём в комментариях ? Переходите на тёмную сторону тему! Теги: Habr, тёмная тема Хабр Экосистема для развития людей, вовлеченных в IT Хабр Карьера Facebook Twitter ВКонтакте Instagram 1.2K Карма 69.4 Рейтинг Алексей @Boomburum Руководитель отдела поддержки пользователей Хабра Сайт Facebook Twitter ВКонтакте Публикации ky0 10 часов назад Ваше мнение очень важно для нас (нет) 3 мин alizar 10 часов назад Глупо покупать технику последней модели, если старая работает хорошо. И можно собрать ПК из комплектующих Простой 6 мин Хабы: Блог компании Хабр, Веб-дизайн, Веб-разработка, Habr, Usability Если эта публикация вас вдохновила и вы хотите поддержать автора — не стесняйтесь нажать на кнопку Задонатить Комментарии 164 +164 ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 4.7K Кейс +77 14 112 +112 7.7K Мнение Подписаться Подписаться omyhosts 22 часа назад Артефакт эпохи: рождение, взлет и падение клипарта Простой 6 мин ru_vds 6 часов назад Я мучился с Git, поэтому создал про него игру Средний 8 мин DRoman0v 11 часов назад Очередная прогулка по барахолке в Испании: отличный ноутбук и много необычных штук 4 мин RationalAnswer 14 часов назад Уголовный кодекс vs Блиновская и Митрошина, а также вебкамщицы со стволами против криптоворов 11 мин Stepan_Burmistrov 9 часов назад Использование лидара от робота-пылесоса для системы предотвращения столкновений в автономных роботах Средний 22 мин +36 23 42 +42 3.4K Ретроспектива +29 12 22 +22 3.8K Кейс Перевод +24 36 5 +5 6.3K +24 3 10 +10 12K Дайджест +23 8 35 +35 2.6K Туториал ProstoKirReal 11 часов назад Сложно о простом. Как работает интернет. Часть 4. Что такое LAN, MAN, WAN, сети Clos и иерархия операторов Средний 20 мин savpek 9 часов назад От психолога до эльфа 80-го уровня: как создать свою уникальную роль для нейросети в домашних условиях и не сойти с ума Простой 26 мин dspmsu 12 часов назад Как я решал задачу 2025 года. Часть 1 Средний 9 мин Показать еще ВАКАНСИИ КОМПАНИИ «ХАБР» Ruby on Rails разработчик от 240 000 до 300 000 ₽ · Хабр · Москва · Можно удаленно Маркетолог (Хабр Карьера) от 100 000 до 120 000 ₽ · Хабр · Москва · Можно удаленно Больше вакансий на Хабр Карьере +20 37 10 +10 5K +19 94 6 +6 1.6K Туториал +17 21 0 1K +14 7 0 ИНФОРМАЦИЯ Сайт habr.com Дата регистрации 9 августа 2008 Дата основания 26 мая 2006 Численность 51–100 человек Местоположение Россия Представитель Алексей ССЫЛКИ Хабр Карьера career.habr.com Хабр Q&A qna.habr.com ВИДЖЕТ ВКОНТАКТЕ Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Настройка языка Техническая поддержка © 2006–2025, Habr БЛОГ НА ХАБРЕ 6 мар в 15:32 Как написать статью о программировании и покорить Хабр? 28 фев в 20:49 Мы закрываем Хабр Фриланс 24 фев в 15:09 Мобильной разработке в 2025 — быть 13 фев в 16:20 Железное будущее, Мона Лиза, марсоход: пишем на Хабр, побеждаем в Технотексте-7 12 фев в 14:12 Песочница Хабра: как получить инвайт и не замучить модератора 2.3K 1 +1 57K 102 +102 1.7K 3 +3 1.6K 12 +12 5.4K 60 +60 "
18,Новые утечки.pdf,getmatch,"Рассказываем о том, как строить карьеру в IT",414.35,4 часa назад,"Блог компании getmatch, разработка под windows, софт, искусственный интеллект","4.74 Оценка 414.35 Рейтинг getmatch Рассказываем о том, как строить карьеру в IT Весь интернет уже готовится к выходу Windows 12. Вероятно, следующая версия будет во многом сосредоточена на поддержке работы с ИИ. На это намекает в том числе недавняя утечка от Intel, а  4 часа назад 5 мин Arnak Новые утечки. Что мы знаем о выходе Windows 12 6.3K Блог компании getmatch ,  Разработка под Windows* ,  Софт ,  Искусственный интеллект КАК СТАТЬ АВТОРОМ также последние действия AMD и направление развития Bing. Мы знаем, что и Intel, и Microsoft готовятся к поддержке работы Windows 12 на новых процессорах. Об этом нам говорит утечка от leaf_hobby, который известен тем, что раскрывает полные спецификации чипов Intel Xeon перед их запуском. На этот раз leaf_hobby опубликовали подробности аппаратных показателей десктопной платформы Intel Meteor Lake, которую планируется выпустить в этом или в следующем году. Сейчас их твиты уже защищены, но интернет помнит™, а у журналистов отдельных изданий, вроде Insider и The Verge, к ним есть доступ. Сообщается, что Intel в имейле для своих сотрудников упоминает, что ее процессоры следующего поколения будут поддерживать Windows 12. Для этого Meteor Lake должен содержать 20 линий PCIe Gen5. Хотя Microsoft не объявляла о каких-либо планах в отношении Windows 12, уже есть признаки того, что компания планирует будущие версии Windows с расчетом на глубокую интеграцию функций ИИ. Даже последнее обновление Windows 11 уже интегрирует новую версию Bing с искусственным интеллектом прямо в панель задач. А ещё в этом обновлении бизнес-пользователи получают рекомендованный ИИ контент в меню «Пуск». Скоро искусственный интеллект придет в Edge, а потом — и во все остальные аспекты продуктов Microsoft. Компания очень воодушевлена успехом своей инвестиции в ChatGPT, и собирается сделать на него очень сильную ставку в надежде победить конкурентов, в том числе Google, у которой дело с инвестициями в ИИ пошло не так хорошо. Один из слайдов от Intel В начале января руководитель разработки Windows Паноса Паная на CES заявил, что «ИИ собирается заново переизобрести всё то, что вы делаете в Windows». Команда Microsoft Surface также сообщила, что «ИИ изменит то, как мы используем ПК». Всё однозначно указывает на то, что в следующей версии Windows компания собирается очень широко использовать ИИ. Это будет главная новая «фишка» платформы. А чтобы всё это могло работать, Microsoft приходится тесно сотрудничать с партнерами по аппаратному обеспечению, такими как Intel и AMD. Ей нужно, чтобы новые процессоры смогли справляться с высокими рабочими нагрузками ИИ, и без ошибок работали с новой технологией. Это объясняет то, почему Intel, как сообщается, уже упоминает об оптимизации под Windows 12 внутри компании. AMD недавно выпустила свои мобильные процессоры Ryzen 7000, и похвасталась тем, что они являются первыми процессорами x86, содержащими специальный механизм искусственного интеллекта, способный поддерживать эффекты Windows Studio. Эти функции, в том числе удаление фонового шума, размытие фона и автоматическое кадрирование, будут доступны только для техники с ускорителями на чипах ARM, которые работают с помощью специального нейронного процессора (NPU). То есть, AMD и Microsoft сейчас тоже активно совместно работают над внедрением расширенного числа функций на основе ИИ в свои продукты. И такие возможности, как заявляют компании, в дальнейшем будут более широко доступны в традиционных ноутбуках с Windows, работающих на AMD. Возможно, это тоже является подготовкой новых поколений процессоров к запуску с Windows 12. Все эти даты очень хорошо стыкуются. Новые процессоры от Intel и AMD ожидают в конце этого года или в 2024-м. А на разработку новой версии ОС у Microsoft раньше уходило 2-3 года. Windows 11 была выпущена в октябре 2021-го. Так что, если новые утечки верны, следующую основную версию Windows можно ожидать в 2024 году — как раз под ряд новых процессоров. Как она будет выглядеть То, что Windows 12 точно находится в разработке, мы узнали и из утечек декабря 2022 года. Тогда во время мероприятия Ignite Keynote 2022 компания Microsoft (вполне возможно, намеренно) «засветила» интерфейс следующего большого обновления Windows. Потом об этом зудел весь интернет, споря о том, что это могло быть. Что, согласитесь, совсем неплохо для одного скриншота. Изображение, «случайно» показанное на Ignite Keynote, демонстрирует ряд нехарактерных для Windows 11 особенностей: плавающая (а не доходящая до краев экрана) панель задач; плавающее меню поиска; системные значки в верхней (а не привычной нижней) части экрана справа. И интересный виджет погоды слева вверху. Видно, что версия будет заметно отличаться от всех предыдущих. Коллективный разум сейчас считает, что она выйдет, скорее всего, в 2024 году, и будет называться Windows 12 как минимум из-за большого количества разных новшеств — так же, как это было с Windows 11. Скорее всего, так будет выглядеть следующая ОС Следующее крупное обновление Windows пока находится на стадии прототипа, но источники, знакомые с ситуацией, говорят, что показанный во время Ignite интерфейс отображает то, к чему стремится Microsoft в следующей версии своей операционной системы. На мероприятии показали только часть возможных новшеств — в реальности их будет больше. Например, ожидается, что в Windows 12 появится новый экран блокировки, новый центр уведомлений, новый поиск. Ну и ИИ везде, где только можно. В плане интерфейса глобальная цель — сделать его более удобным для сенсорного ввода. У Microsoft пока всё не очень хорошо с балансом по этой части: Windows 8 была чересчур сильно ориентированной именно на «тач-взаимодействие», а Windows 10, наоборот, заточена под работу с мышью и клавиатурой. В Windows 11 компания попыталась найти золотую середину, но не получилось: всё-таки удобство управления при помощи мыши однозначно осталось на первом месте. Туманная перспектива Облачная, точнее. Ходят не очень приятные слухи (впрочем, смотря с какой стороны посмотреть). Что, мол, Windows 12 может стать первой облачной ОС от Microsoft. В последние годы Сатья Наделла переносит всё в сеть, в том числе MS Office. И говорит, что за этим будущее, а «облако изменит всё». Microsoft 365 активно развивается, в том числе за счет Windows 365, облачного компьютера, который можно использовать на любом устройстве, способном поддерживать браузер. Сервис был запущен компанией в 2021 году, работать так можно как с Windows 10, так и с Windows 11. Компания позиционирует это как альтернативу облачным сервисам Google: теперь у неё в облаке тоже можно и хранить данные, и редактировать документы, и даже ходить по меню «Пуск» и папкам в привычном интерфейсе, даже если у вас стоит Ubuntu или вы вообще сидите с планшетом на Android. Если Windows 12 будет доступна только в виде такой облачной платформы (по крайней мере, изначально) — это может серьезно повысить видимость пакета 365 в глазах рядовых пользователей. А там, глядишь, многие и задумаются, не удобнее ли им так будет хранить данные, и продолжать работать в одной ОС с разных устройств. Система может быть во многом похожа на традиционные варианты виртуальных рабочих столов, которые уже много лет развертываются предприятиями для поддержки сценариев BYOD. Но с более простой структурой ценообразования, удобным запуском и ориентацией на более широкую аудиторию. Это также может стать более контролируемым способом для Microsoft проверить механизмы работы своих новых ИИ-помощников. При запуске облачной ОС им не придется отпускать ИИ «гулять» на каждый из компьютеров пользователей, все процедуры будут мониториться на собственных серверах. Будем надеяться, что это один из тех наборов слухов, которые скоро будут опровергнуты. Возможно, команда поэкспериментировала с этой идеей, и поняла, что она не приживается. Но пока что — нужно быть морально готовыми к тому, что со всё большим числом сервисов, в том числе от Microsoft, мы будем взаимодействовать через окно браузера. getmatch Рассказываем о том, как строить карьеру в IT Telegram Сайт ВКонтакте Twitter 160 Карма 198.6 Рейтинг @Arnak Пользователь P.S. Тысячи крутых вакансий в РФ и за рубежом — в телеграм-боте getmatch. Задаете нужную зарплату, и к вам приходят лучшие предложения, а наши эксперты помогают пройти интервью. Не нужно ни резюме, ни портфолио, настройка занимает меньше 30 секунд. А ещё это отличный способ следить за текущим рынком труда. Откликаться не обязательно. Теги:   windows , microsoft , windows 12 , AI , intel , chatgpt Хабы:   Блог компании getmatch , Разработка под Windows , Софт , Искусственный интеллект +11 8 32 Комментарии 32 Публикации ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИНФОРМАЦИЯ Сайт getmatch.ru Дата регистрации 30 июля 2020 Численность 51–100 человек Местоположение Россия Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
19,Очередная прогулка по барахолке в Испании_ отличный ноутбук и много необычных штук _ Хабр.pdf,Selectel,IT-инфраструктура для бизнеса,2055.96,11 часов назад,"Блог компании Selectel, Гаджеты, Компьютерное железо, Старое железо, Читальный зал","4.45 Оценка 2055.96 Рейтинг Selectel IT-инфраструктура для бизнеса Подписаться Изображение не загружено Привет, Хабр! Ну что, я снова прошелся по барахолке под Валенсией — у нас тут зарядили дожди, но именно в это воскресенье было безоблачно. На рынок набилось и продавцов и покупателей так, что и не протолкнешься. Ну и я там тоже побывал, видел много интересного, купил несколько интересных устройств. Об этом всем сегодня и расскажу. Мы в Selectel готовим новый сервис. Если арендуете серверы в рабочих или личных проектах, нам очень поможет ваш опыт — записывайтесь на короткое онлайн-интервью. За участие подарим плюшевого Тирекса и бонусы на услуги Selectel. Что я увидел на блошином рынке Сразу попались три бесперебойника — выглядели ничего, но мне не нужны. Думаю, вполне могут оказаться рабочими — там три одинаковые модели. Стояли, видимо, в одном офисе, компания выехала, с бесперебойниками возиться не стала. Изображение не загружено Затем увидел очень интересный девайс — это трекболл от Kensington. Называется Kensington Orbit. Новый стоит, к слову, около 70 евро, так что вполне себе удачная покупка может быть для поклонников такого рода устройств. DRoman0v 11 часов назад Очередная прогулка по барахолке в Испании: отличный ноутбук и много необычных штук 4 мин 6.3K Блог компании Selectel, Гаджеты, Компьютерное железо, Старое железо, Читальный зал +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Зарплаты айтишников Найдено: весенние мероприятия для айти… Изображение не загружено Потом увидел огромную камеру наблюдения — такие вроде обычно на столбах висят. Модель не указана. Изображение не загружено И уже у следующего продавца нашел еще одну камеру. Но в интернете о ней не удалось найти информации. Возможно, кто-то знает, что это за девайс? Изображение не загружено Ну и еще несколько камер, датчиков и центральный блок управления. Я так понимаю, это система безопасности какой-то охранной компании. Здесь все, даже ключ отключения тревоги, насколько вижу. Изображение не загружено Потом — вот такая камера, достаточно симпатичная. По ней инфу уже нашел. Вот главные характеристики: Тип камеры: мгновенная (Instant Film Camera) Формат снимков: Fujifilm Instax Mini (54 × 86 мм, размер изображения – 46 × 62 мм) Объектив: 60 мм f/12.7 Фокусировка: фиксированная (от 0,6 м до бесконечности) Экспозиция: автоматическая, регулируемая вручную (5 режимов) Вспышка: встроенная, всегда активна при съёмке Питание: 2 батарейки AA Габариты: 116 × 118 × 68 мм Вес: около 307 г Камера Instax Mini 8 была выпущена в 2012 году и стала очень популярной благодаря простоте использования и ретро-эстетике. Она позволяет делать моментальные снимки, которые выходят из камеры сразу после нажатия кнопки. Изображение не загружено Затем нашел винтажную камеру. Это кинокамера Konka 418 Auto Sound, предназначенная для съемки на 8-мм пленку. Рядом с камерой находится винтажная вспышка, которая, скорее всего, использовалась для дополнительного освещения во время съемки. Основные характеристики подобных камер: Формат пленки: Super 8 (стандарт 8-мм кинопленки) Объектив: фиксированный или с возможностью ручной фокусировки Зум: механический или моторизованный Автоматический экспонометр: регулирует освещенность кадра Запись звука: вероятно, работает с магнитной дорожкой на Super 8 пленке Питание: обычно батарейки AA или встроенный аккумулятор Скорость съемки: 18 кадров в секунду (стандарт Super 8) Изображение не загружено Потом пошли роботы-пылесосы. Один из них — очень маленький, наверное, сантиметров 10-15 в диаметре. На что такой годится? Со стола крошки собирать после обеда? Изображение не загружено Ну а затем уже нормальный iRobot Roomba — причем с собственной зарядной станцией. Часто они стали попадаться. Это модель 600-й серии. Изображение не загружено Ну и гора самых разных контроллеров и джойстиков, среди которых — настоящая жемчужина от Logitech. Я не любитель таких аксессуаров — это для полетных симуляторов. Модель называется Logitech X52 Flight Control System и включает сразу два джойстика. Продается и сейчас, стоит что-то около 180 евро. Уже дома подумал, что надо было купить, ведь обычно продавцы не знают, что продают (чаще всего) и сбывают за копейки. Изображение не загружено Ну и много всяких обычных контроллеров. Изображение не загружено Изображение не загружено Изображение не загружено Изображение не загружено Попалась даже Binatone TV Master MK 6 – одна из ранних игровых консолей, использовавших технологию Pong (теннисоподобные игры, где игроки отбивают «мяч» между платформами). Это шестая модель в серии TV Master от британской компании Binatone, выпущенной в 1977 году. Теперь — музыкальная секция. Всякие разные инструменты и гаджеты. Кому, например, прозрачные колонки? А вот — винтажный стерео-микшер Akiyama SM-25. Он использовался для DJ-сетов, работы с виниловыми проигрывателями, магнитофонами, микрофонами. Был популярен в 80-90-х среди диджеев и музыкальных энтузиастов. Какой-то недорогой синтезатор. На таком, наверное, хорошо учиться музыке, прежде, чем переходишь к более сложнымм вещам. Musicson TX-100 Digital Auto-Tuner – винтажный цифровой тюнер для настройки гитары и бас- гитары. Крутая аудиосистема, даже две. Модель верхней, к сожалению, в кадр не попала. Нижняя — Aiwa GX-Z700 – кассетная дека высокого класса. Ну и несколько музыкальных инструментов доцифровой эпохи, как же без этого! Первый — это лютня вроде бы или мандолина (я небольшой специалист). А вот, насколько я понимаю, ребаб. Это арабский инструмент, ну а арабов полно в Испании. Так что немудрено, что один такой ребаб попал на барахолку. Возможно я и ошибаюсь, так что если да — поправьте меня в комментариях. Ну и еще увидел дрон, плюс фотоаппараты и бинокли. Есть ли здесь что-то стоящее? Что я купил Приобрел несколько зарядных устройств для ноутбуков HP и ASUS, их вечно не хватает. Один попался на 135 ВТ (6,9А)! В хозяйстве пригодится. Всего купил три зарядника, и все рабочие. Приобрел за 3 евро каждый. Самое ценное приобретение — ноутбук HP 17-by2219ng. Я бы и не покупал, но у него с заднего торца даже пленка осталась защитная. Плюс продавался с зарядником и экран был целый. Продавец отдал все это счастье всего за 18 евро. Дома оказалось, что нет ОЗУ и диска. Но ноутбук полностью рабочий. Так и не понял, почему так дешево и что случилось с этим девайсом, что он оказался на барахолке. Ну и последнее — это медиасистема LaCie LaCinema. Обошлась всего в 4 евро. Купил без кабелей и пульта, только ради диска внутри, там 1 ТБ. Что это за штука? Из загадочных вещей попалась только вот такая вещица. Что-то явно связано с химией. Но что это? Свои догадки пишите в комментариях. Ну а на сегодня все, до новых встреч! Теги: гаджеты, барахолка Хабы: Блог компании Selectel, Гаджеты, Компьютерное железо, Старое железо, Читальный зал Selectel IT-инфраструктура для бизнеса ВКонтакте Telegram Сайт 204 Карма 40 Рейтинг Denis @DRoman0v Инженер Публикации ky0 10 часов назад Ваше мнение очень важно для нас (нет) 3 мин alizar 10 часов назад Глупо покупать технику последней модели, если старая работает хорошо. И можно собрать ПК из комплектующих Простой 6 мин omyhosts 22 часа назад Артефакт эпохи: рождение, взлет и падение клипарта Комментарии 10 +10 ЛУЧШИЕ ЗА СУТКИ ПОХОЖИЕ 4.7K Кейс +77 14 113 +113 7.7K Мнение +36 23 42 +42 Подписаться +24 3 10 +10 Подписаться Простой 6 мин ru_vds 6 часов назад Я мучился с Git, поэтому создал про него игру Средний 8 мин DRoman0v 11 часов назад Очередная прогулка по барахолке в Испании: отличный ноутбук и много необычных штук 4 мин RationalAnswer 14 часов назад Уголовный кодекс vs Блиновская и Митрошина, а также вебкамщицы со стволами против криптоворов 11 мин Stepan_Burmistrov 9 часов назад Использование лидара от робота-пылесоса для системы предотвращения столкновений в автономных роботах Средний 22 мин ProstoKirReal 11 часов назад Сложно о простом. Как работает интернет. Часть 4. Что такое LAN, MAN, WAN, сети Clos и иерархия операторов 3.4K Ретроспектива +29 12 22 +22 3.8K Кейс Перевод +24 36 5 +5 6.3K +24 3 10 +10 12K Дайджест +23 8 35 +35 2.6K Туториал +20 37 10 +10 Средний 20 мин savpek 9 часов назад От психолога до эльфа 80-го уровня: как создать свою уникальную роль для нейросети в домашних условиях и не сойти с ума Простой 26 мин dspmsu 12 часов назад Как я решал задачу 2025 года. Часть 1 Средний 9 мин Показать еще ВАКАНСИИ КОМПАНИИ «SELECTEL» Старший системный инженер / SRE (PaaS) Selectel · Москва · Можно удаленно Больше вакансий на Хабр Карьере 5K +19 94 6 +6 1.6K Туториал +17 21 0 1K +14 7 0 ИНФОРМАЦИЯ Сайт slc.tl Дата регистрации 16 марта 2010 Дата основания 11 сентября 2008 Численность 1 001–5 000 человек Местоположение Россия Представитель Влад Ефименко Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам ССЫЛКИ GPU в облаке от 29 рублей в час selectel.ru Серверы для Data Science за 4,12 ₽/час selectel.ru Серверы для ML-разработки за 4,12 ₽/час selectel.ru 50 ГБ хранилища от 80 рублей в месяц selectel.ru FAQ slc.tl Реферальная программа slc.tl Телеграм-канал о технологиях t.me Телеграм-канал про карьеру в IT t.me Вакансии slc.tl Академия Selectel slc.tl ВКОНТАКТЕ ВИДЖЕТ Настройка языка Техническая поддержка © 2006–2025, Habr ВИДЖЕТ БЛОГ НА ХАБРЕ 11 часов назад Очередная прогулка по барахолке в Испании: отличный ноутбук и много необычных штук 6.3K 10 +10 9 мар в 13:15 Как появление знаменитостей (не) сделало игры лучше 8 мар в 13:00 Framework Desktop: игровой ПК от производителя модульных ноутбуков. Что за система 7 мар в 13:02 Как перестать бояться и задеплоить Django-проект в облако самым простым способом 6 мар в 13:02 Ультимативные крестики-нолики и iPXE 3.8K 11 +11 6.9K 17 +17 3.4K 1 +1 5.1K 4 +4 "
20,"Помощь с текстом, перевод видео с японского и корейского, распознавание QR-кодов — что умеет обновлённый Яндекс Браузер _ Хабр.pdf",Яндекс,Как мы делаем Яндекс,693.28,13 фев 2024 в 12:15,"Блог компании Яндекс, Браузеры, Машинное обучение*, Искусственный интеллект, IT-компании","693.28 Рейтинг Яндекс Как мы делаем Яндекс Тут должна быть обложка, но что-то пошло не так Подписаться NatalieVT 13 фев 2024 в 12:15 Помощь с текстом, перевод видео с японского и корейского, распознавание QR-кодов — что умеет обновлённый Яндекс Браузер Простой 11 мин 13K Блог компании Яндекс, Браузеры, Машинное обучение*, Искусственный интеллект, IT-компании ✏️ Технотекст 2023 +4 Моя лента Все потоки Разработка Администрирование Дизайн Менеджмент Маркетинг Научпоп КАК СТАТЬ АВТОРОМ Скрытая угроза для хакеров: как 2FA рушит их планы Сегодня мы выпускаем большое обновление для Браузера с рекордным числом изменений, в основе которых лежат нейросети или другие методы машинного обучения. Теперь Браузер исправит ошибки в тексте, сократит или улучшит его, перескажет видео с японского или корейского, распознает QR-код в трансляции и предложит перейти по ссылке в один клик, а также защитит от фишинг-страниц и не только. В этой статье расскажем, как мы обучали нейросеть с помощью учебника Розенталя, как модель, отвечающая за субтитры, понимает, что начал говорить другой человек, почему не каждый QR- код легко распознать и за счёт чего мы научились ловить фишинговые сайты, которые появились буквально 5 минут назад. Обо всём этом — под катом. Помощь с текстом: исправление ошибок, улучшение и сокращение Одна из главных новых нейросетевых функций в Браузере — «Помощь с текстом» на основе YandexGPT. Она проверит орфографию и расставит знаки препинания. А если, например, не получается уложиться в формат по количеству знаков, сократит написанный текст и сделает его более понятным и структурированным. За каждое действие отвечают разные модели. Почему они разные? Потому что в режиме редактирования нам нужно, чтобы модель исправила ошибки в тексте, а в режимах сокращения и улучшения — переформулировала его. Поговорим о каждой по порядку. Исправить ошибки Многие люди, которые используют языковые модели для работы с текстами, сталкиваются с одной и той же проблемой. Допустим, в тексте есть десять ошибок, вы отдаёте его модели, а в итоге получаете практически полностью переписанный текст. Модель отработала по принципу «нет слова — нет ошибки». Но это явно не то, чего вы хотели. Для начала мы проверили самую важную гипотезу: может ли эта модель вообще не переписывать текст. Это была наша первая ключевая проверка. Мы взяли тексты, удалили из них все знаки препинания и заглавные буквы — это то, что мы подавали модели на вход. На выходе мы поставили оригиналы текстов. На этом мы обучили модель. Потом дали ей 100 текстов нашего валидационного датасета и попросили расставить знаки препинания. Она прекрасно справилась с заданием и ничего не дописала (diff по Левенштейну был нулевым), а это — победа. Для обучения нейросети мы составили датасет: взяли 5000 текстов из публичного доступа в интернете. Нам было важно, чтобы это были живые тексты: где-то был сленг, мат, специальные символы. В общем, любой шум, который мог усложнить задачу.  После этого мы с редактором взяли учебник Розенталя — пожалуй, главный учебник русского языка — и прошлись по всем примерам. Но мы не вычищали эти тексты до идеала: мы исправляли пунктуацию и орфографию — условно, хардкорные ошибки. Мы хотели сохранить стилистические штуки типа англицизмов и плеоназмов (когда слова полностью или частично дублируют смысл друг друга — например, «коллега с работы» или «краткое резюме»). Нам было важно, чтобы модель научилась отличать орфографическую или пунктуационную ошибку от разговорной речи. Затем мы составили инструкции и попросили редакторов-асессоров по ним исправить тексты.  На этом этапе у нас был готов датасет, и мы загрузили его в модель побольше, чтобы повысить качество. Чтобы проверить работу модели, мы прогнали через неё валидационный датасет и получили хорошие результаты. Сейчас наша модель исправляет 97% ошибок — пропускает одну ошибку в тексте на 5000 знаков. Мы планируем улучшить этот показатель до 99% — смотрим, какие ошибки даются модели труднее всего, и дособираем датасет по ним.  Сократить и улучшить Теперь поговорим про модели, которые отвечают за сокращение и улучшение текста. При сокращении мы следим за тем, чтобы текст на выходе не терял важные факты, сохранял тон и стилистику, а нейросеть не придумывала ничего лишнего. Как и в случае с исправлением ошибок, мы обучили модель на датасете, который разметили редакторы.  Работу модели мы оцениваем по нескольким критериям: сократился ли текст, дописала ли модель что-то новое, сохранила ли она важные факты, а также тон и стиль автора. В 95% случаев модели удаётся добиться заметного сокращения текста.  Улучшить — значит привести текст к нормам русского языка, структурировать его и сделать понятнее. Здесь мы получили модель на основе нашей большой модели YandexGPT 2 и специально подобранного промта. Мы прогнали собранные в интернете тексты через большую модель и получили датасет, на котором уже обучили маленькую модель улучшения текста. Оценить работу модели достаточно сложно, потому что понятие хорошего текста весьма субъективно. Мы определили такие критерии: сохранение важных фактов, тона и стиля автора, а также стал ли текст приятнее и понятнее. На первых порах модель работала с фактами не очень хорошо: она их периодически теряла. Нам помогло дообучение на датасете, который мы разметили вручную. Все три модели улучшения текста работают в любом текстовом поле любого сайта. Например, в браузерных версиях мессенджеров Telegram и WhatsApp, в полях комментариев и постов в VK, ОК и других соцсетях, а также в электронной почте. Сейчас мы работаем над тем, чтобы модель начала работать в текстовых редакторах, например, в Google Docs.  Улучшение текста пока работает только для русского языка, но мы планируем заточить их и под работу с английским. Кроме того, мы хотим попробовать добавить функцию перевода или изменения стилистики (например, сделать текст более весёлым или, наоборот, серьёзным). Если у вас есть идеи, какие ещё инструменты для работы с текстом были бы полезны, пишите их в комментариях. Видео: новые языки, спикеры в субтитрах, распознавание QR-кодов Японский + Корейский За всё время Браузер накопил хороший набор встроенных нейросетевых инструментов для видео: он может переводить их с разных языков и даже озвучивать, а ещё делать краткий пересказ и генерировать субтитры. Придумать здесь что-то новое — довольно сложно, поэтому мы сосредоточились на улучшении того, что у нас есть. Мы добавили новые языки, с которых можно перевести видео — японский и корейский. Почему именно они? Во-первых, это одни из самых распространённых и популярных  азиатских языков наряду с китайским. Во-вторых, нам пришло много запросов на добавление именно этих языков. Вы просили — мы сделали.  В целом, когда мы работали над переводом с китайского, то мы рассчитывали добавить и другие азиатские языки. Тогда нам удалось собрать фреймворк, который можно без проблем масштабировать. Для сравнения: над китайским мы работали около семи месяцев, а корейский и японский нам удалось раскатать примерно в два раза быстрее. Мы уже знали, как собирать и обрабатывать датасет, как организовать процесс перевода, чтобы в итоге не потерять контекст.  Перевод с японского и корейского работает на YouTube. Но если вам нужен перевод на других платформах, напишите в комментариях, на каких и почему.  Спикеры в субтитрах Также мы добавили полезную функцию в нейросетевые субтитры. Дело в том, что чаще всего субтитры на разных платформах не предполагают разбивку на спикеров: когда говорят несколько человек, то отображается сплошной текст, который в лучшем случае разделён точками. Мы научили Браузер определять смену говорящего: речь нового спикера будет начинаться с новой строки и с тире. Это ощутимо повышает читаемость и удобство — не нужно следить, кто сейчас говорит какую фразу. К тому же это плюс к теме доступности: людям с нарушениями слуха гораздо комфортнее смотреть видео и воспринимать происходящее.  У нас уже были модели, которые поддерживают смену спикера. Например, в озвучке применялся Multi-Voice: модель выбирает разные голоса для разных спикеров и озвучивает видео. Отображать смену спикера в субтитрах проще: пайплайн присылает нам явный тег, что говорят разные люди, и Браузер понимает, что нужно визуально обработать смену спикера. Суммаризация с восьми иностранных языков Ещё одна приятная новость — мы обновили суммаризацию видео. Теперь Браузер может пересказывать видео с английского, немецкого, французского, итальянского, испанского, китайского, японского и корейского языков. Сам пересказ будет на русском языке. А ещё мы добавили новые площадки: кроме видео на YouTube, Браузер может пересказать видео в VK, Дзене и Рутубе. Так что посмотреть несколько научных конференций на разных языках за один вечер стало вполне реально. Как работает суммаризация видео, мы скоро расскажем в отдельной статье. К действующей модели мы добавили распознавание входного языка. Если это русскоязычное видео, то весь процесс идёт по обычной схеме: перевод видео в текст, ASR, биометрия (тут мы тоже следим за сменой спикера), чаптеринг, расстановка пунктуации. Если нам нужна суммаризация видео на иностранном языке, то мы начинаем с подобной обработки оригинальной звуковой дорожки, а потом отдаём результат модели, которая переведёт его на русский.  Распознавание QR-кодов В видео довольно часто встречаются QR-коды. Например, по нашей статистике около 20% пользователей, которые пользуются видеоплатформами, ежедневно встречают их в роликах. На десктопе работать с QR сложно: нужно тянуться за телефоном или делать скриншот, чтобы расшифровать код в стороннем сервисе. Поэтому мы решили облегчить пользователям жизнь и научили Браузер распознавать QR-коды на видео. Весь процесс происходит локально. При воспроизведении видео Браузер раз в секунду делает его скриншот. Затем снимки обрабатываются: например, если пользователь смотрит видео 4K или 8K, то скриншоты получатся большим и в итоге сильно нагрузят систему, поэтому мы ужимаем их до FullHD. А если видео не самого высокого качества — улучшаем и детализируем изображение. Скриншоты нигде не сохраняются, а сразу отправляются во встроенную библиотеку, которая понимает, есть ли на полученной картинке QR-код.  В качестве инструмента для самого распознавания мы взяли опенсорс-библиотеку ZXing с небольшими, но важными доработками с нашей стороны. Она возвращает распознанный текст и координаты QR-кода. По координатам Браузер отрисовывает кнопку и рамку вокруг QR. Сценарий использования кнопки зависит от распознанного текста: если это ссылка, то по ней можно перейти, а если это текст, то скопировать.  В целом алгоритм выглядит простым, но реализовать его было сложно. Основная загвоздка была в качестве распознавания. Когда мы использовали чистую ZXing, то она распознавала только 70% «правильных» QR-кодов, то есть тех, которые сделаны по стандарту. Мы разобрались, как устроена библиотека, и придумали три доработки, которые сильно повысили качество распознавания и помогли справиться даже с самыми сложными и необычными QR.  Первое улучшение — апскейлинг изображения. Сначала мы пытаемся распознать QR-код с исходной картинки, а если не получается, то масштабируем её и пытаемся распознать заново. Это улучшило точность с 70% до 76%. Второе — мы починили баг в библиотеке. Суть в том, что если она распознавала QR-код, то потом пыталась распознать его ещё несколько раз, но с другими параметрами. Мы добавили в логику работы условие: если библиотека хотя бы один раз с каким-то набором параметров распознала QR-код, то мы считаем, что распознали его, и больше к нему не возвращаемся. Это дало очень хороший буст — с 76% до 90%.  Третье — улучшенный алгоритм распознавания якорей (поисковых маркеров). Это большие квадраты по углам, с которых и начинается чтение кода. Но иногда QR не соответствуют стандартам: якоря превращаются в кружочки, сердечки и другие фигуры, что усложняет задачу. Мы убрали жёсткую привязку, что якоря должны быть квадратными, и теперь библиотека просто ищет три похожих фигуры в углах. Это улучшение позволило нам приблизиться к 100% распознаванию. Для тестирования мы собрали два офлайн-датасета. Первый был из «правильных» QR-кодов, которые появлялись в трансляциях наших внутренних мероприятий. Мы стремились, чтобы там было 100% распознаваний (что у нас и получилось). Ещё у нас был «плохой» датасет. Мы попросили асессоров посмотреть разные видео и поискать там QR-коды в плохом качестве или нестандартной формы. Там мы распознавали меньше 30%, а после всех наших улучшений — порядка 60%. В этом плане эталоном для нас служил iPhone 15: он тоже распознал не все нестандартные QR, а около 60%. Причём в нём за это отвечает большая и сложная нейросеть, а наш метод работает даже на слабом железе. Кстати, о вопросе производительности и применении нейросетей. В процессе разработки мы поняли, что для распознавания QR-кодов потребуется сложная ML-модель, для работы которой может не хватить ресурсов пользовательских устройств. Она бы работала очень долго и тормозила работу Браузера. Распознавание с помощью библиотеки работает быстрее: в среднем один код распознаётся за 10 миллисекунд, а в самых сложных случаях — за 25 миллисекунд.  Задача с распознаванием QR-кодов — хороший пример того, что всегда нужно думать о пользователе и не стоит пытаться отдать на откуп ИИ всё подряд. Здесь нам было важно выбрать наиболее оптимальный инструмент, чтобы решить задачу, и нейросети не подошли. Хотя в основе нашего решения всё равно лежат методы, используемые в ML.   Распознавание QR-кодов работает на всех десктопных платформах. Его можно включать и отключать в настройках инструментов для видео — там же, где живут перевод, краткий пересказ и субтитры.  Защита от фишинга До обновления Браузера мы искали фишинговые сайты с помощью робота. Он несколько раз в сутки заходил на сомнительные страницы и оценивал их. В этом ему помогали ML-модели на сервере. Если сайт всё же оказывался фишинговым, то мы добавляли его в соответствующую базу. И когда пользователь заходил на новый сайт, Браузер быстро сверялся с базой: если адрес там уже есть, появлялось предупреждение.  Основная проблема была в том, что на весь процесс тратилось довольно много времени и ресурсов: нужно, чтобы робот сходил на сайт, да и вообще как-то узнать о его существовании. Потом — проверка, внесение в базу, её раскатка. Весь этот процесс занимает от нескольких часов до суток — как раз столько в среднем живут все фишинговые сайты. Мошенники делают домен, используют его до тех пор, пока он не попадёт в базу с фишингом, а после берут новый домен и обманывают уже на нём. Были и случаи, когда сайты могли отдавать нашему роботу нормальную страницу вместо фишинговой и пытались таким образом обходить проверку. Сейчас мы внедрили в Браузер ML-модель, которая умеет проверять на клиенте, фишинговый ли сайт. Таким образом мы смогли сократить цепочку проверок. Пользователь заходит на сайт, ML-модель оценивает его содержимое, даже если на этот сайт никто до этого не заходил. Если сайт выглядит как фишинг, то запускается более сложная модель уже на наших серверах. Чтобы принять решение, она использует результаты работы клиентской ML-модели и учитывает дополнительные факторы: посещаемость сайта, на кого этот сайт зарегистрирован, статистику отказов от этого сайта, насколько давно сайт был создан, как много он показывается в поиске. При этом конфиденциальные данные или какая-то текстовая информация со страницы не передаются. После проверки страницы рассчитывается её скоринг и выносится окончательный вердикт. Если сайт похож на фишинговый, то пользователь видит предупреждение, даже если этот домен был создан буквально 5 минут назад и на него ещё никто не заходил. Таким образом, мы убираем уязвимость предыдущего способа ловли фишинга, когда из поля нашего зрения выпадали домены-однодневки, которые менялись на другие домены, пока мы их вносили в базу.  Самое сложное в разработке этой штуки — получить датасет. Как уже было сказано выше, фишинг живет очень мало и, кроме того, бывает доступен только для конкретного человека по конкретному IP с конкретного устройства. Важно успеть собрать факторы, пока сайт не умер. После сбора и фильтрации датасета мы обучили на нём большую BERT-модель,чтобы она умела отличать фишинг от обычного сайта. Но такая модель очень тяжёлая: для серверов она подходит, а вот для работы в real-time — нет. Для нашей цели мы выбрали DSSM: пусть модель, основанная на таком подходе, менее умная, но зато более быстрая. Для обучения ей нужно очень много примеров, поэтому с помощью BERT-модели мы разметили огромный датасет, получили результаты и на них обучили DSSM. В итоге эта модель умеет на лету предсказывать фишинг и весит всего 20 МБ — её мы и встроили в Браузер. Яндекс Как мы делаем Яндекс Github 37 Карма 3.6 Рейтинг Наталья Которева @NatalieVT Коты опять все уронили, а поднимать мне Модель работает очень быстро — в пределах 10 миллисекунд, что не влияет на скорость работы Браузера. Кроме того, мы стали точнее определять фишинг. Да, аудитория Хабра разбирается в технологиях и, как правило, не клюёт на примитивный фишинг, но для многих менее опытных пользователей подобные инструменты защиты особенно востребованы. Месячная аудитория, которая видит наши предупреждения только на десктопной версии, — примерно 1,8 млн человек.  Это не все возможности, которые может предложить обновлённый Браузер. Например, группы вкладок теперь стали облачными и могут синхронизироваться между устройствами​. К тому же вы можете сами выбрать, какие именно вкладки и группы нужно синхронизировать. А ещё Алиса научилась генерировать изображения с помощью нейросети YandexART. Чтобы создать свою картинку, зайдите в диалог с Алисой и активируйте навык «Давай нарисуем».  Нейросетевые функции Браузера активируются с помощью компактных меню, которые автоматически появляются рядом с контентом, где их можно применить. Например, кнопка поверх видео или всплывающая плашка при выделении текста. Если функция влияет сразу на всю страницу, меню появится рядом с адресной строкой.  Новые функции доступны в последних версиях Яндекс Браузера для Windows, macOS и Linux, а также для мобильных версий (кроме помощи с текстом и распознавания QR-кодов в видео). Делитесь своим опытом использования новых инструментов Браузера. Это поможет нам улучшить действующие нейросетевые модели, а также придумать и обучить новые. Теги: яндекс, браузеры, яндекс браузер, нейросети, редактирование текстов, qr-коды,  суммаризация, субтитры, перевод видео, команда яндекс.браузера Хабы: Блог компании Яндекс, Браузеры, Машинное обучение, Искусственный интеллект, IT- компании Подписаться Полезные ссылки 17 апр 2023 в 10:01 Китайский язык очень сложный. Мы сделали для него перевод видео 6 мин 16 мар 2023 в 12:01 Название имеет значение: как получить оптимизацию, переименовав браузер 4 мин 6 июл 2023 в 10:58 YandexGPT в Браузере: как мы учили модель суммаризировать статьи 7 мин 24K +101 74 56 +56 38K +172 69 78 +78 31K +44 62 51 +51 Комментарии 54 +54 ИНФОРМАЦИЯ Сайт www.ya.ru Дата регистрации 9 августа 2008 Дата основания 23 сентября 1997 Численность свыше 10 000 человек Местоположение Россия ВИДЖЕТ Ваш аккаунт Профиль Трекер Диалоги Настройки ППА Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Подписаться БЛОГ НА ХАБРЕ 7 часов назад История Ultimate Play the Game — легендарного разработчика игр для ZX Spectrum 11 мар в 12:00 Доставка день в день: погружение в базовые алгоритмы поиска и назначения курьеров в Яндекс Доставке 7 мар в 13:00 NeurIPS: тренды, инсайты и самые интересные статьи главной ML-конференции года 6 мар в 12:00 Как Яндекс запускает роботов-доставщиков в новых районах и городах 4 мар в 12:00 От каскадных моделей до картинок в 4к: как эволюционировали диффузионки 1.2K 12 +12 2K 5 +5 1.5K 1 +1 3.6K 10 +10 4.7K 5 +5 Настройка языка Техническая поддержка © 2006–2025, Habr "
21,Почему Data Science не для вас.pdf,TourmalineCore,-,0.0,5 часов назад,-,"Data Science сейчас во многом благодаря активному маркетингу становится очень популярной темой. Быть датасаентистом – модно и, как говорят многие рекламки, которые часто попадаются на глаза, не так уж и сложно. Ходят слухи, что работодатели стоят в очереди  5 часов назад Простой9 мин TourmalineCore Почему Data Science не для вас? 2.1K Big Data* ,  Data Engineering* Мнение КАК СТАТЬ АВТОРОМ за возможность взять человека с курсов. Получить оффер на работу крайне легко, ведь в ваши обязанности будет входить требование данных от заказчика (как обычно говорят, чем больше данных – тем лучше) и закидывать их в искусственный интеллект, который работает по принципу черного ящика. Кстати, еще и платят немереное количество денег за всё это.  Спойлер: это не так. В этой душераздирающей статье решили попробовать отговорить людей, которые готовы оставить кучу денег за курсы по Data Science, браться за это дело, а может быть и помочь определиться с тем, что на самом деле стоит сделать, чтобы встать на путь истинный. Обзор понятий И начнем, пожалуй, с того, что может быть вам и не очень-то нужно быть датасаентистом. Есть много направлений, поди попробуй разберись. Датасаентист, дата аналитик, дата инженер… Часто бывает, что всех этих людей гребут под одну гребенку, и совсем не разделяют их обязанностей. Разберемся, что это за звери такие и с чем их едят.  Дата аналитики обычно заняты тем, что получают данные и пытаются найти в них закономерности. Эти люди отвечают на вопросы бизнеса: что, почему, как, как сделать лучше, а еще вот можете на график посмотреть. Аналитики чаще контактируют с другими людьми: чтобы выстроить правильную коммуникацию в команде и правильно описать и презентовать результаты своего анализа. Ваши друзья здесь: мат.статистика, python/R, SQL, платформы типа PowerBI/Tableau/Qlik и прочее необходимое для визуализации результатов. Также необходимо глубокое понимание области, в которой вы проводите анализ. Дата инженеры же заняты поставкой данных и их хранением. Эти люди настраивают хранилища, заставляют данные сопоставляться между собой и быть пригодными к использованию. Создание пайплайна обработки и архитектуры данных  – вот их задача.  Датасаентисты – более расплывчатое понятие. Сначала они анализируют данные, на которых дальше строится предобработка. В предобработку входит очистка от выбросов, заполнение пропущенных значений, трансформация, нормализация и еще куча всякого разного. И после всего этого датасаентисты  обучают модели, которые будут давать какой-то предикт. От этих ребят нужен тот самый «искусственный интеллект» (кстати, стоит убрать из своего лексикона эти слова на собеседовании, расскажем дальше, почему). Здесь надо знать и уметь в машин и дип лёрнинг, опять таки уметь в матстат и обработку того, что вы видите, понимать предметную область, уметь продумать, какие данные нужны для обучения и составить ТЗ по их сбору. Будет большим плюсом понимание того, на каких машинах модели, которые вы собираетесь обучать, будут запускаться.  Хороший доклад на тему того, чем заняты разные люди из разных ветвлений размытого понятия data scientist, есть по ссылке. У описанных направлений совершенно разные ключевые навыки и компетенции. Чем больше компания, тем вероятнее, что в ней есть разделение на эти направления. Но если вы с курсов решили устроиться в стартап, где есть биг дата, но особо нет людей на эту биг дату, тогда вашим ключевым навыком должно быть умение выживать. Именно здесь обычно нужен Data Scientist, который на все руки мастер. Готовы ли вы к этому? Почему всё-таки Data Science не для вас? 1. Data Science – для усидчивых. Над одной и той же задачей придется сидеть очень долго. Пока найдешь все необходимое в данных, пока найдешь оптимальный способ обработки данных, пока погрузишься в область, пока переберешь все модельки, которые могут потенциально хорошо решить эту задачу… А еще надо дождаться, пока модель обучится - это ж целая вечность пройдет! Потом еще выясняется, что у модели метрики недостаточно хорошие, и весь этот путь нужно пройти заново, но с новыми идеями.  Хватит ли вам умения занять себя другими делами во время обучения модели? Вряд ли проджект будет в восторге, если узнает, что несколько дней вы пили кофе, пока модель обучалась. Хватит ли вам терпения перебирать идеи до тех пор, пока точность модели не достигнет нужного бизнесу числа? 2. Математика останавливается не на 2+2 и даже не на интегралах. Вам точно понадобятся знания по мат.статистике, линейной алгебре, теории вероятностей. Нет, вам не хватит встроенных в библиотечки функций. Может быть для какого-то поверхностного анализа и составления простецкой модели – да, Но чтобы лучше анализировать огромные массивы данных, в математике действительно нужно разбираться. 3. Вам нужно знать Python/R/Scala или еще что-то, на чем вы соберете ваше решение. Вы не бэкенд-разработчик, но вам нужно приготовить умную штуку, которую можно будет встроить в прод. Нужно будет составлять POC-шки, функции предобработки данных, которые затем будет имплементировать бэкенд-разраб. Необходимо уметь общаться с бэкендерами на их языке, если им понадобится помощь при внедрении вашего решения. 4. Еще немного о встраивании модели в прод: вам нужно интересоваться тем, на каком железе будет запущена модель. Ну не получится запустить трансформер на сервачке заказчика с двумя ядрами и без малейшего признака GPU. А решение, которое работает на вашем компе и не работает на устройствах клиента не будет принято и, соответственно, оплачено. 5. Нужно знать фреймворки, которые приняты в команде. Если вы до этого делали fit на керасе, а потом внезапно пришли в команду, которая работает на pytorch, уйдет много времени на обучение работе с новыми инструментами. Справедливости ради, вряд ли вас возьмут на работу со стеком, в котором у вас нет опыта, но на всякий случай лучше задавать этот вопрос на первых этапах собеседования. 6. По ходу проектирования решения нужно будет использовать огромное количество, библиотек и лучше знать как ими пользоваться до начала работы над проектом. Или же уметь быстро ориентироваться в документации, а в процессе только добирать необходимое. Обучаться всему и сразу плохая идея. 8. Вам нужно уметь использовать тулзины, которые тесно связаны с разработкой моделей. Модели и данные нуждаются в версионировании. В этом могут помочь DVC, MLflow или, например, Weights&Biases. А еще модели нужно сервить. В этом случае могут быть пригодны TorchServe или Tensorflow Serving. А может быть вам нужно будет написать свою обвязку вокруг модели, чтобы ее можно было быстро потестить. Этот зоопарк огромен. Нужно выбрать из него именно те вещи, которые вам помогут, и учиться ими пользоваться. 9. Нужно уметь не только чистить данные, но и размечать их, когда вам не дали нормальной разметки, искать, какие данные можно добавить к выданному заказчиком датасету, если его явно не хватает. 10. Важной частью вашей работы будет общение с бизнесом для того, чтобы точно определить цель того, что вы делаете, и не сделать того, что не нужно. Иногда заказчики дают расплывчатое ТЗ, внимательно вчитываясь в которое вы обнаружите, что заказчик хочет всё и сразу, но при этом непонятно зачем. Поэтому нужно будет суметь задать такие вопросы, которые прояснят, что действительно требуется от вас, или суметь сократить скоуп работ до реально реализуемого. 11. Если проджект где-то там пообещал нереальные сроки для выполнения вашей задачи, нужно вовремя суметь сказать, что пятилетку за три года – не получится. Нельзя просто так взять и выстроить радужные мечты вокруг того, как быстро вы сможете сделать задачу. А с вас точно потребуют сроки, в которые будет необходимо уложиться. Вопрос оценки времени на реализацию – очень сложен. Здесь важно не только правильно декомпозировать задачу, но и добиться понимания от вышестоящих людей, почему эта задача не может быть реализована в очень сжатые сроки. Так что составляем план, и его придерживаемся. А если что-то выходит из-под контроля срочно трубим о том, что ничего не успевается и с этим надо что-то делать. 12. А еще вам самим придется составлять ТЗ по данным, которые вы хотите получить. Из нашего опыта, чаще приходят заказчики, которые не понимают, что вам от них нужно и в каком объеме. Будьте готовы отвечать на вопрос сколько и каких данных вам надо, вникать в задачу на этапе обсуждения проекта и предполагать, какие кейсы могут возникать в вашей задаче. Также количество размеченных данных, следует запрашивать в таком объеме, который сможет удовлетворить заказчика. Если вы потребуете терабайты размеченных данных, а другой исполнитель скажет, что ему хватит пары гигов, то выберут скорее его, а не вас, просто потому что разметка данных стоит немалых денег. А ответ «чем данных больше – тем лучше» никого не устроит. 13. Data Science – не только про генерацию красивых картинок и красивой музыки без помощи человека. Частый запрос бизнеса – проанализировать информацию о клиентах. Из этой бизнес- задачи может вытекать DS-задача, которая вам не понравится. Например, выстроить контекстную рекламу так, чтобы новые покупатели приобрели товар подороже, или завысить цену продукта в соответствии с предполагаемым уровнем дохода покупателя. Будет ли такая задача вписываться в ваши моральные рамки? Возможно, у вас будет опция отказаться от задач, которые не подходят вашему пониманию хорошо- плохо.Но бывает и так, что вас не спрашивают. Конечно, вопрос, стоит ли работать с компанией с которой у вас разное понимание границ не в плоскости data science, а скорее в общем. Но будьте готовы к тому, что в data science часто встает вопрос этики использования данных, и эта этика зафиксирована в достаточно размытом формате. Вы можете сказать, что на самом деле многие навыки из этого списка входят в программу популярных курсов. Да, это так. Но дадут ли вам необходимое количество разнородных задач, которые присутствуют в реальном мире? Насколько смешанные, странные и неполные данные будут присутствовать в этих задачах? Требования от заказчиков в реальном мире будут сильно сложнее, чем обычно определяют на курсах. На курсах вам создадут рафинированную среду, в которой все понятно, четко и ясно определено в требованиях и точно опирается на предыдущий опыт из лекций. К сожалению, на работе будет не так. Придётся ловить на лету и определять, как действовать, очень быстро. Так что наше мнение: пойти на курсы, чтобы стать датасаентистом, недостаточно.  Иметь высшее образование, кстати, тоже. Знаем, бывали. Даже если ваша вышка с уклоном в дата саенс, по факту, она часто дает не сильно больше курсов. Не спорим, есть разные университеты, но, по нашему опыту, вам там дадут примерно такую же базу, как и на курсах, только чуть более разностороннюю (и то не факт). Но все- таки, это обучение длится подольше, и скорее всего, информации вы впитаете побольше. Еще и курсы по философии и экологии подкинут :) Важное качество в профессии датасаентиста – способность постоянно учиться, находить для себя новые задачи и расти на этом практическом опыте. А еще постоянно следить за событиями в сфере, потому что кажется, что она обновляется чаще и кардинальнее, чем другие сферы IT. Поэтому специализированные курсы – это хорошо, высшее образование – тоже, но важно понимать, что это ещё далеко не всё, это только начало длинного и тяжелого пути. Cекция ""вопрос-ответ"" в лучших традициях пиар- страниц курсов 1. Датасаенс – это весело? Конечно! Но только если вы действительно упороты и любите искать что-то странное и копаться в куче данных, чтобы в итоге делать штуку, которая будет помогать бизнесу в реальных задачах с большой точностью. Но будьте готовы и к не самым веселым моментам: на этом самурайском пути много препятствий. В каком-то смысле, датасаенс – это образ жизни, при котором нужно всегда держать руку на пульсе и интересоваться происходящим в области. Став датасаентистом однажды, больше уже не сможешь смотреть на все эти умные штуки как на абсолютную магию. 2. Я математик по образованию и по желанию и очень хочу в DS. Получится? Отличная база. Математика как основа – это очень хорошо. Но вам еще очень многое придется освоить. Удачи вам в этом нелегком пути! 3. А я вот из программиста решил переквалифицироваться в датасаентиста, каковы шансы? Вам будет тяжело. Шанс есть всегда, но это будет очень сложный путь. На нашем опыте, понять программирование для человека с математическим бэкграундом проще, чем программисту математику. Мы все проходили курс по высшей математике в универе и помним матешу со школы, но это не тот уровень, который будет требоваться на месте работы. 4. А почему вы говорите, что искусственного интеллекта сейчас не существует? Искусственный интеллект – интересное понятие. Этим термином внезапно стало принято называть всё, что может делать что-то за человека. Но до «интеллекта» там далеко. Да данный момент то, что мы называем ИИ, это алгоритмы, которые могут решать узконаправленную задачу. Распознавание лиц? Детекция объектов? Генерация контекстной рекламы? Интеллектуальный анализ данных? Все это узконаправленные сферы. Разве мы можем назвать калькулятор искусственным интеллектом? Ну, это вряд ли. Хотя он также решает узконаправленную задачу – решение математических уравнений. А пару столетий назад это казалось задачей искусственного интеллекта. В целом, можно сказать, что ИИ существует, но в очень примитивном виде. Его эволюцию можно сравнить в эволюцией человека. На ранних стадиях задачи человека также были узконаправленными и примитивными, но человек эволюционировал. Так же можно предположить и с ИИ, он может эволюционировать. И не обязательно базой для «прогрессивного ИИ» будут нейронные сети. Подведем итоги • Data Science – это профессия с высоким порогом входа. Нужно многое знать, нужно многое уметь. Если вы хотите в IT, и вам кто-то сказал, что через дата саенс это сделать проще, чем через другие профессии, то не слушайте этого человека. Обратите внимание на другие возможные варианты входа в IT, например, верстку или ручное тестирование.  • Data Science – это не только про построить предсказывающую модель в jupyter-notebook и отдать ее дальше. Скорее всего, вам придется делать сильно больше этого функционала. • В работе с данными есть разные ветвления профессий, например, инженер данных или аналитик данных. Для них требуются иные навыки. Может быть, если ваше желание именно работать с данными, вам стоит посмотреть на эти специализации. 5 Карма 2 Рейтинг Tourmaline Core @TourmalineCore Пользователь Комментарии 5 Публикации • Data Science – про постоянное обучение и слежение за новыми разработками в этой сфере. Держать руку на пульсе – это один из ключевых навыков датасаентиста. • Курсов, на которых вам за несколько месяцев дают всё необходимое для профессии – не существует. Вам могут дать базу, но стоит ли эта база тех денег, которые за нее просят, это спорный вопрос. • Есть множество курсов по DS, которые находятся в открытом доступе абсолютно бесплатно, и дают примерно тот же материал, что и дорогостоящие. А практические задачи можно найти на kaggle. Кстати, там же есть и решения других людей, которые могут вам помочь обучиться и понять, как же все-таки делать лучше. Рассмотрите этот вариант. Возможно, в будущем мы сделаем статью-сборник с такими материалами, а пока посоветуем отличный ресурс для начала обучения. Теги:   ML , data science , курсы , курсы data science , machine-learning Хабы:   Big Data , Data Engineering +2 19 5 ЛУЧШИЕ ЗА СУТКИ  ПОХОЖИЕ  ·   ·   ·   ·   ·   ·   ·   ·   ·   ·  ИСТОРИИ РАБОТА Ваш аккаунт Войти Регистрация Разделы Публикации Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Мегапроекты Data Scientist 131 вакансия Все вакансии Настройка языка Техническая поддержка Вернуться на старую версию © 2006–2023, Habr "
